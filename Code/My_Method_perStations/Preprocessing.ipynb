{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import calendar\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ObjectId2</th>\n",
       "      <th>Station_Name</th>\n",
       "      <th>Address</th>\n",
       "      <th>City</th>\n",
       "      <th>State_Province</th>\n",
       "      <th>Zip_Postal_Code</th>\n",
       "      <th>Start_Date___Time</th>\n",
       "      <th>Start_Time_Zone</th>\n",
       "      <th>End_Date___Time</th>\n",
       "      <th>End_Time_Zone</th>\n",
       "      <th>Total_Duration__hh_mm_ss_</th>\n",
       "      <th>Charging_Time__hh_mm_ss_</th>\n",
       "      <th>Energy__kWh_</th>\n",
       "      <th>GHG_Savings__kg_</th>\n",
       "      <th>Gasoline_Savings__gallons_</th>\n",
       "      <th>Port_Type</th>\n",
       "      <th>ObjectID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>BOULDER / JUNCTION ST1</td>\n",
       "      <td>2280 Junction Pl</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>80301</td>\n",
       "      <td>1/1/2018 17:49</td>\n",
       "      <td>MDT</td>\n",
       "      <td>1/1/2018 19:52</td>\n",
       "      <td>MDT</td>\n",
       "      <td>2:03:02</td>\n",
       "      <td>2:02:44</td>\n",
       "      <td>6.504</td>\n",
       "      <td>2.732</td>\n",
       "      <td>0.816</td>\n",
       "      <td>Level 2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>BOULDER / JUNCTION ST1</td>\n",
       "      <td>2280 Junction Pl</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>80301</td>\n",
       "      <td>1/2/2018 8:52</td>\n",
       "      <td>MDT</td>\n",
       "      <td>1/2/2018 9:16</td>\n",
       "      <td>MDT</td>\n",
       "      <td>0:24:34</td>\n",
       "      <td>0:24:19</td>\n",
       "      <td>2.481</td>\n",
       "      <td>1.042</td>\n",
       "      <td>0.311</td>\n",
       "      <td>Level 2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>BOULDER / JUNCTION ST1</td>\n",
       "      <td>2280 Junction Pl</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>80301</td>\n",
       "      <td>1/2/2018 21:11</td>\n",
       "      <td>MDT</td>\n",
       "      <td>1/3/2018 6:23</td>\n",
       "      <td>MDT</td>\n",
       "      <td>9:12:21</td>\n",
       "      <td>3:40:52</td>\n",
       "      <td>15.046</td>\n",
       "      <td>6.319</td>\n",
       "      <td>1.888</td>\n",
       "      <td>Level 2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>BOULDER / ALPINE ST1</td>\n",
       "      <td>1275 Alpine Ave</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>80304</td>\n",
       "      <td>1/3/2018 9:19</td>\n",
       "      <td>MDT</td>\n",
       "      <td>1/3/2018 11:14</td>\n",
       "      <td>MDT</td>\n",
       "      <td>1:54:51</td>\n",
       "      <td>1:54:29</td>\n",
       "      <td>6.947</td>\n",
       "      <td>2.918</td>\n",
       "      <td>0.872</td>\n",
       "      <td>Level 2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>BOULDER / BASELINE ST1</td>\n",
       "      <td>900 Baseline Rd</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>80302</td>\n",
       "      <td>1/3/2018 14:13</td>\n",
       "      <td>MDT</td>\n",
       "      <td>1/3/2018 14:30</td>\n",
       "      <td>MDT</td>\n",
       "      <td>0:16:58</td>\n",
       "      <td>0:16:44</td>\n",
       "      <td>1.800</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.226</td>\n",
       "      <td>Level 2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ObjectId2            Station_Name           Address     City  \\\n",
       "0          1  BOULDER / JUNCTION ST1  2280 Junction Pl  Boulder   \n",
       "1          2  BOULDER / JUNCTION ST1  2280 Junction Pl  Boulder   \n",
       "2          3  BOULDER / JUNCTION ST1  2280 Junction Pl  Boulder   \n",
       "3          4    BOULDER / ALPINE ST1   1275 Alpine Ave  Boulder   \n",
       "4          5  BOULDER / BASELINE ST1   900 Baseline Rd  Boulder   \n",
       "\n",
       "  State_Province  Zip_Postal_Code Start_Date___Time Start_Time_Zone  \\\n",
       "0       Colorado            80301    1/1/2018 17:49             MDT   \n",
       "1       Colorado            80301     1/2/2018 8:52             MDT   \n",
       "2       Colorado            80301    1/2/2018 21:11             MDT   \n",
       "3       Colorado            80304     1/3/2018 9:19             MDT   \n",
       "4       Colorado            80302    1/3/2018 14:13             MDT   \n",
       "\n",
       "  End_Date___Time End_Time_Zone Total_Duration__hh_mm_ss_  \\\n",
       "0  1/1/2018 19:52           MDT                   2:03:02   \n",
       "1   1/2/2018 9:16           MDT                   0:24:34   \n",
       "2   1/3/2018 6:23           MDT                   9:12:21   \n",
       "3  1/3/2018 11:14           MDT                   1:54:51   \n",
       "4  1/3/2018 14:30           MDT                   0:16:58   \n",
       "\n",
       "  Charging_Time__hh_mm_ss_  Energy__kWh_  GHG_Savings__kg_  \\\n",
       "0                  2:02:44         6.504             2.732   \n",
       "1                  0:24:19         2.481             1.042   \n",
       "2                  3:40:52        15.046             6.319   \n",
       "3                  1:54:29         6.947             2.918   \n",
       "4                  0:16:44         1.800             0.756   \n",
       "\n",
       "   Gasoline_Savings__gallons_ Port_Type  ObjectID  \n",
       "0                       0.816   Level 2         0  \n",
       "1                       0.311   Level 2         1  \n",
       "2                       1.888   Level 2         2  \n",
       "3                       0.872   Level 2         3  \n",
       "4                       0.226   Level 2         4  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../Dataset/BOULDER_Electric_Vehicle_Charging_Station_Data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These tables contains the indexes of weather data tables for the period Jan-2018 to Nov-2023\n",
    "tables_year = {'2018': [num for num in range(195, 218, 2)],\n",
    "               '2019': [num for num in range(219, 242, 2)],\n",
    "               '2020': [num for num in range(243, 266, 2)],\n",
    "               '2021': [num for num in range(267, 290, 2)],\n",
    "               '2022': [num for num in range(291, 314, 2)],\n",
    "               '2023': [num for num in range(315, 336, 2)]}\n",
    "\n",
    "# Source of Implementation: - https://towardsdatascience.com/a-guide-to-scraping-html-tables-with-pandas-and-beautifulsoup-7fc24c331cf7\n",
    "\n",
    "#                           - https://github.com/SwethaSrikari/Predicting-EV-charging-demand/blob/main/Web_scraping_Colorado_weather.ipynb\n",
    "\n",
    "\n",
    "# Function to scrape weather data table for a specific year from a given URL\n",
    "def get_weather_data_table_basedOnYear(url, year):\n",
    "    # Send a GET request to the specified URL\n",
    "    page = requests.get(url) \n",
    "    # Create a BeautifulSoup object to parse the HTML content\n",
    "    soup = BeautifulSoup(page.text, 'html.parser') \n",
    "    \n",
    "    rows = [] # Initialize an empty list to store rows of data\n",
    "\n",
    "    # Loop through the tables associated with the specified year\n",
    "    for table in tables_year[year]: \n",
    "\n",
    "        # Loop through the children of the table element\n",
    "        for i, child in enumerate(soup.find_all('table')[table].children): \n",
    "            row = [] # Initialize an empty list to store data for each row\n",
    "\n",
    "            # Loop through the cells (td elements) in the row\n",
    "            for td in child: \n",
    "                try:\n",
    "                    row.append(td.text) # Attempt to extract text content from each cell and append to the row list\n",
    "                except:\n",
    "                    continue # If an exception occurs (e.g., if the cell is not a td element), continue to the next iteration\n",
    "\n",
    "            # Check if the row contains any data (i.e., if it's not an empty row)\n",
    "            if len(row) > 0:\n",
    "                rows.append(row) # Append the non-empty row to the list of rows\n",
    "\n",
    "    # Create a DataFrame using the extracted rows, specifying columns and dropping duplicate rows\n",
    "    df = pd.DataFrame(rows[1:], columns=rows[0]).drop_duplicates(keep=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Define the url source\n",
    "url = 'https://psl.noaa.gov/boulder/data.daily.html'\n",
    "\n",
    "# Take the Weather Data Tables for each year\n",
    "df_2018 = get_weather_data_table_basedOnYear(url, str(2018))\n",
    "df_2019 = get_weather_data_table_basedOnYear(url, str(2019))\n",
    "df_2020 = get_weather_data_table_basedOnYear(url, str(2020))\n",
    "df_2021 = get_weather_data_table_basedOnYear(url, str(2021))\n",
    "df_2022 = get_weather_data_table_basedOnYear(url, str(2022))\n",
    "df_2023 = get_weather_data_table_basedOnYear(url, str(2023))\n",
    "\n",
    "\n",
    "# Function to check for leap years\n",
    "def is_leap_year(year):\n",
    "    return calendar.isleap(year)\n",
    "\n",
    "\n",
    "# Function to remove leading and trailing spaces from a string value\n",
    "def strip_spaces(value):\n",
    "    # Check if the value is a string before applying strip()\n",
    "    return value.strip() if isinstance(value, str) else value\n",
    "\n",
    "\n",
    "\n",
    "# Function to prepare weather data\n",
    "def prepare_weather_data(df):\n",
    "    \n",
    "    # Clean column names by removing extra spaces\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # Remove extra spaces from all rows in the DataFrame\n",
    "    df = df.applymap(strip_spaces)\n",
    "\n",
    "    # Handle 'T' as a trace (less than 0.01 inches for precipitation and 0.1 for snow)\n",
    "    df[\"Snow\"] = df[\"Snow\"].replace('T', 0.099)\n",
    "    df[\"Precipitation\"] = df[\"Precipitation\"].replace('T', 0.0099)\n",
    "\n",
    "    # Exclude rows with 'Miss' from the DataFrame\n",
    "    df = df.loc[~(df == 'Miss').any(axis=1)]\n",
    "\n",
    "    # Convert year, month, day to numeric\n",
    "    df[[\"Year\", \"Month\", \"Day\"]] = df[[\"Year\", \"Month\", \"Day\"]].apply(pd.to_numeric)\n",
    "\n",
    "    # Handle February based on leap year\n",
    "    mask = (df[\"Month\"] == 2) & (df[\"Day\"] == 29) & ~df[\"Year\"].apply(is_leap_year)\n",
    "    # If the mask is true for a row, set the value of the \"Day\" column to 28\n",
    "    df.loc[mask, \"Day\"] = 28\n",
    "\n",
    "    # Convert specified columns to numeric and remove 'Snow Depth' column\n",
    "    df = df[[\"Year\", \"Month\", \"Day\", \"Maximum T\", \"Minimum T\", \"Precipitation\", \"Snow\"]].apply(pd.to_numeric)\n",
    "\n",
    "    # Create a date column\n",
    "    df[\"Date\"] = pd.to_datetime(df[[\"Year\", \"Month\", \"Day\"]]).dt.date.astype(\"datetime64\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Clean the Weather Data Tables for each year\n",
    "df_2018 = prepare_weather_data(df_2018)\n",
    "df_2019 = prepare_weather_data(df_2019)\n",
    "df_2020 = prepare_weather_data(df_2020)\n",
    "df_2021 = prepare_weather_data(df_2021)\n",
    "df_2022 = prepare_weather_data(df_2022)\n",
    "df_2023 = prepare_weather_data(df_2023)\n",
    "\n",
    "\n",
    "# Concat all weather dataframes into one DataFrame\n",
    "weather_df = pd.concat([df_2018, df_2019, df_2020, df_2021, df_2022, df_2023], ignore_index=True)\n",
    "weather_df.sort_values('Date', inplace=True)\n",
    "print(weather_df['Date'].is_monotonic_increasing)\n",
    "\n",
    "len(df_2018) + len(df_2019) + len(df_2020) + len(df_2021) + len(df_2022) + len(df_2023)  == len(weather_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(df['Station_Name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_columns_to_datetime(df, date_columns):\n",
    "    for column in date_columns:\n",
    "        df[column] = df[column].apply(lambda x: pd.to_datetime(x, errors='coerce'))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Must have as a seperator ':'\n",
    "def date_columns_to_timedelta(time_str):\n",
    "    try:\n",
    "        hours, minutes, seconds = map(int, time_str.split(':'))\n",
    "        return pd.Timedelta(hours=hours, minutes=minutes, seconds=seconds)\n",
    "    except ValueError:\n",
    "        return pd.NaT  # Handle invalid time format  \n",
    "    \n",
    "# Make the columns datetimes and timedelta\n",
    "df1 = df.copy()\n",
    "\n",
    "date_columns_to_dt= ['Start_Date___Time','End_Date___Time']\n",
    "df1 = date_columns_to_datetime(df1, date_columns_to_dt)\n",
    "\n",
    "date_columns_to_td = ['Total_Duration__hh_mm_ss_','Charging_Time__hh_mm_ss_']\n",
    "for column in date_columns_to_td:\n",
    "    df1[column] = df1[column].apply(date_columns_to_timedelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Create a Date column to use it like a foreign key for the weather DataFrames\n",
    "df1[\"Date\"] = df1['Start_Date___Time'].dt.date.astype(\"datetime64\")\n",
    "\n",
    "# Sort increasing the dates per stations \n",
    "print(df1['Date'].is_monotonic_increasing)\n",
    "#print(df1.groupby('Station_Name')['Date'].apply(lambda x: x.is_monotonic_increasing).all())\n",
    "\n",
    "df1.sort_values(by='Date', inplace=True)\n",
    "print(df1['Date'].is_monotonic_increasing)\n",
    "#df1.sort_values(by=['Station_Name', 'Date'], inplace=True)\n",
    "#print(df1.groupby('Station_Name')['Date'].apply(lambda x: x.is_monotonic_increasing).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148136, 18)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(148136, 25)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the main Dataframe with the Weather Data\n",
    "print(df1.shape)\n",
    "df1 = df1.merge(weather_df, on='Date', how='left')\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We drop the 0.109 % of the Dataset.\n"
     ]
    }
   ],
   "source": [
    "def data_transformation(df):\n",
    "\n",
    "    # Convert inches to millimeters\n",
    "    df['Snow'] = df['Snow'] * 25.4\n",
    "    df['Precipitation'] = df['Precipitation'] * 25.4\n",
    "\n",
    "    # Make the Month and Day to categorical variables \n",
    "    df['Weekday'] = df['Date'].dt.day_name()\n",
    "    df['Month'] = df['Date'].dt.month_name()\n",
    "\n",
    "    # Remove null (0) values in Engery_kwh_\n",
    "    df = df[df['Energy__kWh_'] > 0]\n",
    "\n",
    "    # Change datatype from float to integer\n",
    "    df[[\"Year\", \"Day\"]] = df[[\"Year\", \"Day\"]].astype(np.int64)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df2 = df1.copy()\n",
    "df2 = data_transformation(df1)\n",
    "print('We drop the',round((df1.shape[0] - df2.shape[0]) / df1.shape[0],3),'% of the Dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame before the filtering:\n",
      "Start and End Datetime: 2018-01-01 00:00:00 2023-11-30 00:00:00 \n",
      "Length: 132023 \n",
      "\n",
      "DataFrame after the filtering:\n",
      "Start and End Datetime: 2018-01-01 00:00:00 2023-11-30 00:00:00 \n",
      "Length: 69061 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The paper approach duration (January 2018 to August 2022)\n",
    "print('DataFrame before the filtering:\\nStart and End Datetime:',df2.Date.min(), df2.Date.max(),'\\nLength:',len(df2),'\\n')\n",
    "#df2 = df2[df2['Date'] < '2022-08-01']\n",
    "df2['Date'] = df2.loc[:, 'Date'].apply(pd.to_datetime)\n",
    "\n",
    "columns = ['Energy__kWh_','Date','Day','Year','Weekday','Month','Minimum T','Maximum T','Snow','Precipitation']\n",
    "df2.drop_duplicates(subset=columns, keep='first', inplace=True)\n",
    "print('DataFrame after the filtering:\\nStart and End Datetime:',df2.Date.min(), df2.Date.max(),'\\nLength:',len(df2),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('../../Dataset/EntireBoulderWithWeather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
