{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station_Name</th>\n",
       "      <th>Transactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BOULDER / JUNCTION ST1</td>\n",
       "      <td>Energy__kWh_    Year    Month    W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BOULDER / ALPINE ST1</td>\n",
       "      <td>Energy__kWh_    Year     Month    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BOULDER / BASELINE ST1</td>\n",
       "      <td>Energy__kWh_    Year     Month    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BOULDER / FACILITIES ST1</td>\n",
       "      <td>Energy__kWh_    Year     Month    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COMM VITALITY / 1500PEARL2</td>\n",
       "      <td>Energy__kWh_    Year    Month    W...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Station_Name  \\\n",
       "0      BOULDER / JUNCTION ST1   \n",
       "1        BOULDER / ALPINE ST1   \n",
       "2      BOULDER / BASELINE ST1   \n",
       "3    BOULDER / FACILITIES ST1   \n",
       "4  COMM VITALITY / 1500PEARL2   \n",
       "\n",
       "                                        Transactions  \n",
       "0              Energy__kWh_    Year    Month    W...  \n",
       "1              Energy__kWh_    Year     Month    ...  \n",
       "2              Energy__kWh_    Year     Month    ...  \n",
       "3              Energy__kWh_    Year     Month    ...  \n",
       "4              Energy__kWh_    Year    Month    W...  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_df = pd.read_pickle('../../Dataset/MyMethod/DailyStations_Interpolated.pkl')\n",
    "stations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Energy__kWh_', 'Year', 'Month', 'Weekday', 'Maximum T', 'Minimum T',\n",
       "       'Precipitation', 'Snow', 'Charging_Time_min',\n",
       "       'Remaining_Park_After_Charge_(min)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_df.loc[1,'Transactions'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Energy__kWh_', 'Month', 'Weekday', 'Maximum T', 'Minimum T',\n",
       "       'Precipitation', 'Snow'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_df['Transactions'].apply(lambda row: row.drop(columns={'Year','Charging_Time_min', 'Remaining_Park_After_Charge_(min)'}, inplace=True))\n",
    "stations_df.loc[1,'Transactions'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2157 entries, 2018-01-03 to 2023-11-29\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Energy__kWh_   2157 non-null   float64\n",
      " 1   Month          2157 non-null   object \n",
      " 2   Weekday        2157 non-null   object \n",
      " 3   Maximum T      2157 non-null   float64\n",
      " 4   Minimum T      2157 non-null   float64\n",
      " 5   Precipitation  2157 non-null   float64\n",
      " 6   Snow           2157 non-null   float64\n",
      "dtypes: float64(5), object(2)\n",
      "memory usage: 134.8+ KB\n"
     ]
    }
   ],
   "source": [
    "stations_df.loc[1,'Transactions'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(row, categorical_columns, numerical_columns):\n",
    "\n",
    "    # One-hot encoding for categorical columns\n",
    "    all_dummy_columns = set()  # To store all unique dummy columns\n",
    "\n",
    "    # One-hot encoding for categorical columns\n",
    "    for col in categorical_columns:\n",
    "        row[\"Transactions\"] = pd.get_dummies(row[\"Transactions\"], columns=[col], prefix=col)\n",
    "        all_dummy_columns.update(row[\"Transactions\"].columns)\n",
    "    \n",
    "    # MinMax scaling for numerical columns\n",
    "    scaler = MinMaxScaler()\n",
    "    row[\"Transactions\"][numerical_columns] = scaler.fit_transform(row[\"Transactions\"][numerical_columns])\n",
    "\n",
    "    return row, all_dummy_columns, scaler\n",
    "\n",
    "\n",
    "categorical_columns = ['Weekday', 'Month']\n",
    "numerical_columns = ['Energy__kWh_', 'Minimum T', 'Maximum T', 'Snow', 'Precipitation']\n",
    "\n",
    "result = stations_df.apply(lambda row: preprocess_data(row, categorical_columns, numerical_columns), axis=1)\n",
    "\n",
    "# Unpack the result into two variables\n",
    "stations_df_scaled = result.apply(lambda x: x[0])\n",
    "all_dummy_columns = result.apply(lambda x: x[1]).iloc[0]\n",
    "scaler = result.apply(lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Month_April',\n",
       " 'Month_August',\n",
       " 'Month_December',\n",
       " 'Month_February',\n",
       " 'Month_January',\n",
       " 'Month_July',\n",
       " 'Month_June',\n",
       " 'Month_March',\n",
       " 'Month_May',\n",
       " 'Month_November',\n",
       " 'Month_October',\n",
       " 'Month_September',\n",
       " 'Precipitation',\n",
       " 'Weekday_Friday',\n",
       " 'Weekday_Monday',\n",
       " 'Weekday_Saturday',\n",
       " 'Weekday_Sunday',\n",
       " 'Weekday_Thursday',\n",
       " 'Weekday_Tuesday',\n",
       " 'Weekday_Wednesday'}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Items to remove\n",
    "items_to_remove = ['Weekday','Month', 'Snow', 'Energy__kWh_', 'Maximum T', 'Minimum T']\n",
    "\n",
    "for item in items_to_remove:\n",
    "    all_dummy_columns.discard(item)\n",
    "\n",
    "all_dummy_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 18)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_df_scaled.loc[0,'Transactions'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group_df in stations_df_scaled['Transactions']:\n",
    "    for dummy_col in all_dummy_columns:\n",
    "        if dummy_col not in group_df.columns:\n",
    "            group_df[dummy_col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 24)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_df_scaled.loc[0,'Transactions'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Energy__kWh_', 'Maximum T', 'Minimum T', 'Precipitation', 'Snow',\n",
       "       'Weekday_Friday', 'Weekday_Monday', 'Weekday_Saturday',\n",
       "       'Weekday_Sunday', 'Weekday_Thursday', 'Weekday_Tuesday',\n",
       "       'Weekday_Wednesday', 'Month_August', 'Month_July', 'Month_June',\n",
       "       'Month_November', 'Month_October', 'Month_September', 'Month_April',\n",
       "       'Month_January', 'Month_May', 'Month_March', 'Month_February',\n",
       "       'Month_December'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_df_scaled.loc[0,'Transactions'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Energy__kWh_</th>\n",
       "      <th>Maximum T</th>\n",
       "      <th>Minimum T</th>\n",
       "      <th>Precipitation</th>\n",
       "      <th>Snow</th>\n",
       "      <th>Weekday_Friday</th>\n",
       "      <th>Weekday_Monday</th>\n",
       "      <th>Weekday_Saturday</th>\n",
       "      <th>Weekday_Sunday</th>\n",
       "      <th>Weekday_Thursday</th>\n",
       "      <th>Weekday_Tuesday</th>\n",
       "      <th>Weekday_Wednesday</th>\n",
       "      <th>Month_August</th>\n",
       "      <th>Month_July</th>\n",
       "      <th>Month_June</th>\n",
       "      <th>Month_November</th>\n",
       "      <th>Month_October</th>\n",
       "      <th>Month_September</th>\n",
       "      <th>Month_April</th>\n",
       "      <th>Month_January</th>\n",
       "      <th>Month_May</th>\n",
       "      <th>Month_March</th>\n",
       "      <th>Month_February</th>\n",
       "      <th>Month_December</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-06-21</th>\n",
       "      <td>0.286377</td>\n",
       "      <td>0.291317</td>\n",
       "      <td>0.276371</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-22</th>\n",
       "      <td>0.574469</td>\n",
       "      <td>0.278711</td>\n",
       "      <td>0.289030</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-23</th>\n",
       "      <td>0.496137</td>\n",
       "      <td>0.378151</td>\n",
       "      <td>0.417722</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-24</th>\n",
       "      <td>0.441072</td>\n",
       "      <td>0.417367</td>\n",
       "      <td>0.417722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-25</th>\n",
       "      <td>0.206956</td>\n",
       "      <td>0.173669</td>\n",
       "      <td>0.156118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Energy__kWh_  Maximum T  Minimum T  Precipitation  Snow  \\\n",
       "2023-06-21      0.286377   0.291317   0.276371       0.264151   0.0   \n",
       "2023-06-22      0.574469   0.278711   0.289030       1.000000   0.0   \n",
       "2023-06-23      0.496137   0.378151   0.417722       0.075472   0.0   \n",
       "2023-06-24      0.441072   0.417367   0.417722       0.000000   0.0   \n",
       "2023-06-25      0.206956   0.173669   0.156118       0.000000   0.0   \n",
       "\n",
       "            Weekday_Friday  Weekday_Monday  Weekday_Saturday  Weekday_Sunday  \\\n",
       "2023-06-21               0               0                 0               0   \n",
       "2023-06-22               0               0                 0               0   \n",
       "2023-06-23               1               0                 0               0   \n",
       "2023-06-24               0               0                 1               0   \n",
       "2023-06-25               0               0                 0               1   \n",
       "\n",
       "            Weekday_Thursday  Weekday_Tuesday  Weekday_Wednesday  \\\n",
       "2023-06-21                 0                0                  1   \n",
       "2023-06-22                 1                0                  0   \n",
       "2023-06-23                 0                0                  0   \n",
       "2023-06-24                 0                0                  0   \n",
       "2023-06-25                 0                0                  0   \n",
       "\n",
       "            Month_August  Month_July  Month_June  Month_November  \\\n",
       "2023-06-21             0           0           1               0   \n",
       "2023-06-22             0           0           1               0   \n",
       "2023-06-23             0           0           1               0   \n",
       "2023-06-24             0           0           1               0   \n",
       "2023-06-25             0           0           1               0   \n",
       "\n",
       "            Month_October  Month_September  Month_April  Month_January  \\\n",
       "2023-06-21              0                0            0              0   \n",
       "2023-06-22              0                0            0              0   \n",
       "2023-06-23              0                0            0              0   \n",
       "2023-06-24              0                0            0              0   \n",
       "2023-06-25              0                0            0              0   \n",
       "\n",
       "            Month_May  Month_March  Month_February  Month_December  \n",
       "2023-06-21          0            0               0               0  \n",
       "2023-06-22          0            0               0               0  \n",
       "2023-06-23          0            0               0               0  \n",
       "2023-06-24          0            0               0               0  \n",
       "2023-06-25          0            0               0               0  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_df_scaled.loc[0,'Transactions'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training split ratio:   0.692\n",
      "Validation split ratio: 0.179\n",
      "Testing split ratio:    0.128\n",
      "\n",
      "Shapes of the datasets:\n",
      "(27, 2) (7, 2) (5, 2)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training, validation, and testing sets\n",
    "def split_dataset(df, train_ratio, val_ratio):\n",
    "\n",
    "    total_size = len(df)\n",
    "    train_size = int(train_ratio * total_size)\n",
    "    val_size = int(val_ratio * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    train_df = df[:train_size]\n",
    "    val_df = df[train_size:train_size + val_size]\n",
    "    test_df = df[train_size + val_size:]\n",
    "\n",
    "    assert len(train_df) + len(val_df) + len(test_df) == total_size, \"Dataset not split correctly.\"\n",
    "\n",
    "    print(f'Training split ratio:   {round(len(train_df) / len(df), 3)}')\n",
    "    print(f'Validation split ratio: {round(len(val_df) / len(df), 3)}')\n",
    "    print(f'Testing split ratio:    {round(len(test_df) / len(df), 3)}')\n",
    "    print(\"\\nShapes of the datasets:\")\n",
    "    print(train_df.shape, val_df.shape, test_df.shape)\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_daily_scaled, val_daily_scaled, test_daily_scaled = split_dataset(stations_df_scaled, train_ratio=0.7, val_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Energy__kWh_</th>\n",
       "      <th>Maximum T</th>\n",
       "      <th>Minimum T</th>\n",
       "      <th>Precipitation</th>\n",
       "      <th>Snow</th>\n",
       "      <th>Weekday_Friday</th>\n",
       "      <th>Weekday_Monday</th>\n",
       "      <th>Weekday_Saturday</th>\n",
       "      <th>Weekday_Sunday</th>\n",
       "      <th>Weekday_Thursday</th>\n",
       "      <th>Weekday_Tuesday</th>\n",
       "      <th>Weekday_Wednesday</th>\n",
       "      <th>Month_August</th>\n",
       "      <th>Month_July</th>\n",
       "      <th>Month_June</th>\n",
       "      <th>Month_November</th>\n",
       "      <th>Month_October</th>\n",
       "      <th>Month_September</th>\n",
       "      <th>Month_April</th>\n",
       "      <th>Month_January</th>\n",
       "      <th>Month_May</th>\n",
       "      <th>Month_March</th>\n",
       "      <th>Month_February</th>\n",
       "      <th>Month_December</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-06-21</th>\n",
       "      <td>0.286377</td>\n",
       "      <td>0.291317</td>\n",
       "      <td>0.276371</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-22</th>\n",
       "      <td>0.574469</td>\n",
       "      <td>0.278711</td>\n",
       "      <td>0.289030</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-23</th>\n",
       "      <td>0.496137</td>\n",
       "      <td>0.378151</td>\n",
       "      <td>0.417722</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-24</th>\n",
       "      <td>0.441072</td>\n",
       "      <td>0.417367</td>\n",
       "      <td>0.417722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-25</th>\n",
       "      <td>0.206956</td>\n",
       "      <td>0.173669</td>\n",
       "      <td>0.156118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Energy__kWh_  Maximum T  Minimum T  Precipitation  Snow  \\\n",
       "2023-06-21      0.286377   0.291317   0.276371       0.264151   0.0   \n",
       "2023-06-22      0.574469   0.278711   0.289030       1.000000   0.0   \n",
       "2023-06-23      0.496137   0.378151   0.417722       0.075472   0.0   \n",
       "2023-06-24      0.441072   0.417367   0.417722       0.000000   0.0   \n",
       "2023-06-25      0.206956   0.173669   0.156118       0.000000   0.0   \n",
       "\n",
       "            Weekday_Friday  Weekday_Monday  Weekday_Saturday  Weekday_Sunday  \\\n",
       "2023-06-21               0               0                 0               0   \n",
       "2023-06-22               0               0                 0               0   \n",
       "2023-06-23               1               0                 0               0   \n",
       "2023-06-24               0               0                 1               0   \n",
       "2023-06-25               0               0                 0               1   \n",
       "\n",
       "            Weekday_Thursday  Weekday_Tuesday  Weekday_Wednesday  \\\n",
       "2023-06-21                 0                0                  1   \n",
       "2023-06-22                 1                0                  0   \n",
       "2023-06-23                 0                0                  0   \n",
       "2023-06-24                 0                0                  0   \n",
       "2023-06-25                 0                0                  0   \n",
       "\n",
       "            Month_August  Month_July  Month_June  Month_November  \\\n",
       "2023-06-21             0           0           1               0   \n",
       "2023-06-22             0           0           1               0   \n",
       "2023-06-23             0           0           1               0   \n",
       "2023-06-24             0           0           1               0   \n",
       "2023-06-25             0           0           1               0   \n",
       "\n",
       "            Month_October  Month_September  Month_April  Month_January  \\\n",
       "2023-06-21              0                0            0              0   \n",
       "2023-06-22              0                0            0              0   \n",
       "2023-06-23              0                0            0              0   \n",
       "2023-06-24              0                0            0              0   \n",
       "2023-06-25              0                0            0              0   \n",
       "\n",
       "            Month_May  Month_March  Month_February  Month_December  \n",
       "2023-06-21          0            0               0               0  \n",
       "2023-06-22          0            0               0               0  \n",
       "2023-06-23          0            0               0               0  \n",
       "2023-06-24          0            0               0               0  \n",
       "2023-06-25          0            0               0               0  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_daily_scaled.loc[0,'Transactions'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences_targets(data, features, target_columns, sequence_length):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    for group_df in data:\n",
    "        group_df = group_df[features]\n",
    "\n",
    "        for t in range(0, len(group_df) - sequence_length):\n",
    "            sequence = group_df.iloc[t:t + sequence_length].values\n",
    "            target = group_df.iloc[t + sequence_length][target_columns].values\n",
    "\n",
    "            inputs.append(sequence)\n",
    "            targets.append(target)\n",
    "    \n",
    "    inputs_array = np.array(inputs)\n",
    "    targets_array = np.array(targets)\n",
    "\n",
    "    print(f'Dataset split into sequences:')\n",
    "    print(f'Sequences shape: {inputs_array.shape}')\n",
    "    print(f'Targets shape: {targets_array.shape}\\n')\n",
    "\n",
    "    return inputs_array, targets_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split into sequences:\n",
      "Sequences shape: (3753, 24, 24)\n",
      "Targets shape: (3753, 1)\n",
      "\n",
      "Dataset split into sequences:\n",
      "Sequences shape: (973, 24, 24)\n",
      "Targets shape: (973, 1)\n",
      "\n",
      "Dataset split into sequences:\n",
      "Sequences shape: (695, 24, 24)\n",
      "Targets shape: (695, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming stations_test is your DataFrame\n",
    "features = train_daily_scaled.loc[0,'Transactions'].columns.to_list()\n",
    "target_columns = ['Energy__kWh_']\n",
    "sequence_length = 24\n",
    "\n",
    "train_data_inputs, train_data_targets = generate_sequences_targets(train_daily_scaled['Transactions'], features, target_columns, sequence_length)\n",
    "val_data_inputs, val_data_targets = generate_sequences_targets(val_daily_scaled['Transactions'], features, target_columns, sequence_length)\n",
    "test_data_inputs, test_data_targets = generate_sequences_targets(test_daily_scaled['Transactions'], features, target_columns, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3753, 24, 24), (973, 24, 24), (695, 24, 24))"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = len(features)\n",
    "\n",
    "# The input Datasets must have this input shape (-1, sequence_length, num_features)\n",
    "train_data_inputs = train_data_inputs.reshape((-1, sequence_length, num_features))\n",
    "val_data_inputs = val_data_inputs.reshape((-1, sequence_length, num_features))\n",
    "test_data_inputs = test_data_inputs.reshape((-1, sequence_length, num_features))\n",
    "\n",
    "train_data_inputs.shape, val_data_inputs.shape, test_data_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 24, 24)]     0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention_95 (Multi  (None, None, 24)    2400        ['input_6[0][0]',                \n",
      " HeadAttention)                                                   'input_6[0][0]',                \n",
      "                                                                  'input_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_201 (Dropout)          (None, None, 24)     0           ['multi_head_attention_95[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_100 (TFOp  (None, 24, 24)      0           ['input_6[0][0]',                \n",
      " Lambda)                                                          'dropout_201[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_160 (Layer  (None, 24, 24)      48          ['tf.__operators__.add_100[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " positionwise_feed_forward_41 (  (None, 24, 24)      3160        ['layer_normalization_160[0][0]']\n",
      " PositionwiseFeedForward)                                                                         \n",
      "                                                                                                  \n",
      " dropout_203 (Dropout)          (None, 24, 24)       0           ['positionwise_feed_forward_41[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_101 (TFOp  (None, 24, 24)      0           ['layer_normalization_160[0][0]',\n",
      " Lambda)                                                          'dropout_203[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_161 (Layer  (None, 24, 24)      48          ['tf.__operators__.add_101[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " multi_head_attention_106 (Mult  (None, None, 24)    2400        ['layer_normalization_161[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_161[0][0]',\n",
      "                                                                  'layer_normalization_161[0][0]']\n",
      "                                                                                                  \n",
      " dropout_224 (Dropout)          (None, None, 24)     0           ['multi_head_attention_106[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_117 (TFOp  (None, 24, 24)      0           ['dropout_224[0][0]',            \n",
      " Lambda)                                                          'layer_normalization_161[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_177 (Layer  (None, 24, 24)      48          ['tf.__operators__.add_117[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " multi_head_attention_107 (Mult  (None, None, 24)    2400        ['layer_normalization_177[0][0]',\n",
      " iHeadAttention)                                                  'input_6[0][0]',                \n",
      "                                                                  'input_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_225 (Dropout)          (None, None, 24)     0           ['multi_head_attention_107[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_118 (TFOp  (None, None, 24)    0           ['dropout_224[0][0]',            \n",
      " Lambda)                                                          'dropout_225[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_178 (Layer  (None, None, 24)    48          ['tf.__operators__.add_118[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " positionwise_feed_forward_47 (  (None, None, 24)    3160        ['layer_normalization_178[0][0]']\n",
      " PositionwiseFeedForward)                                                                         \n",
      "                                                                                                  \n",
      " dropout_227 (Dropout)          (None, None, 24)     0           ['positionwise_feed_forward_47[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_119 (TFOp  (None, None, 24)    0           ['layer_normalization_178[0][0]',\n",
      " Lambda)                                                          'dropout_227[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_179 (Layer  (None, None, 24)    48          ['tf.__operators__.add_119[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " time_distributed_5 (TimeDistri  (None, None, 1)     25          ['layer_normalization_179[0][0]']\n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13,785\n",
      "Trainable params: 13,785\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "%run \"../Transformer_Paper/Transformer.ipynb\"\n",
    "\n",
    "# Define the hyperparameters of the manual model\n",
    "input_shape = (sequence_length, num_features)\n",
    "num_heads = 1\n",
    "d_ff = 64\n",
    "num_layers = 6\n",
    "dropout_rate = 0.1\n",
    "encoder_mask = None\n",
    "decoder_mask = tf.linalg.band_part(tf.ones((sequence_length, sequence_length)), -1, 0)  # Create a lower triangular mask\n",
    "decoder_mask = 1 - decoder_mask  # Invert the mask\n",
    "\n",
    "# Create the transformer model\n",
    "manul_model = TransformerModel(input_shape, num_heads, d_ff, num_layers, dropout_rate, encoder_mask, decoder_mask)\n",
    "\n",
    "manul_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 24, 24)]     0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention_108 (Mult  (None, 24, 24)      2400        ['input_7[0][0]',                \n",
      " iHeadAttention)                                                  'input_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_228 (Dropout)          (None, 24, 24)       0           ['multi_head_attention_108[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_180 (Layer  (None, 24, 24)      48          ['dropout_228[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_72 (Sequential)     (None, 24, 24)       3160        ['layer_normalization_180[0][0]']\n",
      "                                                                                                  \n",
      " dropout_229 (Dropout)          (None, 24, 24)       0           ['sequential_72[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_181 (Layer  (None, 24, 24)      48          ['dropout_229[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_109 (Mult  (None, 24, 24)      2400        ['layer_normalization_181[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_181[0][0]']\n",
      "                                                                                                  \n",
      " dropout_230 (Dropout)          (None, 24, 24)       0           ['multi_head_attention_109[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_182 (Layer  (None, 24, 24)      48          ['dropout_230[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_73 (Sequential)     (None, 24, 24)       3160        ['layer_normalization_182[0][0]']\n",
      "                                                                                                  \n",
      " dropout_231 (Dropout)          (None, 24, 24)       0           ['sequential_73[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_183 (Layer  (None, 24, 24)      48          ['dropout_231[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_110 (Mult  (None, 24, 24)      2400        ['layer_normalization_183[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_183[0][0]']\n",
      "                                                                                                  \n",
      " dropout_232 (Dropout)          (None, 24, 24)       0           ['multi_head_attention_110[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_184 (Layer  (None, 24, 24)      48          ['dropout_232[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_74 (Sequential)     (None, 24, 24)       3160        ['layer_normalization_184[0][0]']\n",
      "                                                                                                  \n",
      " dropout_233 (Dropout)          (None, 24, 24)       0           ['sequential_74[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_185 (Layer  (None, 24, 24)      48          ['dropout_233[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_111 (Mult  (None, 24, 24)      2400        ['layer_normalization_185[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_185[0][0]']\n",
      "                                                                                                  \n",
      " dropout_234 (Dropout)          (None, 24, 24)       0           ['multi_head_attention_111[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_186 (Layer  (None, 24, 24)      48          ['dropout_234[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_75 (Sequential)     (None, 24, 24)       3160        ['layer_normalization_186[0][0]']\n",
      "                                                                                                  \n",
      " dropout_235 (Dropout)          (None, 24, 24)       0           ['sequential_75[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_187 (Layer  (None, 24, 24)      48          ['dropout_235[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_112 (Mult  (None, 24, 24)      2400        ['layer_normalization_187[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_187[0][0]']\n",
      "                                                                                                  \n",
      " dropout_236 (Dropout)          (None, 24, 24)       0           ['multi_head_attention_112[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_188 (Layer  (None, 24, 24)      48          ['dropout_236[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_76 (Sequential)     (None, 24, 24)       3160        ['layer_normalization_188[0][0]']\n",
      "                                                                                                  \n",
      " dropout_237 (Dropout)          (None, 24, 24)       0           ['sequential_76[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_189 (Layer  (None, 24, 24)      48          ['dropout_237[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_113 (Mult  (None, 24, 24)      2400        ['layer_normalization_189[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_189[0][0]']\n",
      "                                                                                                  \n",
      " dropout_238 (Dropout)          (None, 24, 24)       0           ['multi_head_attention_113[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_190 (Layer  (None, 24, 24)      48          ['dropout_238[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_77 (Sequential)     (None, 24, 24)       3160        ['layer_normalization_190[0][0]']\n",
      "                                                                                                  \n",
      " dropout_239 (Dropout)          (None, 24, 24)       0           ['sequential_77[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_191 (Layer  (None, 24, 24)      48          ['dropout_239[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_114 (Mult  (None, 24, 24)      2400        ['layer_normalization_191[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_191[0][0]']\n",
      "                                                                                                  \n",
      " dropout_240 (Dropout)          (None, 24, 24)       0           ['multi_head_attention_114[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_192 (Layer  (None, 24, 24)      48          ['dropout_240[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_115 (Mult  (None, 24, 24)      2400        ['layer_normalization_192[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_191[0][0]']\n",
      "                                                                                                  \n",
      " dropout_241 (Dropout)          (None, 24, 24)       0           ['multi_head_attention_115[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_193 (Layer  (None, 24, 24)      48          ['dropout_241[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_78 (Sequential)     (None, 24, 24)       3160        ['layer_normalization_193[0][0]']\n",
      "                                                                                                  \n",
      " dropout_242 (Dropout)          (None, 24, 24)       0           ['sequential_78[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_194 (Layer  (None, 24, 24)      48          ['dropout_242[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_116 (Mult  (None, 24, 24)      2400        ['layer_normalization_194[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_194[0][0]']\n",
      "                                                                                                  \n",
      " dropout_243 (Dropout)          (None, 24, 24)       0           ['multi_head_attention_116[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_195 (Layer  (None, 24, 24)      48          ['dropout_243[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_117 (Mult  (None, 24, 24)      2400        ['layer_normalization_195[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_191[0][0]']\n",
      "                                                                                                  \n",
      " dropout_244 (Dropout)          (None, 24, 24)       0           ['multi_head_attention_117[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_196 (Layer  (None, 24, 24)      48          ['dropout_244[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_79 (Sequential)     (None, 24, 24)       3160        ['layer_normalization_196[0][0]']\n",
      "                                                                                                  \n",
      " dropout_245 (Dropout)          (None, 24, 24)       0           ['sequential_79[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_197 (Layer  (None, 24, 24)      48          ['dropout_245[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_118 (Mult  (None, 24, 24)      2400        ['layer_normalization_197[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_197[0][0]']\n",
      "                                                                                                  \n",
      " dropout_246 (Dropout)          (None, 24, 24)       0           ['multi_head_attention_118[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_198 (Layer  (None, 24, 24)      48          ['dropout_246[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_119 (Mult  (None, 24, 24)      2400        ['layer_normalization_198[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_191[0][0]']\n",
      "                                                                                                  \n",
      " dropout_247 (Dropout)          (None, 24, 24)       0           ['multi_head_attention_119[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_199 (Layer  (None, 24, 24)      48          ['dropout_247[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_80 (Sequential)     (None, 24, 24)       3160        ['layer_normalization_199[0][0]']\n",
      "                                                                                                  \n",
      " dropout_248 (Dropout)          (None, 24, 24)       0           ['sequential_80[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_200 (Layer  (None, 24, 24)      48          ['dropout_248[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_120 (Mult  (None, 24, 24)      2400        ['layer_normalization_200[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_200[0][0]']\n",
      "                                                                                                  \n",
      " dropout_249 (Dropout)          (None, 24, 24)       0           ['multi_head_attention_120[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_201 (Layer  (None, 24, 24)      48          ['dropout_249[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_121 (Mult  (None, 24, 24)      2400        ['layer_normalization_201[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_191[0][0]']\n",
      "                                                                                                  \n",
      " dropout_250 (Dropout)          (None, 24, 24)       0           ['multi_head_attention_121[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_202 (Layer  (None, 24, 24)      48          ['dropout_250[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_81 (Sequential)     (None, 24, 24)       3160        ['layer_normalization_202[0][0]']\n",
      "                                                                                                  \n",
      " dropout_251 (Dropout)          (None, 24, 24)       0           ['sequential_81[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_203 (Layer  (None, 24, 24)      48          ['dropout_251[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_122 (Mult  (None, 24, 24)      2400        ['layer_normalization_203[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_203[0][0]']\n",
      "                                                                                                  \n",
      " dropout_252 (Dropout)          (None, 24, 24)       0           ['multi_head_attention_122[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_204 (Layer  (None, 24, 24)      48          ['dropout_252[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_123 (Mult  (None, 24, 24)      2400        ['layer_normalization_204[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_191[0][0]']\n",
      "                                                                                                  \n",
      " dropout_253 (Dropout)          (None, 24, 24)       0           ['multi_head_attention_123[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_205 (Layer  (None, 24, 24)      48          ['dropout_253[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_82 (Sequential)     (None, 24, 24)       3160        ['layer_normalization_205[0][0]']\n",
      "                                                                                                  \n",
      " dropout_254 (Dropout)          (None, 24, 24)       0           ['sequential_82[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_206 (Layer  (None, 24, 24)      48          ['dropout_254[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_124 (Mult  (None, 24, 24)      2400        ['layer_normalization_206[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_206[0][0]']\n",
      "                                                                                                  \n",
      " dropout_255 (Dropout)          (None, 24, 24)       0           ['multi_head_attention_124[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_207 (Layer  (None, 24, 24)      48          ['dropout_255[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_125 (Mult  (None, 24, 24)      2400        ['layer_normalization_207[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_191[0][0]']\n",
      "                                                                                                  \n",
      " dropout_256 (Dropout)          (None, 24, 24)       0           ['multi_head_attention_125[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_208 (Layer  (None, 24, 24)      48          ['dropout_256[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_83 (Sequential)     (None, 24, 24)       3160        ['layer_normalization_208[0][0]']\n",
      "                                                                                                  \n",
      " dropout_257 (Dropout)          (None, 24, 24)       0           ['sequential_83[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_209 (Layer  (None, 24, 24)      48          ['dropout_257[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " time_distributed_6 (TimeDistri  (None, 24, 1)       25          ['layer_normalization_209[0][0]']\n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 82,585\n",
      "Trainable params: 82,585\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the transformer model\n",
    "keras_model = keras_transformer_model(input_shape, num_heads, d_ff, num_layers, dropout_rate)\n",
    "\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return tf.keras.backend.sqrt(\n",
    "        tf.keras.backend.mean(\n",
    "            tf.keras.backend.square(\n",
    "                y_pred - y_true\n",
    "            )\n",
    "        ) + 1e-9\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate for Adam optimizer\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Compile the manual model\n",
    "manul_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss='mse',  metrics=['mae', 'mse', root_mean_squared_error])\n",
    "\n",
    "# Compile the keras model\n",
    "keras_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss='mse',  metrics=['mae', 'mse', root_mean_squared_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3753, 24, 24), (3753, 1), (973, 24, 24), (973, 1))"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the parameters for training\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "# Convert the data to float32\n",
    "train_data_inputs = train_data_inputs.astype('float32')\n",
    "train_data_targets = train_data_targets.astype('float32')\n",
    "\n",
    "val_data_inputs = val_data_inputs.astype('float32')\n",
    "val_data_targets = val_data_targets.astype('float32')\n",
    "\n",
    "train_data_inputs.shape, train_data_targets.shape, val_data_inputs.shape, val_data_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 10s 31ms/step - loss: 0.1920 - mae: 0.2764 - mse: 0.1920 - root_mean_squared_error: 0.3390 - val_loss: 0.0358 - val_mae: 0.1559 - val_mse: 0.0358 - val_root_mean_squared_error: 0.1868\n",
      "Epoch 2/50\n",
      "118/118 [==============================] - 2s 19ms/step - loss: 0.0367 - mae: 0.1441 - mse: 0.0367 - root_mean_squared_error: 0.1889 - val_loss: 0.0307 - val_mae: 0.1323 - val_mse: 0.0307 - val_root_mean_squared_error: 0.1720\n",
      "Epoch 3/50\n",
      "118/118 [==============================] - 2s 19ms/step - loss: 0.0328 - mae: 0.1343 - mse: 0.0328 - root_mean_squared_error: 0.1788 - val_loss: 0.0315 - val_mae: 0.1392 - val_mse: 0.0315 - val_root_mean_squared_error: 0.1746\n",
      "Epoch 4/50\n",
      "118/118 [==============================] - 2s 20ms/step - loss: 0.0318 - mae: 0.1321 - mse: 0.0318 - root_mean_squared_error: 0.1750 - val_loss: 0.0302 - val_mae: 0.1350 - val_mse: 0.0302 - val_root_mean_squared_error: 0.1707\n",
      "Epoch 5/50\n",
      "118/118 [==============================] - 2s 18ms/step - loss: 0.0303 - mae: 0.1287 - mse: 0.0303 - root_mean_squared_error: 0.1716 - val_loss: 0.0296 - val_mae: 0.1340 - val_mse: 0.0296 - val_root_mean_squared_error: 0.1690\n",
      "Epoch 6/50\n",
      "118/118 [==============================] - 2s 17ms/step - loss: 0.0300 - mae: 0.1280 - mse: 0.0300 - root_mean_squared_error: 0.1701 - val_loss: 0.0281 - val_mae: 0.1210 - val_mse: 0.0281 - val_root_mean_squared_error: 0.1639\n",
      "Epoch 7/50\n",
      "118/118 [==============================] - 2s 18ms/step - loss: 0.0303 - mae: 0.1289 - mse: 0.0303 - root_mean_squared_error: 0.1710 - val_loss: 0.0324 - val_mae: 0.1467 - val_mse: 0.0324 - val_root_mean_squared_error: 0.1774\n",
      "Epoch 8/50\n",
      "118/118 [==============================] - 2s 17ms/step - loss: 0.0297 - mae: 0.1268 - mse: 0.0297 - root_mean_squared_error: 0.1691 - val_loss: 0.0278 - val_mae: 0.1255 - val_mse: 0.0278 - val_root_mean_squared_error: 0.1634\n",
      "Epoch 9/50\n",
      "118/118 [==============================] - 2s 17ms/step - loss: 0.0293 - mae: 0.1273 - mse: 0.0293 - root_mean_squared_error: 0.1685 - val_loss: 0.0274 - val_mae: 0.1191 - val_mse: 0.0274 - val_root_mean_squared_error: 0.1619\n",
      "Epoch 10/50\n",
      "118/118 [==============================] - 2s 17ms/step - loss: 0.0289 - mae: 0.1248 - mse: 0.0289 - root_mean_squared_error: 0.1676 - val_loss: 0.0271 - val_mae: 0.1258 - val_mse: 0.0271 - val_root_mean_squared_error: 0.1618\n",
      "Epoch 11/50\n",
      "118/118 [==============================] - 2s 16ms/step - loss: 0.0298 - mae: 0.1274 - mse: 0.0298 - root_mean_squared_error: 0.1704 - val_loss: 0.0294 - val_mae: 0.1350 - val_mse: 0.0294 - val_root_mean_squared_error: 0.1689\n",
      "Epoch 12/50\n",
      "118/118 [==============================] - 2s 17ms/step - loss: 0.0292 - mae: 0.1265 - mse: 0.0292 - root_mean_squared_error: 0.1690 - val_loss: 0.0286 - val_mae: 0.1333 - val_mse: 0.0286 - val_root_mean_squared_error: 0.1667\n",
      "Epoch 13/50\n",
      "118/118 [==============================] - 2s 16ms/step - loss: 0.0273 - mae: 0.1220 - mse: 0.0273 - root_mean_squared_error: 0.1626 - val_loss: 0.0256 - val_mae: 0.1152 - val_mse: 0.0256 - val_root_mean_squared_error: 0.1568\n",
      "Epoch 14/50\n",
      "118/118 [==============================] - 2s 16ms/step - loss: 0.0267 - mae: 0.1200 - mse: 0.0267 - root_mean_squared_error: 0.1616 - val_loss: 0.0276 - val_mae: 0.1285 - val_mse: 0.0276 - val_root_mean_squared_error: 0.1626\n",
      "Epoch 15/50\n",
      "118/118 [==============================] - 2s 16ms/step - loss: 0.0263 - mae: 0.1195 - mse: 0.0263 - root_mean_squared_error: 0.1595 - val_loss: 0.0251 - val_mae: 0.1218 - val_mse: 0.0251 - val_root_mean_squared_error: 0.1558\n",
      "Epoch 16/50\n",
      "118/118 [==============================] - 2s 15ms/step - loss: 0.0243 - mae: 0.1156 - mse: 0.0243 - root_mean_squared_error: 0.1536 - val_loss: 0.0226 - val_mae: 0.1122 - val_mse: 0.0226 - val_root_mean_squared_error: 0.1474\n",
      "Epoch 17/50\n",
      "118/118 [==============================] - 2s 16ms/step - loss: 0.0251 - mae: 0.1174 - mse: 0.0251 - root_mean_squared_error: 0.1560 - val_loss: 0.0228 - val_mae: 0.1115 - val_mse: 0.0228 - val_root_mean_squared_error: 0.1479\n",
      "Epoch 18/50\n",
      "118/118 [==============================] - 2s 15ms/step - loss: 0.0243 - mae: 0.1157 - mse: 0.0243 - root_mean_squared_error: 0.1541 - val_loss: 0.0213 - val_mae: 0.1085 - val_mse: 0.0213 - val_root_mean_squared_error: 0.1431\n",
      "Epoch 19/50\n",
      "118/118 [==============================] - 2s 16ms/step - loss: 0.0221 - mae: 0.1122 - mse: 0.0221 - root_mean_squared_error: 0.1469 - val_loss: 0.0275 - val_mae: 0.1371 - val_mse: 0.0275 - val_root_mean_squared_error: 0.1642\n",
      "Epoch 20/50\n",
      "118/118 [==============================] - 2s 16ms/step - loss: 0.0208 - mae: 0.1091 - mse: 0.0208 - root_mean_squared_error: 0.1420 - val_loss: 0.0182 - val_mae: 0.1008 - val_mse: 0.0182 - val_root_mean_squared_error: 0.1326\n",
      "Epoch 21/50\n",
      "118/118 [==============================] - 2s 16ms/step - loss: 0.0191 - mae: 0.1043 - mse: 0.0191 - root_mean_squared_error: 0.1357 - val_loss: 0.0170 - val_mae: 0.0957 - val_mse: 0.0170 - val_root_mean_squared_error: 0.1278\n",
      "Epoch 22/50\n",
      "118/118 [==============================] - 2s 16ms/step - loss: 0.0180 - mae: 0.1004 - mse: 0.0180 - root_mean_squared_error: 0.1321 - val_loss: 0.0167 - val_mae: 0.0954 - val_mse: 0.0167 - val_root_mean_squared_error: 0.1268\n",
      "Epoch 23/50\n",
      "118/118 [==============================] - 2s 15ms/step - loss: 0.0176 - mae: 0.0995 - mse: 0.0176 - root_mean_squared_error: 0.1302 - val_loss: 0.0168 - val_mae: 0.0988 - val_mse: 0.0168 - val_root_mean_squared_error: 0.1271\n",
      "Epoch 24/50\n",
      "118/118 [==============================] - 2s 16ms/step - loss: 0.0170 - mae: 0.0965 - mse: 0.0170 - root_mean_squared_error: 0.1284 - val_loss: 0.0158 - val_mae: 0.0939 - val_mse: 0.0158 - val_root_mean_squared_error: 0.1228\n",
      "Epoch 25/50\n",
      "118/118 [==============================] - 2s 16ms/step - loss: 0.0169 - mae: 0.0970 - mse: 0.0169 - root_mean_squared_error: 0.1278 - val_loss: 0.0151 - val_mae: 0.0868 - val_mse: 0.0151 - val_root_mean_squared_error: 0.1198\n",
      "Epoch 26/50\n",
      "118/118 [==============================] - 2s 16ms/step - loss: 0.0172 - mae: 0.0966 - mse: 0.0172 - root_mean_squared_error: 0.1292 - val_loss: 0.0152 - val_mae: 0.0925 - val_mse: 0.0152 - val_root_mean_squared_error: 0.1203\n",
      "Epoch 27/50\n",
      "118/118 [==============================] - 2s 17ms/step - loss: 0.0162 - mae: 0.0937 - mse: 0.0162 - root_mean_squared_error: 0.1247 - val_loss: 0.0145 - val_mae: 0.0885 - val_mse: 0.0145 - val_root_mean_squared_error: 0.1173\n",
      "Epoch 28/50\n",
      "118/118 [==============================] - 2s 18ms/step - loss: 0.0164 - mae: 0.0939 - mse: 0.0164 - root_mean_squared_error: 0.1253 - val_loss: 0.0263 - val_mae: 0.1320 - val_mse: 0.0263 - val_root_mean_squared_error: 0.1600\n",
      "Epoch 29/50\n",
      "118/118 [==============================] - 2s 16ms/step - loss: 0.0155 - mae: 0.0909 - mse: 0.0155 - root_mean_squared_error: 0.1215 - val_loss: 0.0134 - val_mae: 0.0811 - val_mse: 0.0134 - val_root_mean_squared_error: 0.1119\n",
      "Epoch 30/50\n",
      "118/118 [==============================] - 2s 17ms/step - loss: 0.0151 - mae: 0.0890 - mse: 0.0151 - root_mean_squared_error: 0.1199 - val_loss: 0.0142 - val_mae: 0.0860 - val_mse: 0.0142 - val_root_mean_squared_error: 0.1159\n",
      "Epoch 31/50\n",
      "118/118 [==============================] - 2s 16ms/step - loss: 0.0143 - mae: 0.0859 - mse: 0.0143 - root_mean_squared_error: 0.1166 - val_loss: 0.0133 - val_mae: 0.0804 - val_mse: 0.0133 - val_root_mean_squared_error: 0.1112\n",
      "Epoch 32/50\n",
      "118/118 [==============================] - 2s 17ms/step - loss: 0.0141 - mae: 0.0859 - mse: 0.0141 - root_mean_squared_error: 0.1160 - val_loss: 0.0140 - val_mae: 0.0838 - val_mse: 0.0140 - val_root_mean_squared_error: 0.1148\n",
      "Epoch 33/50\n",
      "118/118 [==============================] - 2s 17ms/step - loss: 0.0138 - mae: 0.0850 - mse: 0.0138 - root_mean_squared_error: 0.1151 - val_loss: 0.0128 - val_mae: 0.0857 - val_mse: 0.0128 - val_root_mean_squared_error: 0.1095\n",
      "Epoch 34/50\n",
      "118/118 [==============================] - 2s 18ms/step - loss: 0.0125 - mae: 0.0800 - mse: 0.0125 - root_mean_squared_error: 0.1090 - val_loss: 0.0127 - val_mae: 0.0831 - val_mse: 0.0127 - val_root_mean_squared_error: 0.1077\n",
      "Epoch 35/50\n",
      "118/118 [==============================] - 2s 16ms/step - loss: 0.0122 - mae: 0.0786 - mse: 0.0122 - root_mean_squared_error: 0.1083 - val_loss: 0.0137 - val_mae: 0.0900 - val_mse: 0.0137 - val_root_mean_squared_error: 0.1137\n",
      "Epoch 36/50\n",
      "118/118 [==============================] - 2s 20ms/step - loss: 0.0122 - mae: 0.0788 - mse: 0.0122 - root_mean_squared_error: 0.1072 - val_loss: 0.0119 - val_mae: 0.0725 - val_mse: 0.0119 - val_root_mean_squared_error: 0.1041\n",
      "Epoch 37/50\n",
      "118/118 [==============================] - 2s 21ms/step - loss: 0.0119 - mae: 0.0772 - mse: 0.0119 - root_mean_squared_error: 0.1061 - val_loss: 0.0114 - val_mae: 0.0781 - val_mse: 0.0114 - val_root_mean_squared_error: 0.1025\n",
      "Epoch 38/50\n",
      "118/118 [==============================] - 2s 18ms/step - loss: 0.0117 - mae: 0.0763 - mse: 0.0117 - root_mean_squared_error: 0.1050 - val_loss: 0.0118 - val_mae: 0.0791 - val_mse: 0.0118 - val_root_mean_squared_error: 0.1049\n",
      "Epoch 39/50\n",
      "118/118 [==============================] - 2s 17ms/step - loss: 0.0121 - mae: 0.0787 - mse: 0.0121 - root_mean_squared_error: 0.1068 - val_loss: 0.0109 - val_mae: 0.0735 - val_mse: 0.0109 - val_root_mean_squared_error: 0.0992\n",
      "Epoch 40/50\n",
      "118/118 [==============================] - 2s 17ms/step - loss: 0.0118 - mae: 0.0763 - mse: 0.0118 - root_mean_squared_error: 0.1063 - val_loss: 0.0110 - val_mae: 0.0738 - val_mse: 0.0110 - val_root_mean_squared_error: 0.0997\n",
      "Epoch 41/50\n",
      "118/118 [==============================] - 2s 17ms/step - loss: 0.0118 - mae: 0.0769 - mse: 0.0118 - root_mean_squared_error: 0.1063 - val_loss: 0.0108 - val_mae: 0.0745 - val_mse: 0.0108 - val_root_mean_squared_error: 0.0991\n",
      "Epoch 42/50\n",
      "118/118 [==============================] - 2s 17ms/step - loss: 0.0114 - mae: 0.0751 - mse: 0.0114 - root_mean_squared_error: 0.1050 - val_loss: 0.0103 - val_mae: 0.0721 - val_mse: 0.0103 - val_root_mean_squared_error: 0.0977\n",
      "Epoch 43/50\n",
      "118/118 [==============================] - 2s 17ms/step - loss: 0.0108 - mae: 0.0728 - mse: 0.0108 - root_mean_squared_error: 0.1008 - val_loss: 0.0108 - val_mae: 0.0712 - val_mse: 0.0108 - val_root_mean_squared_error: 0.0993\n",
      "Epoch 44/50\n",
      "118/118 [==============================] - 2s 17ms/step - loss: 0.0112 - mae: 0.0741 - mse: 0.0112 - root_mean_squared_error: 0.1030 - val_loss: 0.0104 - val_mae: 0.0757 - val_mse: 0.0104 - val_root_mean_squared_error: 0.0986\n",
      "Epoch 45/50\n",
      "118/118 [==============================] - 2s 17ms/step - loss: 0.0108 - mae: 0.0733 - mse: 0.0108 - root_mean_squared_error: 0.1017 - val_loss: 0.0112 - val_mae: 0.0737 - val_mse: 0.0112 - val_root_mean_squared_error: 0.1029\n",
      "Epoch 46/50\n",
      "118/118 [==============================] - 2s 18ms/step - loss: 0.0109 - mae: 0.0735 - mse: 0.0109 - root_mean_squared_error: 0.1020 - val_loss: 0.0098 - val_mae: 0.0722 - val_mse: 0.0098 - val_root_mean_squared_error: 0.0948\n",
      "Epoch 47/50\n",
      "118/118 [==============================] - 2s 16ms/step - loss: 0.0107 - mae: 0.0731 - mse: 0.0107 - root_mean_squared_error: 0.1014 - val_loss: 0.0094 - val_mae: 0.0664 - val_mse: 0.0094 - val_root_mean_squared_error: 0.0947\n",
      "Epoch 48/50\n",
      "118/118 [==============================] - 2s 17ms/step - loss: 0.0125 - mae: 0.0796 - mse: 0.0125 - root_mean_squared_error: 0.1092 - val_loss: 0.0103 - val_mae: 0.0705 - val_mse: 0.0103 - val_root_mean_squared_error: 0.0985\n",
      "Epoch 49/50\n",
      "118/118 [==============================] - 2s 18ms/step - loss: 0.0116 - mae: 0.0769 - mse: 0.0116 - root_mean_squared_error: 0.1057 - val_loss: 0.0098 - val_mae: 0.0723 - val_mse: 0.0098 - val_root_mean_squared_error: 0.0960\n",
      "Epoch 50/50\n",
      "118/118 [==============================] - 2s 16ms/step - loss: 0.0098 - mae: 0.0709 - mse: 0.0098 - root_mean_squared_error: 0.0970 - val_loss: 0.0084 - val_mae: 0.0631 - val_mse: 0.0084 - val_root_mean_squared_error: 0.0882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2344ed79f40>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the manual model\n",
    "manul_model.fit(train_data_inputs, train_data_targets,\n",
    "          validation_data=(val_data_inputs, val_data_targets),\n",
    "          epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "118/118 [==============================] - 38s 117ms/step - loss: 0.1891 - mae: 0.2485 - mse: 0.1891 - root_mean_squared_error: 0.3016 - val_loss: 0.0337 - val_mae: 0.1450 - val_mse: 0.0337 - val_root_mean_squared_error: 0.1807\n",
      "Epoch 2/50\n",
      "118/118 [==============================] - 10s 87ms/step - loss: 0.0348 - mae: 0.1400 - mse: 0.0348 - root_mean_squared_error: 0.1831 - val_loss: 0.0328 - val_mae: 0.1291 - val_mse: 0.0328 - val_root_mean_squared_error: 0.1768\n",
      "Epoch 3/50\n",
      "118/118 [==============================] - 12s 99ms/step - loss: 0.0337 - mae: 0.1369 - mse: 0.0337 - root_mean_squared_error: 0.1805 - val_loss: 0.0318 - val_mae: 0.1324 - val_mse: 0.0318 - val_root_mean_squared_error: 0.1749\n",
      "Epoch 4/50\n",
      "118/118 [==============================] - 11s 90ms/step - loss: 0.0332 - mae: 0.1360 - mse: 0.0332 - root_mean_squared_error: 0.1802 - val_loss: 0.0319 - val_mae: 0.1338 - val_mse: 0.0319 - val_root_mean_squared_error: 0.1751\n",
      "Epoch 5/50\n",
      "118/118 [==============================] - 10s 83ms/step - loss: 0.0334 - mae: 0.1364 - mse: 0.0334 - root_mean_squared_error: 0.1799 - val_loss: 0.0321 - val_mae: 0.1299 - val_mse: 0.0321 - val_root_mean_squared_error: 0.1754\n",
      "Epoch 6/50\n",
      "118/118 [==============================] - 10s 83ms/step - loss: 0.0333 - mae: 0.1362 - mse: 0.0333 - root_mean_squared_error: 0.1781 - val_loss: 0.0319 - val_mae: 0.1348 - val_mse: 0.0319 - val_root_mean_squared_error: 0.1754\n",
      "Epoch 7/50\n",
      "118/118 [==============================] - 10s 83ms/step - loss: 0.0331 - mae: 0.1365 - mse: 0.0331 - root_mean_squared_error: 0.1795 - val_loss: 0.0319 - val_mae: 0.1309 - val_mse: 0.0319 - val_root_mean_squared_error: 0.1749\n",
      "Epoch 8/50\n",
      "118/118 [==============================] - 10s 83ms/step - loss: 0.0326 - mae: 0.1347 - mse: 0.0326 - root_mean_squared_error: 0.1791 - val_loss: 0.0320 - val_mae: 0.1354 - val_mse: 0.0320 - val_root_mean_squared_error: 0.1756\n",
      "Epoch 9/50\n",
      "118/118 [==============================] - 10s 83ms/step - loss: 0.0330 - mae: 0.1354 - mse: 0.0330 - root_mean_squared_error: 0.1778 - val_loss: 0.0319 - val_mae: 0.1311 - val_mse: 0.0319 - val_root_mean_squared_error: 0.1749\n",
      "Epoch 10/50\n",
      "118/118 [==============================] - 10s 82ms/step - loss: 0.0334 - mae: 0.1360 - mse: 0.0334 - root_mean_squared_error: 0.1791 - val_loss: 0.0352 - val_mae: 0.1516 - val_mse: 0.0352 - val_root_mean_squared_error: 0.1851\n",
      "Epoch 11/50\n",
      "118/118 [==============================] - 11s 92ms/step - loss: 0.0337 - mae: 0.1369 - mse: 0.0337 - root_mean_squared_error: 0.1806 - val_loss: 0.0324 - val_mae: 0.1293 - val_mse: 0.0324 - val_root_mean_squared_error: 0.1760\n",
      "Epoch 12/50\n",
      "118/118 [==============================] - 11s 90ms/step - loss: 0.0342 - mae: 0.1382 - mse: 0.0342 - root_mean_squared_error: 0.1814 - val_loss: 0.0319 - val_mae: 0.1307 - val_mse: 0.0319 - val_root_mean_squared_error: 0.1750\n",
      "Epoch 13/50\n",
      "118/118 [==============================] - 11s 91ms/step - loss: 0.0326 - mae: 0.1347 - mse: 0.0326 - root_mean_squared_error: 0.1785 - val_loss: 0.0319 - val_mae: 0.1313 - val_mse: 0.0319 - val_root_mean_squared_error: 0.1749\n",
      "Epoch 14/50\n",
      "118/118 [==============================] - 11s 90ms/step - loss: 0.0328 - mae: 0.1350 - mse: 0.0328 - root_mean_squared_error: 0.1774 - val_loss: 0.0331 - val_mae: 0.1422 - val_mse: 0.0331 - val_root_mean_squared_error: 0.1790\n",
      "Epoch 15/50\n",
      "118/118 [==============================] - 10s 88ms/step - loss: 0.0329 - mae: 0.1352 - mse: 0.0329 - root_mean_squared_error: 0.1777 - val_loss: 0.0323 - val_mae: 0.1381 - val_mse: 0.0323 - val_root_mean_squared_error: 0.1767\n",
      "Epoch 16/50\n",
      "118/118 [==============================] - 10s 87ms/step - loss: 0.0334 - mae: 0.1360 - mse: 0.0334 - root_mean_squared_error: 0.1793 - val_loss: 0.0319 - val_mae: 0.1312 - val_mse: 0.0319 - val_root_mean_squared_error: 0.1749\n",
      "Epoch 17/50\n",
      "118/118 [==============================] - 10s 88ms/step - loss: 0.0323 - mae: 0.1335 - mse: 0.0323 - root_mean_squared_error: 0.1759 - val_loss: 0.0319 - val_mae: 0.1335 - val_mse: 0.0319 - val_root_mean_squared_error: 0.1750\n",
      "Epoch 18/50\n",
      "118/118 [==============================] - 12s 98ms/step - loss: 0.0327 - mae: 0.1349 - mse: 0.0327 - root_mean_squared_error: 0.1778 - val_loss: 0.0319 - val_mae: 0.1317 - val_mse: 0.0319 - val_root_mean_squared_error: 0.1749\n",
      "Epoch 19/50\n",
      "118/118 [==============================] - 11s 95ms/step - loss: 0.0325 - mae: 0.1338 - mse: 0.0325 - root_mean_squared_error: 0.1771 - val_loss: 0.0332 - val_mae: 0.1428 - val_mse: 0.0332 - val_root_mean_squared_error: 0.1794\n",
      "Epoch 20/50\n",
      "118/118 [==============================] - 11s 97ms/step - loss: 0.0329 - mae: 0.1352 - mse: 0.0329 - root_mean_squared_error: 0.1776 - val_loss: 0.0327 - val_mae: 0.1292 - val_mse: 0.0327 - val_root_mean_squared_error: 0.1767\n",
      "Epoch 21/50\n",
      "118/118 [==============================] - 10s 85ms/step - loss: 0.0326 - mae: 0.1340 - mse: 0.0326 - root_mean_squared_error: 0.1766 - val_loss: 0.0318 - val_mae: 0.1331 - val_mse: 0.0318 - val_root_mean_squared_error: 0.1750\n",
      "Epoch 22/50\n",
      "118/118 [==============================] - 10s 87ms/step - loss: 0.0323 - mae: 0.1337 - mse: 0.0323 - root_mean_squared_error: 0.1771 - val_loss: 0.0323 - val_mae: 0.1380 - val_mse: 0.0323 - val_root_mean_squared_error: 0.1767\n",
      "Epoch 23/50\n",
      "118/118 [==============================] - 11s 92ms/step - loss: 0.0320 - mae: 0.1333 - mse: 0.0320 - root_mean_squared_error: 0.1757 - val_loss: 0.0319 - val_mae: 0.1344 - val_mse: 0.0319 - val_root_mean_squared_error: 0.1753\n",
      "Epoch 24/50\n",
      "118/118 [==============================] - 10s 86ms/step - loss: 0.0324 - mae: 0.1341 - mse: 0.0324 - root_mean_squared_error: 0.1769 - val_loss: 0.0319 - val_mae: 0.1339 - val_mse: 0.0319 - val_root_mean_squared_error: 0.1751\n",
      "Epoch 25/50\n",
      "118/118 [==============================] - 10s 83ms/step - loss: 0.0322 - mae: 0.1334 - mse: 0.0322 - root_mean_squared_error: 0.1760 - val_loss: 0.0319 - val_mae: 0.1345 - val_mse: 0.0319 - val_root_mean_squared_error: 0.1753\n",
      "Epoch 26/50\n",
      "118/118 [==============================] - 10s 85ms/step - loss: 0.0324 - mae: 0.1333 - mse: 0.0324 - root_mean_squared_error: 0.1771 - val_loss: 0.0325 - val_mae: 0.1391 - val_mse: 0.0325 - val_root_mean_squared_error: 0.1772\n",
      "Epoch 27/50\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 0.0321 - mae: 0.1332 - mse: 0.0321 - root_mean_squared_error: 0.1769 - val_loss: 0.0327 - val_mae: 0.1292 - val_mse: 0.0327 - val_root_mean_squared_error: 0.1766\n",
      "Epoch 28/50\n",
      "118/118 [==============================] - 10s 86ms/step - loss: 0.0322 - mae: 0.1334 - mse: 0.0322 - root_mean_squared_error: 0.1762 - val_loss: 0.0337 - val_mae: 0.1294 - val_mse: 0.0337 - val_root_mean_squared_error: 0.1791\n",
      "Epoch 29/50\n",
      "118/118 [==============================] - 11s 91ms/step - loss: 0.0324 - mae: 0.1335 - mse: 0.0324 - root_mean_squared_error: 0.1764 - val_loss: 0.0319 - val_mae: 0.1317 - val_mse: 0.0319 - val_root_mean_squared_error: 0.1749\n",
      "Epoch 30/50\n",
      "118/118 [==============================] - 10s 85ms/step - loss: 0.0323 - mae: 0.1332 - mse: 0.0323 - root_mean_squared_error: 0.1772 - val_loss: 0.0320 - val_mae: 0.1351 - val_mse: 0.0320 - val_root_mean_squared_error: 0.1755\n",
      "Epoch 31/50\n",
      "118/118 [==============================] - 10s 82ms/step - loss: 0.0322 - mae: 0.1337 - mse: 0.0322 - root_mean_squared_error: 0.1757 - val_loss: 0.0323 - val_mae: 0.1295 - val_mse: 0.0323 - val_root_mean_squared_error: 0.1757\n",
      "Epoch 32/50\n",
      "118/118 [==============================] - 11s 92ms/step - loss: 0.0322 - mae: 0.1335 - mse: 0.0322 - root_mean_squared_error: 0.1760 - val_loss: 0.0319 - val_mae: 0.1317 - val_mse: 0.0319 - val_root_mean_squared_error: 0.1749\n",
      "Epoch 33/50\n",
      "118/118 [==============================] - 11s 91ms/step - loss: 0.0322 - mae: 0.1331 - mse: 0.0322 - root_mean_squared_error: 0.1758 - val_loss: 0.0319 - val_mae: 0.1311 - val_mse: 0.0319 - val_root_mean_squared_error: 0.1749\n",
      "Epoch 34/50\n",
      "118/118 [==============================] - 11s 92ms/step - loss: 0.0321 - mae: 0.1334 - mse: 0.0321 - root_mean_squared_error: 0.1762 - val_loss: 0.0318 - val_mae: 0.1327 - val_mse: 0.0318 - val_root_mean_squared_error: 0.1749\n",
      "Epoch 35/50\n",
      "118/118 [==============================] - 10s 87ms/step - loss: 0.0319 - mae: 0.1326 - mse: 0.0319 - root_mean_squared_error: 0.1751 - val_loss: 0.0331 - val_mae: 0.1292 - val_mse: 0.0331 - val_root_mean_squared_error: 0.1777\n",
      "Epoch 36/50\n",
      "118/118 [==============================] - 10s 83ms/step - loss: 0.0323 - mae: 0.1331 - mse: 0.0323 - root_mean_squared_error: 0.1769 - val_loss: 0.0319 - val_mae: 0.1339 - val_mse: 0.0319 - val_root_mean_squared_error: 0.1751\n",
      "Epoch 37/50\n",
      "118/118 [==============================] - 10s 82ms/step - loss: 0.0320 - mae: 0.1330 - mse: 0.0320 - root_mean_squared_error: 0.1754 - val_loss: 0.0325 - val_mae: 0.1394 - val_mse: 0.0325 - val_root_mean_squared_error: 0.1774\n",
      "Epoch 38/50\n",
      "118/118 [==============================] - 10s 84ms/step - loss: 0.0321 - mae: 0.1329 - mse: 0.0321 - root_mean_squared_error: 0.1759 - val_loss: 0.0318 - val_mae: 0.1329 - val_mse: 0.0318 - val_root_mean_squared_error: 0.1749\n",
      "Epoch 39/50\n",
      "118/118 [==============================] - 10s 84ms/step - loss: 0.0319 - mae: 0.1325 - mse: 0.0319 - root_mean_squared_error: 0.1754 - val_loss: 0.0322 - val_mae: 0.1368 - val_mse: 0.0322 - val_root_mean_squared_error: 0.1761\n",
      "Epoch 40/50\n",
      "118/118 [==============================] - 12s 98ms/step - loss: 0.0320 - mae: 0.1328 - mse: 0.0320 - root_mean_squared_error: 0.1761 - val_loss: 0.0322 - val_mae: 0.1373 - val_mse: 0.0322 - val_root_mean_squared_error: 0.1764\n",
      "Epoch 41/50\n",
      "118/118 [==============================] - 10s 86ms/step - loss: 0.0320 - mae: 0.1328 - mse: 0.0320 - root_mean_squared_error: 0.1754 - val_loss: 0.0318 - val_mae: 0.1324 - val_mse: 0.0318 - val_root_mean_squared_error: 0.1749\n",
      "Epoch 42/50\n",
      "118/118 [==============================] - 10s 83ms/step - loss: 0.0319 - mae: 0.1327 - mse: 0.0319 - root_mean_squared_error: 0.1749 - val_loss: 0.0319 - val_mae: 0.1337 - val_mse: 0.0319 - val_root_mean_squared_error: 0.1751\n",
      "Epoch 43/50\n",
      "118/118 [==============================] - 10s 88ms/step - loss: 0.0319 - mae: 0.1326 - mse: 0.0319 - root_mean_squared_error: 0.1760 - val_loss: 0.0319 - val_mae: 0.1345 - val_mse: 0.0319 - val_root_mean_squared_error: 0.1753\n",
      "Epoch 44/50\n",
      "118/118 [==============================] - 10s 87ms/step - loss: 0.0320 - mae: 0.1334 - mse: 0.0320 - root_mean_squared_error: 0.1755 - val_loss: 0.0321 - val_mae: 0.1300 - val_mse: 0.0321 - val_root_mean_squared_error: 0.1753\n",
      "Epoch 45/50\n",
      "118/118 [==============================] - 10s 84ms/step - loss: 0.0320 - mae: 0.1328 - mse: 0.0320 - root_mean_squared_error: 0.1760 - val_loss: 0.0318 - val_mae: 0.1328 - val_mse: 0.0318 - val_root_mean_squared_error: 0.1749\n",
      "Epoch 46/50\n",
      "118/118 [==============================] - 11s 92ms/step - loss: 0.0320 - mae: 0.1328 - mse: 0.0320 - root_mean_squared_error: 0.1754 - val_loss: 0.0318 - val_mae: 0.1329 - val_mse: 0.0318 - val_root_mean_squared_error: 0.1749\n",
      "Epoch 47/50\n",
      "118/118 [==============================] - 11s 93ms/step - loss: 0.0322 - mae: 0.1331 - mse: 0.0322 - root_mean_squared_error: 0.1762 - val_loss: 0.0319 - val_mae: 0.1307 - val_mse: 0.0319 - val_root_mean_squared_error: 0.1750\n",
      "Epoch 48/50\n",
      "118/118 [==============================] - 11s 92ms/step - loss: 0.0323 - mae: 0.1336 - mse: 0.0323 - root_mean_squared_error: 0.1769 - val_loss: 0.0323 - val_mae: 0.1377 - val_mse: 0.0323 - val_root_mean_squared_error: 0.1765\n",
      "Epoch 49/50\n",
      "118/118 [==============================] - 11s 93ms/step - loss: 0.0321 - mae: 0.1333 - mse: 0.0321 - root_mean_squared_error: 0.1756 - val_loss: 0.0319 - val_mae: 0.1315 - val_mse: 0.0319 - val_root_mean_squared_error: 0.1749\n",
      "Epoch 50/50\n",
      "118/118 [==============================] - 11s 96ms/step - loss: 0.0319 - mae: 0.1325 - mse: 0.0319 - root_mean_squared_error: 0.1757 - val_loss: 0.0319 - val_mae: 0.1342 - val_mse: 0.0319 - val_root_mean_squared_error: 0.1752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x234744a1040>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the keras model\n",
    "keras_model.fit(train_data_inputs, train_data_targets,\n",
    "          validation_data=(val_data_inputs, val_data_targets),\n",
    "          epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/31 [====================>.........] - ETA: 0s - loss: 0.0083 - mae: 0.0627 - mse: 0.0083 - root_mean_squared_error: 0.0883"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 8ms/step - loss: 0.0084 - mae: 0.0631 - mse: 0.0084 - root_mean_squared_error: 0.0882\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0319 - mae: 0.1342 - mse: 0.0319 - root_mean_squared_error: 0.1752\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0084 - mae: 0.0631 - mse: 0.0084 - root_mean_squared_error: 0.0885\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.0319 - mae: 0.1342 - mse: 0.0319 - root_mean_squared_error: 0.1762\n",
      "\n",
      "\n",
      "Manual Transformer:\n",
      "-------------------\n",
      "Validation Loss: 0.008366188034415245, Validation MSE: 0.008366188034415245, Validation MAE: 0.06310807913541794, Validation RMSE: 0.08815622329711914\n",
      "Test Loss: 0.00836618896573782, Test MSE: 0.00836618896573782, Test MAE: 0.06310807168483734, Test RMSE: 0.08852962404489517\n",
      "\n",
      "Keras Transformer:\n",
      "------------------\n",
      "Validation Loss: 0.031895961612463, Validation MSE: 0.031895961612463, Validation MAE: 0.13421513140201569, Validation RMSE: 0.17521920800209045\n",
      "Test Loss: 0.031895961612463, Test MSE: 0.031895961612463, Test MAE: 0.13421513140201569, Test RMSE: 0.17624206840991974\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "val_metrics_manul = manul_model.evaluate(val_data_inputs, val_data_targets, return_dict=True)\n",
    "val_metrics_keras = keras_model.evaluate(val_data_inputs, val_data_targets, return_dict=True)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics_manul = manul_model.evaluate(test_data_inputs, test_data_targets, return_dict=True)\n",
    "test_metrics_keras = keras_model.evaluate(test_data_inputs, test_data_targets, return_dict=True)\n",
    "\n",
    "# Extract individual metrics\n",
    "val_loss_manul, val_mae_manul, val_mse_manul, val_rmse_manul = val_metrics_manul['loss'], val_metrics_manul['mae'], val_metrics_manul['mse'], val_metrics_manul['root_mean_squared_error']\n",
    "test_loss_manul, test_mae_manul, test_mse_manul, test_rmse_manul = test_metrics_manul['loss'], test_metrics_manul['mae'], test_metrics_manul['mse'], test_metrics_manul['root_mean_squared_error']\n",
    "\n",
    "val_loss_keras, val_mae_keras, val_mse_keras, val_rmse_keras = val_metrics_keras['loss'], val_metrics_keras['mae'], val_metrics_keras['mse'], val_metrics_keras['root_mean_squared_error']\n",
    "test_loss_keras, test_mae_keras, test_mse_keras, test_rmse_keras = test_metrics_keras['loss'], test_metrics_keras['mae'], test_metrics_keras['mse'], test_metrics_keras['root_mean_squared_error']\n",
    "\n",
    "print('\\n\\nManual Transformer:\\n-------------------')\n",
    "print(f'Validation Loss: {val_loss_manul}, Validation MSE: {val_mse_manul}, Validation MAE: {val_mae_manul}, Validation RMSE: {val_rmse_manul}')\n",
    "print(f'Test Loss: {test_loss_manul}, Test MSE: {test_mse_manul}, Test MAE: {test_mae_manul}, Test RMSE: {test_rmse_manul}')\n",
    "\n",
    "print('\\nKeras Transformer:\\n------------------')\n",
    "print(f'Validation Loss: {val_loss_keras}, Validation MSE: {val_mse_keras}, Validation MAE: {val_mae_keras}, Validation RMSE: {val_rmse_keras}')\n",
    "print(f'Test Loss: {test_loss_keras}, Test MSE: {test_mse_keras}, Test MAE: {test_mae_keras}, Test RMSE: {test_rmse_keras}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 6ms/step\n",
      "22/22 [==============================] - 0s 6ms/step\n",
      "31/31 [==============================] - 1s 28ms/step\n",
      "22/22 [==============================] - 1s 28ms/step\n",
      "\n",
      "\n",
      "Manual Transformer:\n",
      "-------------------\n",
      "Test MAE: 0.06310807128942844\n",
      "Test RMSE: 0.09146687775698849\n",
      "\n",
      "==============================\n",
      "\n",
      "Keras Transformer:\n",
      "------------------\n",
      "Test MAE: 0.13421513115106617\n",
      "Test RMSE: 0.1785944178203732\n"
     ]
    }
   ],
   "source": [
    "# Manual Model Predictions\n",
    "val_predictions_manul = manul_model.predict(val_data_inputs)\n",
    "val_predictions_manul_flat = val_predictions_manul.reshape((val_data_targets.shape[0], -1))\n",
    "\n",
    "test_predictions_manul = manul_model.predict(test_data_inputs)\n",
    "test_predictions_manul_flat = test_predictions_manul.reshape((test_data_inputs.shape[0], -1))\n",
    "\n",
    "# Keras Model Predictions\n",
    "val_predictions_keras = keras_model.predict(val_data_inputs)\n",
    "val_predictions_keras_flat = val_predictions_keras.reshape((val_data_targets.shape[0], -1))\n",
    "\n",
    "test_predictions_keras = keras_model.predict(test_data_inputs)\n",
    "test_predictions_keras_flat = test_predictions_keras.reshape((test_data_inputs.shape[0], -1))\n",
    "\n",
    "# Calculate MAE and RMSE for test set\n",
    "test_mae_manul = np.mean(np.abs(test_data_targets - test_predictions_manul_flat))\n",
    "test_rmse_manul = np.sqrt(np.mean(np.square(test_data_targets - test_predictions_manul_flat)))\n",
    "\n",
    "test_mae_keras = np.mean(np.abs(test_data_targets - test_predictions_keras_flat))\n",
    "test_rmse_keras = np.sqrt(np.mean(np.square(test_data_targets - test_predictions_keras_flat)))\n",
    "\n",
    "print('\\n\\nManual Transformer:\\n-------------------')\n",
    "print(f'Test MAE: {test_mae_manul}')\n",
    "print(f'Test RMSE: {test_rmse_manul}')\n",
    "print('\\n==============================')\n",
    "print('\\nKeras Transformer:\\n------------------')\n",
    "print(f'Test MAE: {test_mae_keras }')\n",
    "print(f'Test RMSE: {test_rmse_keras }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1368x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing Forecasts for Validation and Test Set\n",
    "plt.figure(figsize=(19, 8))\n",
    "\n",
    "# Subplot for Validation Set\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(val_data_targets[-1], label='True Values', marker='o')\n",
    "plt.plot(val_predictions_manul_flat[-1], label='Manual Transformer Forecast', marker='o')\n",
    "plt.plot(val_predictions_keras_flat[-1], label='Keras Transformer Forecast', marker='o')\n",
    "plt.title('Validation Set - One-Step Forecast')\n",
    "plt.legend()\n",
    "\n",
    "# Subplot for Test Set\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(test_data_targets[-1], label='True Values', marker='o')\n",
    "plt.plot(test_predictions_manul_flat[-1], label='Manual Transformer Forecast', marker='o')\n",
    "plt.plot(test_predictions_keras_flat[-1], label='Keras Transformer Forecast', marker='o')\n",
    "plt.title('Test Set - One-Step Forecast')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Prediction | One step Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "\n",
      "\n",
      "One-Step Forecast Evaluation:\n",
      "-------------------------------\n",
      "Manual Model:\n",
      "MAE: 0.05488822638685647\n",
      "RMSE: 0.05850515385499535\n",
      "\n",
      "Keras Model:\n",
      "MAE: 0.12139765078519922\n",
      "RMSE: 0.12139765078519923\n"
     ]
    }
   ],
   "source": [
    "# Select the last sequence from the test data\n",
    "input_sequence = test_data_inputs[-1].reshape((1, sequence_length, num_features))\n",
    "\n",
    "# Make predictions using the manual model\n",
    "manual_forecast = manul_model.predict(input_sequence)\n",
    "\n",
    "# Make predictions using the keras model\n",
    "keras_forecast = keras_model.predict(input_sequence)\n",
    "\n",
    "# Reshape the forecasts if needed\n",
    "manual_forecast = manual_forecast.reshape((1, -1))\n",
    "keras_forecast = keras_forecast.reshape((1, -1))\n",
    "\n",
    "# Evaluate the forecasts\n",
    "forecast_target = test_data_targets[-1]  # Actual values for the next time step\n",
    "\n",
    "# Calculate MAE and RMSE for manual model\n",
    "manual_mae = np.mean(np.abs(forecast_target - manual_forecast))\n",
    "manual_rmse = np.sqrt(np.mean(np.square(forecast_target - manual_forecast)))\n",
    "\n",
    "# Calculate MAE and RMSE for keras model\n",
    "keras_mae = np.mean(np.abs(forecast_target - keras_forecast))\n",
    "keras_rmse = np.sqrt(np.mean(np.square(forecast_target - keras_forecast)))\n",
    "\n",
    "print('\\n\\nOne-Step Forecast Evaluation:\\n-------------------------------')\n",
    "print('Manual Model:')\n",
    "print(f'MAE: {manual_mae}')\n",
    "print(f'RMSE: {manual_rmse}')\n",
    "\n",
    "print('\\nKeras Model:')\n",
    "print(f'MAE: {keras_mae}')\n",
    "print(f'RMSE: {keras_rmse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
