{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Transformer.ipynb to script\n",
      "[NbConvertApp] Writing 16243 bytes to Transformer.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script Transformer.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source of implementation for Positional Encoding:**\n",
    "- https://stackoverflow.com/questions/68477306/positional-encoding-for-time-series-based-data-for-transformer-dnn-models\n",
    "- https://www.tensorflow.org/text/tutorials/transformer\n",
    "- https://github.com/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb\n",
    "- https://github.com/antonio-f/PositionalEncoding/blob/main/positional_encoding.py\n",
    "\n",
    "![](../../../images/Positional_Encoding.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    # Calculate angle rates based on dimension indices\n",
    "    angle_rates = 1 / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model))\n",
    "    # Generate a matrix of shape (pos, d_model) containing angle values for each position and dimension\n",
    "    angle_rads = np.arange(pos)[:, np.newaxis] * angle_rates\n",
    "    # Apply sine to even indices in the matrix; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    # Apply cosine to odd indices in the matrix; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    # Add a new axis to the matrix to create a 3D tensor and cast to TensorFlow float32 dtype\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self and Multi-Head Attention**<br><br>\n",
    "![](../../../images/Attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source of implementation for Scaled Dot Product Attention:** \n",
    "- https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras/\n",
    "- https://rmoklesur.medium.com/understanding-scaled-dot-product-attention-with-tensorflow-f570245bc12c\n",
    "- https://jamesmccaffrey.wordpress.com/2020/09/10/trying-to-understand-scaled-dot-product-attention-for-transformer-architecture/\n",
    "- https://gist.github.com/edumunozsala/72d25ca4ef1d5fde7eb4ebbd5d51792f\n",
    "\n",
    "![](../../../images/Self_Attention.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Scaled-Dot Product Attention - Self Attention\n",
    "def scaled_dot_product_attention(queries, keys, values, mask):\n",
    "    # Calculate the dot product of query and key matrices\n",
    "    matmul_qk = tf.matmul(queries, keys, transpose_b=True) # Q * K.T\n",
    "    \n",
    "    # Get the dimension of the key matrix and cast to float32\n",
    "    d_k = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
    "    \n",
    "    # Scale the attention scores by the square root of the key dimension / Scoring the queries against the keys after transposing the latter, and scaling\n",
    "    scaled_attention_scores = matmul_qk / tf.math.sqrt(d_k)\n",
    "    \n",
    "    # Apply the mask to the attention scores (if mask is provided)\n",
    "    if mask is not None:\n",
    "        scaled_attention_scores += (mask * -1e9)\n",
    "    \n",
    "    # Computing the weights by a softmax operation\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_scores, axis=-1)\n",
    "    \n",
    "    # Calculate the output by multiplying attention weights with value matrix\n",
    "    output = tf.matmul(attention_weights, values)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating a Transformer Manually Without Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source of implementation for Multi-Head Attention:** \n",
    "- https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras/\n",
    "\n",
    "![](../../../images/Multi_Head_Att.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ScaledDotProductAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, queries, keys, values, mask):\n",
    "        # Calculate the dot product of query and key matrices\n",
    "        matmul_qk = tf.matmul(queries, keys, transpose_b=True) # Q * K.T\n",
    "\n",
    "        # Get the dimension of the key matrix and cast to float32\n",
    "        d_k = keys.shape[-1]\n",
    "        d_k = tf.cast(d_k, tf.float32)\n",
    "\n",
    "        # Scale the attention scores by the square root of the key dimension / Scoring the queries against the keys after transposing the latter, and scaling\n",
    "        scaled_attention_scores = matmul_qk / tf.math.sqrt(d_k)\n",
    "\n",
    "        # Apply the mask to the attention scores (if mask is provided)\n",
    "        if mask is not None:\n",
    "            scaled_attention_scores += (mask * -1e9)\n",
    "\n",
    "        # Computing the weights by a softmax operation\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_scores, axis=-1)\n",
    "\n",
    "        # Calculate the output by multiplying attention weights with value matrix\n",
    "        output = tf.matmul(attention_weights, values)\n",
    "\n",
    "        return output\n",
    "    \n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.h = h\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Initialize linear layers for projections\n",
    "        self.query_projection = tf.keras.layers.Dense(units=(h * d_k), activation=None)\n",
    "        self.key_projection = tf.keras.layers.Dense(units=(h * d_k), activation=None)\n",
    "        self.value_projection = tf.keras.layers.Dense(units=(h * d_v), activation=None)\n",
    "\n",
    "        # Initialize the final linear layer\n",
    "        self.output_projection = tf.keras.layers.Dense(units=d_model, activation=None)\n",
    "\n",
    "        # Initialize attention layer\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "\n",
    "    def call(self, queries, keys, values, mask):\n",
    "        batch_size = tf.shape(queries)[0]\n",
    "\n",
    "        # Linear projections\n",
    "        queries = tf.reshape(self.query_projection(queries), (batch_size, -1, self.h, self.d_k))\n",
    "        keys = tf.reshape(self.key_projection(keys), (batch_size, -1, self.h, self.d_k))\n",
    "        values = tf.reshape(self.value_projection(values), (batch_size, -1, self.h, self.d_v))\n",
    "\n",
    "        # Transpose to have dimensions [batch_size, num_heads, seq_len, d_k/d_v]\n",
    "        queries = tf.transpose(queries, perm=[0, 2, 1, 3])\n",
    "        keys = tf.transpose(keys, perm=[0, 2, 1, 3])\n",
    "        values = tf.transpose(values, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # Apply attention\n",
    "        attention_output = self.attention(queries, keys, values, mask)\n",
    "\n",
    "        # Transpose and concatenate to get the final output\n",
    "        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n",
    "        attention_output = tf.reshape(attention_output, (batch_size, -1, self.h * self.d_v))\n",
    "\n",
    "        # Apply final linear layer\n",
    "        output = self.output_projection(attention_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source of implementation for Position-wise-Feed-Forward:**\n",
    "- https://stackoverflow.com/questions/74979359/how-is-position-wise-feed-forward-neural-network-implemented-for-transformers\n",
    "\n",
    "![](../../../images/Positionwise_FeedForward.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff, dropout_rate, **kwargs):\n",
    "        super(PositionwiseFeedForward, self).__init__(**kwargs)\n",
    "\n",
    "        # Feedforward neural network with a ReLU activation\n",
    "        self.ffn = tf.keras.Sequential([tf.keras.layers.Dense(d_ff, activation='relu'),tf.keras.layers.Dense(d_model, activation=None)])\n",
    "\n",
    "        # Dropout layer to prevent overfitting\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Pass the inputs through the feedforward neural network\n",
    "        ff_output = self.ffn(inputs)\n",
    "\n",
    "        # Apply dropout to the output\n",
    "        ff_output = self.dropout(ff_output)\n",
    "\n",
    "        return ff_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source of implementation for _Encoder_:**\n",
    "- https://machinelearningmastery.com/implementing-the-transformer-encoder-from-scratch-in-tensorflow-and-keras/\n",
    "- https://github.com/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb\n",
    "- https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_10_5_keras_transformers.ipynb\n",
    "\n",
    "![](../../../images/Encoder.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoder(encoder_input, num_heads, d_ff, dropout_rate, encoder_mask):\n",
    "    inputs = encoder_input\n",
    "    \n",
    "    # Extract the size of the model from the input shape\n",
    "    d_model = inputs.shape[-1]\n",
    "\n",
    "    # Multi-Head Self Attention\n",
    "    attention_output = MultiHeadAttention(h=num_heads,\n",
    "                                 d_k=d_model // num_heads,\n",
    "                                 d_v=d_model // num_heads,\n",
    "                                 d_model=d_model)(inputs, inputs, inputs, mask=encoder_mask)\n",
    "    # Apply dropout for regularization\n",
    "    attention_output = tf.keras.layers.Dropout(dropout_rate)(attention_output)\n",
    "\n",
    "    # Add and Normalize step after Multi-Head Self Attention\n",
    "    norm_attention_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention_output)\n",
    "\n",
    "    # Feedforward Neural Network\n",
    "    ffn = PositionwiseFeedForward(\n",
    "        d_model=d_model,\n",
    "        d_ff=d_ff,\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    ff_output = ffn(norm_attention_output)\n",
    "    # Apply dropout for regularization\n",
    "    ff_output = tf.keras.layers.Dropout(dropout_rate)(ff_output)\n",
    "\n",
    "    # Add and Normalize step after the Feedforward Neural Network\n",
    "    encoder_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(norm_attention_output + ff_output)\n",
    "\n",
    "    return encoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source of implementation for _Decoder_:**\n",
    "- https://machinelearningmastery.com/implementing-the-transformer-decoder-from-scratch-in-tensorflow-and-keras/\n",
    "\n",
    "![](../../../images/Decoder.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decoder(inputs, encoder_output, num_heads, d_ff, dropout_rate, decoder_mask):\n",
    "    inputs = inputs\n",
    "\n",
    "    # Extract the size of the model from the input shape\n",
    "    d_model = inputs.shape[-1]\n",
    "\n",
    "    # Masked Self-Attention\n",
    "    masked_attention_output = MultiHeadAttention(\n",
    "                                                 h=num_heads,\n",
    "                                                 d_k=d_model // num_heads,\n",
    "                                                 d_v=d_model // num_heads,\n",
    "                                                 d_model=d_model\n",
    "                                                )(inputs, inputs, inputs, mask=decoder_mask)\n",
    "    # Apply dropout for regularization\n",
    "    masked_attention_output = tf.keras.layers.Dropout(dropout_rate)(masked_attention_output)\n",
    "\n",
    "    # Add and Normalize the Masked Self-Attention output\n",
    "    norm_masked_attention_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(masked_attention_output + inputs)\n",
    "\n",
    "    # Cross-Attention with Encoder Output\n",
    "    attention_output = MultiHeadAttention(\n",
    "                                          h=num_heads,\n",
    "                                          d_k=d_model // num_heads,\n",
    "                                          d_v=d_model // num_heads,\n",
    "                                          d_model=d_model\n",
    "                                         )(norm_masked_attention_output, encoder_output, encoder_output, mask=decoder_mask)  # < ----- try and mask = mask\n",
    "    # Apply dropout for regularization\n",
    "    attention_output = tf.keras.layers.Dropout(dropout_rate)(attention_output)\n",
    "\n",
    "    # Add and Normalize the Cross-Attention output\n",
    "    attention_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(norm_masked_attention_output + attention_output)\n",
    "\n",
    "    # Feedforward Neural Network\n",
    "    ffn = PositionwiseFeedForward(\n",
    "                                  d_model=d_model,\n",
    "                                  d_ff=d_ff,\n",
    "                                  dropout_rate=dropout_rate\n",
    "                                 )\n",
    "    ff_output = ffn(attention_output)\n",
    "    # Apply dropout for regularization\n",
    "    ff_output = tf.keras.layers.Dropout(dropout_rate)(ff_output)\n",
    "\n",
    "    # Add and Normalize\n",
    "    decoder_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_output + ff_output)\n",
    "\n",
    "    return decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source of implementation for _Transformer Model_:**\n",
    "- https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/\n",
    "\n",
    "![](../../../images/Transformer_model.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransformerModel(input_shape, num_heads, d_ff, num_layers, dropout_rate, encoder_mask, decoder_mask):\n",
    "    # Define the input layer\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    encoder = inputs\n",
    "    for _ in range(num_layers):\n",
    "        # Apply the Encoder function to the input for each layer\n",
    "        encoder_output = Encoder(encoder, num_heads, d_ff, dropout_rate, encoder_mask)\n",
    "\n",
    "    # Decoder\n",
    "    decoder = encoder_output\n",
    "    for _ in range(num_layers):\n",
    "        # Apply the Decoder function to the encoder output for each layer\n",
    "        decoder_output = Decoder(decoder, encoder, num_heads, d_ff, dropout_rate, decoder_mask)\n",
    "\n",
    "    # Generate the final output with a TimeDistributed Dense layer\n",
    "    pull_time_window = tf.keras.layers.GlobalAveragePooling1D(data_format='channels_first')(decoder_output)\n",
    "    pull_time_window = tf.keras.layers.Dropout(0.1)(pull_time_window)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='linear')(pull_time_window)\n",
    "    #outputs = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=1))(decoder_output)\n",
    "\n",
    "    # Build the Keras model using the specified inputs and outputs\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(sequence_length, num_features, num_heads, d_ff, num_layers, dropout_rate, encoder_mask, decoder_mask):\n",
    "    # Define the input_shape\n",
    "    input_shape = (sequence_length, num_features)\n",
    "    # Create the transformer model\n",
    "    manual_model = TransformerModel(input_shape, num_heads, d_ff, num_layers, dropout_rate, encoder_mask, decoder_mask)\n",
    "\n",
    "    # Print the model summary\n",
    "    manual_model.summary()\n",
    "\n",
    "    return manual_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create a Transformer with keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_transformer_model(input_shape, num_heads, dff, num_layers, dropout_rate):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    encoder = inputs\n",
    "    # encoder = layers.Dense(units=input_shape[0], activation=\"relu\")(inputs) # < ------\n",
    "    for i in range(num_layers):\n",
    "        # Multi-Head Self Attention\n",
    "        encoder = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=input_shape[0])(encoder, encoder)\n",
    "        encoder = tf.keras.layers.Dropout(dropout_rate)(encoder)\n",
    "        # Add and Normalize\n",
    "        encoder = tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoder)\n",
    "        # Feedforward\n",
    "        ffn = keras.Sequential([tf.keras.layers.Dense(dff, activation=\"relu\"), tf.keras.layers.Dense(input_shape[0])])\n",
    "        encoder = ffn(encoder)\n",
    "        encoder = tf.keras.layers.Dropout(dropout_rate)(encoder)\n",
    "        # Add and Normalize\n",
    "        encoder = tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoder)\n",
    "        \n",
    "    # Decoder\n",
    "    decoder = encoder\n",
    "    # decoder = layers.Dense(units=input_shape[0], activation=\"relu\")(inputs)\n",
    "    for i in range(num_layers):\n",
    "        # Masked Multi-Head Attention\n",
    "        decoder = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=input_shape[0])(decoder, decoder)\n",
    "        decoder = tf.keras.layers.Dropout(dropout_rate)(decoder)\n",
    "        # Add and Normalize the masked multi-head attention\n",
    "        decoder = tf.keras.layers.LayerNormalization(epsilon=1e-6)(decoder)\n",
    "        # Multi-Head Attention\n",
    "        decoder = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=input_shape[0])(decoder, encoder) # < ------\n",
    "        decoder = tf.keras.layers.Dropout(dropout_rate)(decoder) # < ------\n",
    "        # Add and Normalize the multi-head attention\n",
    "        decoder = tf.keras.layers.LayerNormalization(epsilon=1e-6)(decoder) # < ------\n",
    "        # Feedforward\n",
    "        ffn = keras.Sequential([tf.keras.layers.Dense(dff, activation=\"relu\"), tf.keras.layers.Dense(input_shape[0])])\n",
    "        decoder = ffn(decoder)\n",
    "        decoder = tf.keras.layers.Dropout(dropout_rate)(decoder)\n",
    "        # Add and Normalize\n",
    "        decoder = tf.keras.layers.LayerNormalization(epsilon=1e-6)(decoder)\n",
    "\n",
    "    outputs = tf.keras.layers.GlobalAveragePooling1D(data_format='channels_first')(decoder)\n",
    "    outputs = tf.keras.layers.Dropout(0.1)(outputs)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='linear')(outputs)\n",
    "    #outputs = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=1))(decoder)\n",
    "    #outputs = tf.keras.layers.Dense(units=input_shape[0])(decoder) # < ------\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
