{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time2Vector(tf.keras.layers.Layer):\n",
    "  def __init__(self, seq_len, **kwargs):\n",
    "    super(Time2Vector, self).__init__()\n",
    "    self.seq_len = seq_len\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    '''Initialize weights and biases with shape (batch, seq_len)'''\n",
    "    self.weights_linear = self.add_weight(name='weight_linear',\n",
    "                                shape=(int(self.seq_len),),\n",
    "                                initializer='uniform',\n",
    "                                trainable=True)\n",
    "    \n",
    "    self.bias_linear = self.add_weight(name='bias_linear',\n",
    "                                shape=(int(self.seq_len),),\n",
    "                                initializer='uniform',\n",
    "                                trainable=True)\n",
    "    \n",
    "    self.weights_periodic = self.add_weight(name='weight_periodic',\n",
    "                                shape=(int(self.seq_len),),\n",
    "                                initializer='uniform',\n",
    "                                trainable=True)\n",
    "\n",
    "    self.bias_periodic = self.add_weight(name='bias_periodic',\n",
    "                                shape=(int(self.seq_len),),\n",
    "                                initializer='uniform',\n",
    "                                trainable=True)\n",
    "    \n",
    "  def call(self, x):\n",
    "    '''Calculate linear and periodic time features\n",
    "    \n",
    "    Args:\n",
    "        x (tensor): Input tensor of shape (batch_size, seq_len, features).\n",
    "        \n",
    "    Returns:\n",
    "        tensor: Concatenated linear and periodic time features of shape (batch_size, seq_len, 2).\n",
    "    '''\n",
    "    # Exclude volume and average across all features for our data, resulting in the shape (batch_size, seq_len)\n",
    "    x = tf.math.reduce_mean(x[:,:,:], axis=-1)\n",
    "\n",
    "    # Calculate the non-periodic (linear) time feature and expand the dimension by 1 again i.e., (batch_size, seq_len, 1)\n",
    "    time_linear = self.weights_linear * x + self.bias_linear # Linear time feature\n",
    "    time_linear = tf.expand_dims(time_linear, axis=-1) # Add dimension (batch, seq_len, 1)\n",
    "    \n",
    "    # Repeat for the periodic time feature, also resulting in the same matrix shape. (batch_size, seq_len, 1)\n",
    "    time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)\n",
    "    time_periodic = tf.expand_dims(time_periodic, axis=-1) # Add dimension (batch, seq_len, 1)\n",
    "\n",
    "    # Concatenate the linear and periodic time feature. (batch_size, seq_len, 2)\n",
    "    return tf.concat([time_linear, time_periodic], axis=-1) # shape = (batch, seq_len, 2)\n",
    "   \n",
    "  def get_config(self):\n",
    "      '''Get configuration for saving and loading model with custom layer.\n",
    "      \n",
    "      Returns:\n",
    "          dict: Configuration dictionary.\n",
    "      '''\n",
    "      config = super().get_config().copy()\n",
    "      config.update({'seq_len': self.seq_len})\n",
    "      return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttention(tf.keras.layers.Layer):\n",
    "    '''Single Attention Layer'''\n",
    "    def __init__(self, d_k, d_v):\n",
    "        super(SingleAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''Builds the layer'''\n",
    "        self.query = tf.keras.layers.Dense(self.d_k, \n",
    "                                           input_shape=input_shape, \n",
    "                                           kernel_initializer='glorot_uniform', \n",
    "                                           bias_initializer='glorot_uniform')\n",
    "        \n",
    "        self.key = tf.keras.layers.Dense(self.d_k, \n",
    "                                         input_shape=input_shape, \n",
    "                                         kernel_initializer='glorot_uniform', \n",
    "                                         bias_initializer='glorot_uniform')\n",
    "        \n",
    "        self.value = tf.keras.layers.Dense(self.d_v, \n",
    "                                           input_shape=input_shape, \n",
    "                                           kernel_initializer='glorot_uniform', \n",
    "                                           bias_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
    "        '''Executes the layer'''\n",
    "        q = self.query(inputs[0])\n",
    "        k = self.key(inputs[1])\n",
    "\n",
    "        attn_weights = tf.matmul(q, k, transpose_b=True)\n",
    "        attn_weights = tf.map_fn(lambda x: x/np.sqrt(self.d_k), attn_weights)\n",
    "        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
    "        \n",
    "        v = self.value(inputs[2])\n",
    "        attn_out = tf.matmul(attn_weights, v)\n",
    "        return attn_out    \n",
    "\n",
    "class MultiAttention(tf.keras.layers.Layer):\n",
    "    '''Multi-Head Attention Layer'''\n",
    "    def __init__(self, d_k, d_v, n_heads):\n",
    "        super(MultiAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "        self.attn_heads = list()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''Builds the layer'''\n",
    "        for n in range(self.n_heads):\n",
    "            self.attn_heads.append(SingleAttention(self.d_k, self.d_v))  \n",
    "        \n",
    "        # input_shape[0]=(batch, seq_len, num_features), input_shape[0][-1]=num_features\n",
    "        self.linear = tf.keras.layers.Dense(input_shape[0][-1], \n",
    "                                            input_shape=input_shape, \n",
    "                                            kernel_initializer='glorot_uniform', \n",
    "                                            bias_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        '''Executes the layer'''\n",
    "        attn = [self.attn_heads[i](inputs) for i in range(self.n_heads)]\n",
    "        concat_attn = tf.concat(attn, axis=-1)\n",
    "        multi_linear = self.linear(concat_attn)\n",
    "        return multi_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    '''Transformer Encoder Layer'''\n",
    "    def __init__(self, d_k, d_v, n_heads, ff_dim, dropout=0.1, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.attn_heads = list()\n",
    "        self.dropout_rate = dropout\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''Builds the layer'''\n",
    "        self.attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads)\n",
    "        self.attn_dropout = tf.keras.layers.Dropout(self.dropout_rate)\n",
    "        self.attn_normalize = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.ff_dense_1 = tf.keras.layers.Dense(units=self.ff_dim, activation='relu')\n",
    "        self.ff_dense_2 = tf.keras.layers.Dense(units=input_shape[0][-1]) \n",
    "        self.ff_dropout = tf.keras.layers.Dropout(self.dropout_rate)\n",
    "        self.ff_normalize = tf.keras.layers.LayerNormalization(epsilon=1e-6)    \n",
    "\n",
    "    def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
    "        '''Executes the layer'''\n",
    "        # Multi-Head Self Attention\n",
    "        attention_output = self.attn_multi(inputs)\n",
    "        # Apply dropout for regularization\n",
    "        attention_output = self.attn_dropout(attention_output)\n",
    "\n",
    "        # Add and Normalize step after Multi-Head Self Attention\n",
    "        norm_attention_output = self.attn_normalize(inputs[0] + attention_output)\n",
    "\n",
    "        # Feedforward Neural Network\n",
    "        ff_output = self.ff_dense_1(norm_attention_output)\n",
    "        ff_output = self.ff_dense_2(ff_output)\n",
    "        # Apply dropout for regularization\n",
    "        ff_output = self.ff_dropout(ff_output)\n",
    "\n",
    "        # Add and Normalize step after the Feedforward Neural Network\n",
    "        encoder_output = self.ff_normalize(inputs[0] + ff_output)\n",
    "\n",
    "        return encoder_output \n",
    "\n",
    "    def get_config(self): # Needed for saving and loading model with custom layer\n",
    "        '''Gets configuration for saving and loading'''\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'d_k': self.d_k,\n",
    "                       'd_v': self.d_v,\n",
    "                       'n_heads': self.n_heads,\n",
    "                       'ff_dim': self.ff_dim,\n",
    "                       'attn_heads': self.attn_heads,\n",
    "                       'dropout_rate': self.dropout_rate})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(tf.keras.layers.Layer):\n",
    "    '''Transformer Decoder Layer'''\n",
    "    def __init__(self, d_k, d_v, n_heads, ff_dim, dropout=0.1, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout_rate = dropout\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''Builds the layer'''\n",
    "        self.dec_attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads)\n",
    "        self.dec_attn_dropout = tf.keras.layers.Dropout(self.dropout_rate)\n",
    "        self.dec_attn_normalize = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.enc_dec_attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads)\n",
    "        self.enc_dec_attn_dropout = tf.keras.layers.Dropout(self.dropout_rate)\n",
    "        self.enc_dec_attn_normalize = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.ff_dense_1 = tf.keras.layers.Dense(units=self.ff_dim, activation='relu')\n",
    "        self.ff_dense_2 = tf.keras.layers.Dense(units=input_shape[0][-1]) \n",
    "        self.ff_dropout = tf.keras.layers.Dropout(self.dropout_rate)\n",
    "        self.ff_normalize = tf.keras.layers.LayerNormalization(epsilon=1e-6)    \n",
    "\n",
    "    def call(self, inputs): \n",
    "        '''Executes the layer'''\n",
    "        dec_inputs, enc_outputs = inputs\n",
    "        # Masked Self-Attention\n",
    "        masked_attention_output = self.dec_attn_multi((dec_inputs, dec_inputs, dec_inputs))\n",
    "        # Apply dropout for regularization\n",
    "        masked_attention_output = self.dec_attn_dropout(masked_attention_output)\n",
    "\n",
    "        # Add and Normalize the Masked Self-Attention output\n",
    "        norm_masked_attention_output = self.dec_attn_normalize(dec_inputs + masked_attention_output)\n",
    "\n",
    "        # Cross-Attention with Encoder Output\n",
    "        attention_output = self.enc_dec_attn_multi((norm_masked_attention_output, enc_outputs, enc_outputs))\n",
    "        # Apply dropout for regularization\n",
    "        attention_output = self.enc_dec_attn_dropout(attention_output)\n",
    "\n",
    "        # Add and Normalize the Cross-Attention output\n",
    "        attention_output = self.enc_dec_attn_normalize(norm_masked_attention_output + attention_output)\n",
    "\n",
    "        # Feedforward Neural Network\n",
    "        ff_output = self.ff_dense_1(attention_output)\n",
    "        ff_output = self.ff_dense_2(ff_output)\n",
    "        # Apply dropout for regularization\n",
    "        ff_output = self.ff_dropout(ff_output)\n",
    "\n",
    "        # Add and Normalize\n",
    "        decoder_output = self.ff_normalize(attention_output + ff_output)\n",
    "        \n",
    "        return decoder_output\n",
    "    \n",
    "    def get_config(self): \n",
    "        '''Gets configuration for saving and loading'''\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'d_k': self.d_k,\n",
    "                       'd_v': self.d_v,\n",
    "                       'n_heads': self.n_heads,\n",
    "                       'ff_dim': self.ff_dim,\n",
    "                       'dropout_rate': self.dropout_rate})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Time2VecTranformer(d_k, d_v, n_heads, ff_dim, num_layers, sequence_length, num_features):\n",
    "    '''Initialize time and transformer layers'''\n",
    "    time_embedding = Time2Vector(sequence_length)\n",
    "    \n",
    "    # Initialize multiple TransformerEncoder layers\n",
    "    encoder_layers = [TransformerEncoder(d_k, d_v, n_heads, ff_dim) for _ in range(num_layers)]\n",
    "    \n",
    "    # Initialize multiple TransformerDecoder layers\n",
    "    decoder_layers = [TransformerDecoder(d_k, d_v, n_heads, ff_dim) for _ in range(num_layers)]\n",
    "\n",
    "    '''Construct model'''\n",
    "    in_seq = tf.keras.layers.Input(shape=(sequence_length, num_features))\n",
    "    input_embeddings = time_embedding(in_seq)\n",
    "    inputs_encoder = tf.keras.layers.Concatenate(axis=-1)([in_seq, input_embeddings])\n",
    "    \n",
    "    # Connect multiple TransformerEncoder layers sequentially\n",
    "    enc_output = inputs_encoder\n",
    "    for encoder_layer in encoder_layers:\n",
    "        enc_output = encoder_layer((enc_output, enc_output, enc_output))\n",
    "    \n",
    "    # Connect multiple TransformerDecoder layers sequentially\n",
    "    dec_output = enc_output\n",
    "    for decoder_layer in decoder_layers:\n",
    "        dec_output = decoder_layer((dec_output, enc_output))\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D(data_format='channels_first')(dec_output)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    out = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=in_seq, outputs=out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
