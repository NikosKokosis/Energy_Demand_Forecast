{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 239 entries, 0 to 238\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   Date           239 non-null    datetime64[ns]\n",
      " 1   Energy__kWh_   239 non-null    float64       \n",
      " 2   Day            239 non-null    int64         \n",
      " 3   Minimum T      239 non-null    int64         \n",
      " 4   Maximum T      239 non-null    int64         \n",
      " 5   Snow           239 non-null    float64       \n",
      " 6   Precipitation  239 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(3), int64(3)\n",
      "memory usage: 13.2 KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "weekly = pd.read_csv('../Dataset/Boulder_Weekly.csv')\n",
    "weekly['Date'] = pd.to_datetime(weekly['Date'])\n",
    "weekly.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "if weekly['Date'].is_monotonic_increasing == False:\n",
    "    weekly.sort_values(by='Date', ascending=True, inplace=True)\n",
    "\n",
    "weekly['Month'] = weekly['Date'].dt.month_name()\n",
    "weekly.drop(columns={'Date', 'Day'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Scale the Dataset with MinMaxScaler / One-Hot Encode and Extract the Entire Scaled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Energy__kWh_', 'Minimum T', 'Maximum T', 'Snow', 'Precipitation',\n",
       "       'Month_April', 'Month_August', 'Month_December', 'Month_February',\n",
       "       'Month_January', 'Month_July', 'Month_June', 'Month_March', 'Month_May',\n",
       "       'Month_November', 'Month_October', 'Month_September'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the columns we need to scale\n",
    "columns_to_scale = ['Energy__kWh_', 'Minimum T', 'Maximum T', 'Snow', 'Precipitation']\n",
    "\n",
    "# MinMax scaling for numerical columns\n",
    "scaler = MinMaxScaler()\n",
    "weekly_scaled = weekly.copy()\n",
    "weekly_scaled[columns_to_scale] = scaler.fit_transform(weekly[columns_to_scale])\n",
    "\n",
    "# Define the columns we need to use for One-Hot Encoding\n",
    "categorical_columns = ['Month']\n",
    "\n",
    "# One-hot encoding for 'Weekday' and 'Month'\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "categorical_encoded = onehot_encoder.fit_transform(weekly[categorical_columns])\n",
    "\n",
    "# Get the feature names from the encoder\n",
    "encoded_columns = []\n",
    "for col, values in zip(categorical_columns, onehot_encoder.categories_):\n",
    "    encoded_columns.extend([f'{col}_{value}' for value in values])\n",
    "\n",
    "# Create DataFrame with encoded columns\n",
    "categorical_encoded_df = pd.DataFrame(categorical_encoded, columns=encoded_columns)\n",
    "\n",
    "# Concatenate the new encoded columns to the original DataFrame\n",
    "weekly_scaled = pd.concat([weekly_scaled, categorical_encoded_df], axis=1)\n",
    "\n",
    "# Drop the original categorical columns\n",
    "weekly_scaled = weekly_scaled.drop(categorical_columns, axis=1)\n",
    "weekly_scaled.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Divided the dataset into training, testing, and validation datasets according to 0.70, 0.20, and 0.10, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and testing sets\n",
    "def split_dataset(df, train_ratio, val_ratio):\n",
    "\n",
    "    total_size = len(df)\n",
    "    train_size = int(train_ratio * total_size)\n",
    "    val_size = int(val_ratio * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    train_df = df[:train_size]\n",
    "    val_df = df[train_size:train_size + val_size]\n",
    "    test_df = df[train_size + val_size:]\n",
    "\n",
    "    assert len(train_df) + len(val_df) + len(test_df) == total_size, \"Dataset not split correctly.\"\n",
    "\n",
    "    print(f'Training split ratio:   {round(len(train_df) / len(df), 3)}')\n",
    "    print(f'Validation split ratio: {round(len(val_df) / len(df), 3)}')\n",
    "    print(f'Testing split ratio:    {round(len(test_df) / len(df), 3)}')\n",
    "    print(\"\\nShapes of the datasets:\")\n",
    "    print(train_df.shape, val_df.shape, test_df.shape)\n",
    "\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training split ratio:   0.699\n",
      "Validation split ratio: 0.197\n",
      "Testing split ratio:    0.105\n",
      "\n",
      "Shapes of the datasets:\n",
      "(167, 17) (47, 17) (25, 17)\n"
     ]
    }
   ],
   "source": [
    "train_weekly_scaled, val_weekly_scaled, test_weekly_scaled = split_dataset(weekly_scaled, train_ratio=0.7, val_ratio=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Create sequences for the Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data\n",
    "def create_sequences(data, sequence_length):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequence = data.iloc[i:i + sequence_length].values\n",
    "        target = data.iloc[i + sequence_length]['Energy__kWh_']  # Predict the next value\n",
    "        inputs.append(sequence)\n",
    "        targets.append(target)\n",
    "\n",
    "    inputs_array = np.array(inputs)\n",
    "    targets_array = np.array(targets)\n",
    "    \n",
    "    print(f'Dataset split into sequences:')\n",
    "    print(f'Sequences shape: {inputs_array.shape}')\n",
    "    print(f'Targets shape: {targets_array.shape}\\n')\n",
    "\n",
    "    return np.array(inputs), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split into sequences:\n",
      "Sequences shape: (145, 22, 17)\n",
      "Targets shape: (145,)\n",
      "\n",
      "Dataset split into sequences:\n",
      "Sequences shape: (25, 22, 17)\n",
      "Targets shape: (25,)\n",
      "\n",
      "Dataset split into sequences:\n",
      "Sequences shape: (3, 22, 17)\n",
      "Targets shape: (3,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 22\n",
    "num_features = len(weekly_scaled.columns)\n",
    "\n",
    "# Create the training, validation, and test data sequences\n",
    "train_data_inputs, train_data_targets = create_sequences(train_weekly_scaled, sequence_length)\n",
    "val_data_inputs, val_data_targets = create_sequences(val_weekly_scaled, sequence_length)\n",
    "test_data_inputs, test_data_targets = create_sequences(test_weekly_scaled, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((145, 22, 17), (25, 22, 17), (3, 22, 17))"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The input Datasets must have this input shape (-1, sequence_length, num_features)\n",
    "train_data_inputs = train_data_inputs.reshape((-1, sequence_length, num_features))\n",
    "val_data_inputs = val_data_inputs.reshape((-1, sequence_length, num_features))\n",
    "test_data_inputs = test_data_inputs.reshape((-1, sequence_length, num_features))\n",
    "\n",
    "train_data_inputs.shape, val_data_inputs.shape, test_data_inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Create the Transformer Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)          [(None, 22, 17)]     0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention_182 (Mult  (None, None, 17)    1224        ['input_21[0][0]',               \n",
      " iHeadAttention)                                                  'input_21[0][0]',               \n",
      "                                                                  'input_21[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_366 (Dropout)          (None, None, 17)     0           ['multi_head_attention_182[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_154 (TFOp  (None, 22, 17)      0           ['input_21[0][0]',               \n",
      " Lambda)                                                          'dropout_366[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_304 (Layer  (None, 22, 17)      34          ['tf.__operators__.add_154[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " positionwise_feed_forward_62 (  (None, 22, 17)      2257        ['layer_normalization_304[0][0]']\n",
      " PositionwiseFeedForward)                                                                         \n",
      "                                                                                                  \n",
      " dropout_368 (Dropout)          (None, 22, 17)       0           ['positionwise_feed_forward_62[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_155 (TFOp  (None, 22, 17)      0           ['layer_normalization_304[0][0]',\n",
      " Lambda)                                                          'dropout_368[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_305 (Layer  (None, 22, 17)      34          ['tf.__operators__.add_155[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " multi_head_attention_187 (Mult  (None, None, 17)    1224        ['layer_normalization_305[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_305[0][0]',\n",
      "                                                                  'layer_normalization_305[0][0]']\n",
      "                                                                                                  \n",
      " dropout_377 (Dropout)          (None, None, 17)     0           ['multi_head_attention_187[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_162 (TFOp  (None, 22, 17)      0           ['dropout_377[0][0]',            \n",
      " Lambda)                                                          'layer_normalization_305[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_312 (Layer  (None, 22, 17)      34          ['tf.__operators__.add_162[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " multi_head_attention_188 (Mult  (None, None, 17)    1224        ['layer_normalization_312[0][0]',\n",
      " iHeadAttention)                                                  'input_21[0][0]',               \n",
      "                                                                  'input_21[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_378 (Dropout)          (None, None, 17)     0           ['multi_head_attention_188[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_163 (TFOp  (None, None, 17)    0           ['dropout_377[0][0]',            \n",
      " Lambda)                                                          'dropout_378[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_313 (Layer  (None, None, 17)    34          ['tf.__operators__.add_163[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " positionwise_feed_forward_65 (  (None, None, 17)    2257        ['layer_normalization_313[0][0]']\n",
      " PositionwiseFeedForward)                                                                         \n",
      "                                                                                                  \n",
      " dropout_380 (Dropout)          (None, None, 17)     0           ['positionwise_feed_forward_65[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_164 (TFOp  (None, None, 17)    0           ['layer_normalization_313[0][0]',\n",
      " Lambda)                                                          'dropout_380[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_314 (Layer  (None, None, 17)    34          ['tf.__operators__.add_164[0][0]'\n",
      " Normalization)                                                  ]                                \n",
      "                                                                                                  \n",
      " time_distributed_20 (TimeDistr  (None, None, 1)     18          ['layer_normalization_314[0][0]']\n",
      " ibuted)                                                                                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,374\n",
      "Trainable params: 8,374\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "%run \"../Code/Transformer.ipynb\"\n",
    "\n",
    "# Define the hyperparameters of the manual model\n",
    "input_shape = (sequence_length, num_features)\n",
    "num_heads = 1\n",
    "d_ff = 64\n",
    "num_layers = 3\n",
    "dropout_rate = 0.1\n",
    "encoder_mask = None\n",
    "decoder_mask = tf.linalg.band_part(tf.ones((sequence_length, sequence_length)), -1, 0)  # Create a lower triangular mask\n",
    "decoder_mask = 1 - decoder_mask  # Invert the mask\n",
    "\n",
    "# Create the transformer model\n",
    "manul_model = TransformerModel(input_shape, num_heads, d_ff, num_layers, dropout_rate, encoder_mask, decoder_mask)\n",
    "\n",
    "manul_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_21\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_22 (InputLayer)          [(None, 22, 17)]     0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention_189 (Mult  (None, 22, 17)      1579        ['input_22[0][0]',               \n",
      " iHeadAttention)                                                  'input_22[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_381 (Dropout)          (None, 22, 17)       0           ['multi_head_attention_189[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_315 (Layer  (None, 22, 17)      34          ['dropout_381[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_126 (Sequential)    (None, 22, 22)       2582        ['layer_normalization_315[0][0]']\n",
      "                                                                                                  \n",
      " dropout_382 (Dropout)          (None, 22, 22)       0           ['sequential_126[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_316 (Layer  (None, 22, 22)      44          ['dropout_382[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_190 (Mult  (None, 22, 22)      2024        ['layer_normalization_316[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_316[0][0]']\n",
      "                                                                                                  \n",
      " dropout_383 (Dropout)          (None, 22, 22)       0           ['multi_head_attention_190[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_317 (Layer  (None, 22, 22)      44          ['dropout_383[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_127 (Sequential)    (None, 22, 22)       2902        ['layer_normalization_317[0][0]']\n",
      "                                                                                                  \n",
      " dropout_384 (Dropout)          (None, 22, 22)       0           ['sequential_127[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_318 (Layer  (None, 22, 22)      44          ['dropout_384[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_191 (Mult  (None, 22, 22)      2024        ['layer_normalization_318[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_318[0][0]']\n",
      "                                                                                                  \n",
      " dropout_385 (Dropout)          (None, 22, 22)       0           ['multi_head_attention_191[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_319 (Layer  (None, 22, 22)      44          ['dropout_385[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_128 (Sequential)    (None, 22, 22)       2902        ['layer_normalization_319[0][0]']\n",
      "                                                                                                  \n",
      " dropout_386 (Dropout)          (None, 22, 22)       0           ['sequential_128[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_320 (Layer  (None, 22, 22)      44          ['dropout_386[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_192 (Mult  (None, 22, 22)      2024        ['layer_normalization_320[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_320[0][0]']\n",
      "                                                                                                  \n",
      " dropout_387 (Dropout)          (None, 22, 22)       0           ['multi_head_attention_192[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_321 (Layer  (None, 22, 22)      44          ['dropout_387[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_193 (Mult  (None, 22, 22)      2024        ['layer_normalization_321[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_320[0][0]']\n",
      "                                                                                                  \n",
      " dropout_388 (Dropout)          (None, 22, 22)       0           ['multi_head_attention_193[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_322 (Layer  (None, 22, 22)      44          ['dropout_388[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_129 (Sequential)    (None, 22, 22)       2902        ['layer_normalization_322[0][0]']\n",
      "                                                                                                  \n",
      " dropout_389 (Dropout)          (None, 22, 22)       0           ['sequential_129[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_323 (Layer  (None, 22, 22)      44          ['dropout_389[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_194 (Mult  (None, 22, 22)      2024        ['layer_normalization_323[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_323[0][0]']\n",
      "                                                                                                  \n",
      " dropout_390 (Dropout)          (None, 22, 22)       0           ['multi_head_attention_194[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_324 (Layer  (None, 22, 22)      44          ['dropout_390[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_195 (Mult  (None, 22, 22)      2024        ['layer_normalization_324[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_320[0][0]']\n",
      "                                                                                                  \n",
      " dropout_391 (Dropout)          (None, 22, 22)       0           ['multi_head_attention_195[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_325 (Layer  (None, 22, 22)      44          ['dropout_391[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_130 (Sequential)    (None, 22, 22)       2902        ['layer_normalization_325[0][0]']\n",
      "                                                                                                  \n",
      " dropout_392 (Dropout)          (None, 22, 22)       0           ['sequential_130[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_326 (Layer  (None, 22, 22)      44          ['dropout_392[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_196 (Mult  (None, 22, 22)      2024        ['layer_normalization_326[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_326[0][0]']\n",
      "                                                                                                  \n",
      " dropout_393 (Dropout)          (None, 22, 22)       0           ['multi_head_attention_196[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_327 (Layer  (None, 22, 22)      44          ['dropout_393[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_197 (Mult  (None, 22, 22)      2024        ['layer_normalization_327[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_320[0][0]']\n",
      "                                                                                                  \n",
      " dropout_394 (Dropout)          (None, 22, 22)       0           ['multi_head_attention_197[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_328 (Layer  (None, 22, 22)      44          ['dropout_394[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_131 (Sequential)    (None, 22, 22)       2902        ['layer_normalization_328[0][0]']\n",
      "                                                                                                  \n",
      " dropout_395 (Dropout)          (None, 22, 22)       0           ['sequential_131[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_329 (Layer  (None, 22, 22)      44          ['dropout_395[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " time_distributed_21 (TimeDistr  (None, 22, 1)       23          ['layer_normalization_329[0][0]']\n",
      " ibuted)                                                                                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 35,536\n",
      "Trainable params: 35,536\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the transformer model\n",
    "keras_model = keras_transformer_model(input_shape, num_heads, d_ff, num_layers, dropout_rate)\n",
    "\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Compile the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return tf.keras.backend.sqrt(\n",
    "        tf.keras.backend.mean(\n",
    "            tf.keras.backend.square(\n",
    "                y_pred - y_true\n",
    "            )\n",
    "        ) + 1e-9\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate for Adam optimizer\n",
    "learning_rate = 0.2\n",
    "\n",
    "# Compile the manual model\n",
    "manul_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss='mse',  metrics=['mae', 'mse', root_mean_squared_error])\n",
    "\n",
    "# Compile the keras model\n",
    "keras_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss='mse',  metrics=['mae', 'mse', root_mean_squared_error])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Train the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((145, 22, 17), (145,), (25, 22, 17), (25,))"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the parameters for training\n",
    "epochs = 200\n",
    "batch_size = 514\n",
    "\n",
    "# Convert the data to float32\n",
    "train_data_inputs = train_data_inputs.astype('float32')\n",
    "train_data_targets = train_data_targets.astype('float32')\n",
    "\n",
    "val_data_inputs = val_data_inputs.astype('float32')\n",
    "val_data_targets = val_data_targets.astype('float32')\n",
    "\n",
    "train_data_inputs.shape, train_data_targets.shape, val_data_inputs.shape, val_data_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 6s 6s/step - loss: 0.4450 - mae: 0.5350 - mse: 0.4450 - root_mean_squared_error: 0.6671 - val_loss: 27.2776 - val_mae: 5.2223 - val_mse: 27.2776 - val_root_mean_squared_error: 5.2228\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 27.2221 - mae: 5.1944 - mse: 27.2221 - root_mean_squared_error: 5.2175 - val_loss: 0.0883 - val_mae: 0.2889 - val_mse: 0.0883 - val_root_mean_squared_error: 0.2972\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4150 - mae: 0.5385 - mse: 0.4150 - root_mean_squared_error: 0.6442 - val_loss: 15.2436 - val_mae: 3.9037 - val_mse: 15.2436 - val_root_mean_squared_error: 3.9043\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 11.7976 - mae: 3.4158 - mse: 11.7976 - root_mean_squared_error: 3.4348 - val_loss: 5.8940 - val_mae: 2.4268 - val_mse: 5.8940 - val_root_mean_squared_error: 2.4278\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 4.1860 - mae: 2.0285 - mse: 4.1860 - root_mean_squared_error: 2.0460 - val_loss: 1.1685 - val_mae: 1.0788 - val_mse: 1.1685 - val_root_mean_squared_error: 1.0810\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5810 - mae: 0.7389 - mse: 0.5810 - root_mean_squared_error: 0.7622 - val_loss: 0.0052 - val_mae: 0.0537 - val_mse: 0.0052 - val_root_mean_squared_error: 0.0723\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.1335 - mae: 0.3276 - mse: 0.1335 - root_mean_squared_error: 0.3653 - val_loss: 0.7052 - val_mae: 0.8371 - val_mse: 0.7052 - val_root_mean_squared_error: 0.8398\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.2811 - mae: 1.1173 - mse: 1.2811 - root_mean_squared_error: 1.1318 - val_loss: 1.3654 - val_mae: 1.1665 - val_mse: 1.3654 - val_root_mean_squared_error: 1.1685\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.1388 - mae: 1.4497 - mse: 2.1388 - root_mean_squared_error: 1.4625 - val_loss: 1.0471 - val_mae: 1.0210 - val_mse: 1.0471 - val_root_mean_squared_error: 1.0233\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.7757 - mae: 1.3179 - mse: 1.7757 - root_mean_squared_error: 1.3325 - val_loss: 0.3273 - val_mae: 0.5681 - val_mse: 0.3273 - val_root_mean_squared_error: 0.5721\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.8381 - mae: 0.8926 - mse: 0.8381 - root_mean_squared_error: 0.9155 - val_loss: 0.0048 - val_mae: 0.0572 - val_mse: 0.0048 - val_root_mean_squared_error: 0.0690\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.1536 - mae: 0.3374 - mse: 0.1536 - root_mean_squared_error: 0.3919 - val_loss: 0.3427 - val_mae: 0.5815 - val_mse: 0.3427 - val_root_mean_squared_error: 0.5854\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0971 - mae: 0.2629 - mse: 0.0971 - root_mean_squared_error: 0.3115 - val_loss: 1.0437 - val_mae: 1.0194 - val_mse: 1.0437 - val_root_mean_squared_error: 1.0216\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4556 - mae: 0.6332 - mse: 0.4556 - root_mean_squared_error: 0.6750 - val_loss: 1.5461 - val_mae: 1.2416 - val_mse: 1.5461 - val_root_mean_squared_error: 1.2434\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.7908 - mae: 0.8559 - mse: 0.7908 - root_mean_squared_error: 0.8893 - val_loss: 1.4558 - val_mae: 1.2047 - val_mse: 1.4558 - val_root_mean_squared_error: 1.2066\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.7334 - mae: 0.8240 - mse: 0.7334 - root_mean_squared_error: 0.8564 - val_loss: 0.8759 - val_mae: 0.9334 - val_mse: 0.8759 - val_root_mean_squared_error: 0.9359\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3675 - mae: 0.5652 - mse: 0.3675 - root_mean_squared_error: 0.6062 - val_loss: 0.2652 - val_mae: 0.5105 - val_mse: 0.2652 - val_root_mean_squared_error: 0.5150\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0699 - mae: 0.2194 - mse: 0.0699 - root_mean_squared_error: 0.2645 - val_loss: 0.0075 - val_mae: 0.0742 - val_mse: 0.0075 - val_root_mean_squared_error: 0.0864\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.1198 - mae: 0.2903 - mse: 0.1198 - root_mean_squared_error: 0.3462 - val_loss: 0.0961 - val_mae: 0.3026 - val_mse: 0.0961 - val_root_mean_squared_error: 0.3100\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4266 - mae: 0.6238 - mse: 0.4266 - root_mean_squared_error: 0.6531 - val_loss: 0.2066 - val_mae: 0.4495 - val_mse: 0.2066 - val_root_mean_squared_error: 0.4545\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.6058 - mae: 0.7549 - mse: 0.6058 - root_mean_squared_error: 0.7783 - val_loss: 0.1290 - val_mae: 0.3527 - val_mse: 0.1290 - val_root_mean_squared_error: 0.3591\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4739 - mae: 0.6626 - mse: 0.4739 - root_mean_squared_error: 0.6884 - val_loss: 0.0080 - val_mae: 0.0695 - val_mse: 0.0080 - val_root_mean_squared_error: 0.0896\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1691 - mae: 0.3674 - mse: 0.1691 - root_mean_squared_error: 0.4112 - val_loss: 0.1092 - val_mae: 0.3234 - val_mse: 0.1092 - val_root_mean_squared_error: 0.3304\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0369 - mae: 0.1539 - mse: 0.0369 - root_mean_squared_error: 0.1921 - val_loss: 0.4513 - val_mae: 0.6684 - val_mse: 0.4513 - val_root_mean_squared_error: 0.6718\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.1529 - mae: 0.3473 - mse: 0.1529 - root_mean_squared_error: 0.3910 - val_loss: 0.7604 - val_mae: 0.8694 - val_mse: 0.7604 - val_root_mean_squared_error: 0.8720\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3323 - mae: 0.5425 - mse: 0.3323 - root_mean_squared_error: 0.5764 - val_loss: 0.7557 - val_mae: 0.8667 - val_mse: 0.7557 - val_root_mean_squared_error: 0.8693\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3328 - mae: 0.5439 - mse: 0.3328 - root_mean_squared_error: 0.5769 - val_loss: 0.4635 - val_mae: 0.6775 - val_mse: 0.4635 - val_root_mean_squared_error: 0.6808\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.1646 - mae: 0.3642 - mse: 0.1646 - root_mean_squared_error: 0.4057 - val_loss: 0.1499 - val_mae: 0.3813 - val_mse: 0.1499 - val_root_mean_squared_error: 0.3872\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.0389 - mae: 0.1594 - mse: 0.0389 - root_mean_squared_error: 0.1971 - val_loss: 0.0112 - val_mae: 0.0938 - val_mse: 0.0112 - val_root_mean_squared_error: 0.1060\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0724 - mae: 0.2228 - mse: 0.0724 - root_mean_squared_error: 0.2690 - val_loss: 0.0193 - val_mae: 0.1215 - val_mse: 0.0193 - val_root_mean_squared_error: 0.1390\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.1876 - mae: 0.3983 - mse: 0.1876 - root_mean_squared_error: 0.4331 - val_loss: 0.0326 - val_mae: 0.1674 - val_mse: 0.0326 - val_root_mean_squared_error: 0.1805\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.2229 - mae: 0.4417 - mse: 0.2229 - root_mean_squared_error: 0.4721 - val_loss: 0.0075 - val_mae: 0.0669 - val_mse: 0.0075 - val_root_mean_squared_error: 0.0865\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.1338 - mae: 0.3277 - mse: 0.1338 - root_mean_squared_error: 0.3658 - val_loss: 0.0310 - val_mae: 0.1627 - val_mse: 0.0310 - val_root_mean_squared_error: 0.1761\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0418 - mae: 0.1665 - mse: 0.0418 - root_mean_squared_error: 0.2046 - val_loss: 0.1652 - val_mae: 0.4008 - val_mse: 0.1652 - val_root_mean_squared_error: 0.4064\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0414 - mae: 0.1648 - mse: 0.0414 - root_mean_squared_error: 0.2034 - val_loss: 0.3329 - val_mae: 0.5730 - val_mse: 0.3329 - val_root_mean_squared_error: 0.5770\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.1075 - mae: 0.2889 - mse: 0.1075 - root_mean_squared_error: 0.3279 - val_loss: 0.3964 - val_mae: 0.6260 - val_mse: 0.3964 - val_root_mean_squared_error: 0.6296\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1428 - mae: 0.3430 - mse: 0.1428 - root_mean_squared_error: 0.3779 - val_loss: 0.3064 - val_mae: 0.5494 - val_mse: 0.3064 - val_root_mean_squared_error: 0.5535\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0971 - mae: 0.2727 - mse: 0.0971 - root_mean_squared_error: 0.3116 - val_loss: 0.1507 - val_mae: 0.3823 - val_mse: 0.1507 - val_root_mean_squared_error: 0.3882\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0357 - mae: 0.1524 - mse: 0.0357 - root_mean_squared_error: 0.1890 - val_loss: 0.0417 - val_mae: 0.1927 - val_mse: 0.0417 - val_root_mean_squared_error: 0.2042\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0297 - mae: 0.1419 - mse: 0.0297 - root_mean_squared_error: 0.1722 - val_loss: 0.0070 - val_mae: 0.0717 - val_mse: 0.0070 - val_root_mean_squared_error: 0.0835\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0672 - mae: 0.2199 - mse: 0.0672 - root_mean_squared_error: 0.2592 - val_loss: 0.0046 - val_mae: 0.0538 - val_mse: 0.0046 - val_root_mean_squared_error: 0.0676\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0912 - mae: 0.2650 - mse: 0.0912 - root_mean_squared_error: 0.3020 - val_loss: 0.0066 - val_mae: 0.0696 - val_mse: 0.0066 - val_root_mean_squared_error: 0.0812\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0686 - mae: 0.2234 - mse: 0.0686 - root_mean_squared_error: 0.2620 - val_loss: 0.0330 - val_mae: 0.1685 - val_mse: 0.0330 - val_root_mean_squared_error: 0.1815\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0305 - mae: 0.1440 - mse: 0.0305 - root_mean_squared_error: 0.1746 - val_loss: 0.1034 - val_mae: 0.3144 - val_mse: 0.1034 - val_root_mean_squared_error: 0.3216\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0238 - mae: 0.1264 - mse: 0.0238 - root_mean_squared_error: 0.1544"
     ]
    }
   ],
   "source": [
    "# Train the manual model\n",
    "manul_model.fit(train_data_inputs, train_data_targets,\n",
    "          validation_data=(val_data_inputs, val_data_targets),\n",
    "          epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 12s 12s/step - loss: 2.5907 - mae: 1.4821 - mse: 2.5907 - root_mean_squared_error: 1.6096 - val_loss: 29.7330 - val_mae: 5.4524 - val_mse: 29.7330 - val_root_mean_squared_error: 5.4528\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 30.1648 - mae: 5.4798 - mse: 30.1648 - root_mean_squared_error: 5.4923 - val_loss: 6.4220 - val_mae: 2.5333 - val_mse: 6.4220 - val_root_mean_squared_error: 2.5342\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 7.1283 - mae: 2.6417 - mse: 7.1283 - root_mean_squared_error: 2.6699 - val_loss: 1.6185 - val_mae: 1.2704 - val_mse: 1.6185 - val_root_mean_squared_error: 1.2722\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 1.0019 - mae: 0.9765 - mse: 1.0019 - root_mean_squared_error: 1.0010 - val_loss: 5.9899 - val_mae: 2.4465 - val_mse: 5.9899 - val_root_mean_squared_error: 2.4474\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 4.4713 - mae: 2.1037 - mse: 4.4713 - root_mean_squared_error: 2.1145 - val_loss: 6.3551 - val_mae: 2.5200 - val_mse: 6.3551 - val_root_mean_squared_error: 2.5209\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 4.7516 - mae: 2.1705 - mse: 4.7516 - root_mean_squared_error: 2.1798 - val_loss: 4.6708 - val_mae: 2.1601 - val_mse: 4.6708 - val_root_mean_squared_error: 2.1612\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 3.3545 - mae: 1.8220 - mse: 3.3545 - root_mean_squared_error: 1.8315 - val_loss: 2.7725 - val_mae: 1.6637 - val_mse: 2.7725 - val_root_mean_squared_error: 1.6651\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 1.8467 - mae: 1.3485 - mse: 1.8467 - root_mean_squared_error: 1.3589 - val_loss: 1.3624 - val_mae: 1.1653 - val_mse: 1.3624 - val_root_mean_squared_error: 1.1672\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.7763 - mae: 0.8668 - mse: 0.7763 - root_mean_squared_error: 0.8811 - val_loss: 0.5228 - val_mae: 0.7199 - val_mse: 0.5228 - val_root_mean_squared_error: 0.7230\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.2145 - mae: 0.4365 - mse: 0.2145 - root_mean_squared_error: 0.4631 - val_loss: 0.1232 - val_mae: 0.3444 - val_mse: 0.1232 - val_root_mean_squared_error: 0.3510\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0293 - mae: 0.1378 - mse: 0.0293 - root_mean_squared_error: 0.1713 - val_loss: 0.0063 - val_mae: 0.0680 - val_mse: 0.0063 - val_root_mean_squared_error: 0.0795\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0715 - mae: 0.2288 - mse: 0.0715 - root_mean_squared_error: 0.2675 - val_loss: 0.0392 - val_mae: 0.1862 - val_mse: 0.0392 - val_root_mean_squared_error: 0.1981\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.2162 - mae: 0.4376 - mse: 0.2162 - root_mean_squared_error: 0.4649 - val_loss: 0.1187 - val_mae: 0.3378 - val_mse: 0.1187 - val_root_mean_squared_error: 0.3445\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.3695 - mae: 0.5867 - mse: 0.3695 - root_mean_squared_error: 0.6079 - val_loss: 0.1737 - val_mae: 0.4113 - val_mse: 0.1737 - val_root_mean_squared_error: 0.4168\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.4557 - mae: 0.6557 - mse: 0.4557 - root_mean_squared_error: 0.6751 - val_loss: 0.1727 - val_mae: 0.4100 - val_mse: 0.1727 - val_root_mean_squared_error: 0.4155\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.4551 - mae: 0.6555 - mse: 0.4551 - root_mean_squared_error: 0.6746 - val_loss: 0.1220 - val_mae: 0.3427 - val_mse: 0.1220 - val_root_mean_squared_error: 0.3493\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.3688 - mae: 0.5864 - mse: 0.3688 - root_mean_squared_error: 0.6073 - val_loss: 0.0543 - val_mae: 0.2231 - val_mse: 0.0543 - val_root_mean_squared_error: 0.2331\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.2457 - mae: 0.4712 - mse: 0.2457 - root_mean_squared_error: 0.4957 - val_loss: 0.0091 - val_mae: 0.0743 - val_mse: 0.0091 - val_root_mean_squared_error: 0.0952\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.1247 - mae: 0.3209 - mse: 0.1247 - root_mean_squared_error: 0.3531 - val_loss: 0.0164 - val_mae: 0.1143 - val_mse: 0.0164 - val_root_mean_squared_error: 0.1280\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0434 - mae: 0.1719 - mse: 0.0434 - root_mean_squared_error: 0.2084 - val_loss: 0.0871 - val_mae: 0.2874 - val_mse: 0.0871 - val_root_mean_squared_error: 0.2952\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0221 - mae: 0.1239 - mse: 0.0221 - root_mean_squared_error: 0.1485 - val_loss: 0.2094 - val_mae: 0.4526 - val_mse: 0.2094 - val_root_mean_squared_error: 0.4576\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0576 - mae: 0.1999 - mse: 0.0576 - root_mean_squared_error: 0.2400 - val_loss: 0.3516 - val_mae: 0.5891 - val_mse: 0.3516 - val_root_mean_squared_error: 0.5930\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.1239 - mae: 0.3214 - mse: 0.1239 - root_mean_squared_error: 0.3520 - val_loss: 0.4730 - val_mae: 0.6845 - val_mse: 0.4730 - val_root_mean_squared_error: 0.6878\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.1930 - mae: 0.4152 - mse: 0.1930 - root_mean_squared_error: 0.4393 - val_loss: 0.5381 - val_mae: 0.7304 - val_mse: 0.5381 - val_root_mean_squared_error: 0.7335\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.2326 - mae: 0.4604 - mse: 0.2326 - root_mean_squared_error: 0.4823 - val_loss: 0.5301 - val_mae: 0.7250 - val_mse: 0.5301 - val_root_mean_squared_error: 0.7281\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.2259 - mae: 0.4533 - mse: 0.2259 - root_mean_squared_error: 0.4753 - val_loss: 0.4563 - val_mae: 0.6721 - val_mse: 0.4563 - val_root_mean_squared_error: 0.6755\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.1826 - mae: 0.4027 - mse: 0.1826 - root_mean_squared_error: 0.4274 - val_loss: 0.3420 - val_mae: 0.5809 - val_mse: 0.3420 - val_root_mean_squared_error: 0.5848\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.1180 - mae: 0.3124 - mse: 0.1180 - root_mean_squared_error: 0.3435 - val_loss: 0.2197 - val_mae: 0.4638 - val_mse: 0.2197 - val_root_mean_squared_error: 0.4687\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0587 - mae: 0.2037 - mse: 0.0587 - root_mean_squared_error: 0.2423 - val_loss: 0.1171 - val_mae: 0.3354 - val_mse: 0.1171 - val_root_mean_squared_error: 0.3421\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0253 - mae: 0.1286 - mse: 0.0253 - root_mean_squared_error: 0.1591 - val_loss: 0.0487 - val_mae: 0.2101 - val_mse: 0.0487 - val_root_mean_squared_error: 0.2207\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0230 - mae: 0.1279 - mse: 0.0230 - root_mean_squared_error: 0.1517 - val_loss: 0.0148 - val_mae: 0.1087 - val_mse: 0.0148 - val_root_mean_squared_error: 0.1218\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0464 - mae: 0.1784 - mse: 0.0464 - root_mean_squared_error: 0.2155 - val_loss: 0.0050 - val_mae: 0.0591 - val_mse: 0.0050 - val_root_mean_squared_error: 0.0705\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0778 - mae: 0.2429 - mse: 0.0778 - root_mean_squared_error: 0.2788 - val_loss: 0.0052 - val_mae: 0.0537 - val_mse: 0.0052 - val_root_mean_squared_error: 0.0723\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.1018 - mae: 0.2865 - mse: 0.1018 - root_mean_squared_error: 0.3191 - val_loss: 0.0057 - val_mae: 0.0567 - val_mse: 0.0057 - val_root_mean_squared_error: 0.0758\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.1076 - mae: 0.2961 - mse: 0.1076 - root_mean_squared_error: 0.3281 - val_loss: 0.0046 - val_mae: 0.0534 - val_mse: 0.0046 - val_root_mean_squared_error: 0.0679\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0920 - mae: 0.2694 - mse: 0.0920 - root_mean_squared_error: 0.3033 - val_loss: 0.0070 - val_mae: 0.0720 - val_mse: 0.0070 - val_root_mean_squared_error: 0.0838\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0655 - mae: 0.2186 - mse: 0.0655 - root_mean_squared_error: 0.2559 - val_loss: 0.0206 - val_mae: 0.1289 - val_mse: 0.0206 - val_root_mean_squared_error: 0.1436\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0386 - mae: 0.1617 - mse: 0.0386 - root_mean_squared_error: 0.1966 - val_loss: 0.0499 - val_mae: 0.2130 - val_mse: 0.0499 - val_root_mean_squared_error: 0.2235\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0227 - mae: 0.1272 - mse: 0.0227 - root_mean_squared_error: 0.1506 - val_loss: 0.0931 - val_mae: 0.2975 - val_mse: 0.0931 - val_root_mean_squared_error: 0.3051\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0211 - mae: 0.1211 - mse: 0.0211 - root_mean_squared_error: 0.1454 - val_loss: 0.1417 - val_mae: 0.3703 - val_mse: 0.1417 - val_root_mean_squared_error: 0.3764\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0301 - mae: 0.1387 - mse: 0.0301 - root_mean_squared_error: 0.1735 - val_loss: 0.1843 - val_mae: 0.4239 - val_mse: 0.1843 - val_root_mean_squared_error: 0.4293\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0437 - mae: 0.1705 - mse: 0.0437 - root_mean_squared_error: 0.2091 - val_loss: 0.2101 - val_mae: 0.4534 - val_mse: 0.2101 - val_root_mean_squared_error: 0.4584\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0543 - mae: 0.1946 - mse: 0.0543 - root_mean_squared_error: 0.2331 - val_loss: 0.2133 - val_mae: 0.4568 - val_mse: 0.2133 - val_root_mean_squared_error: 0.4618\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0550 - mae: 0.1960 - mse: 0.0550 - root_mean_squared_error: 0.2345 - val_loss: 0.1950 - val_mae: 0.4364 - val_mse: 0.1950 - val_root_mean_squared_error: 0.4416\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0479 - mae: 0.1802 - mse: 0.0479 - root_mean_squared_error: 0.2189 - val_loss: 0.1619 - val_mae: 0.3966 - val_mse: 0.1619 - val_root_mean_squared_error: 0.4023\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0361 - mae: 0.1524 - mse: 0.0361 - root_mean_squared_error: 0.1899 - val_loss: 0.1231 - val_mae: 0.3444 - val_mse: 0.1231 - val_root_mean_squared_error: 0.3509\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0256 - mae: 0.1287 - mse: 0.0256 - root_mean_squared_error: 0.1599 - val_loss: 0.0871 - val_mae: 0.2873 - val_mse: 0.0871 - val_root_mean_squared_error: 0.2951\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0201 - mae: 0.1195 - mse: 0.0201 - root_mean_squared_error: 0.1418 - val_loss: 0.0588 - val_mae: 0.2330 - val_mse: 0.0588 - val_root_mean_squared_error: 0.2426\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0207 - mae: 0.1229 - mse: 0.0207 - root_mean_squared_error: 0.1440 - val_loss: 0.0399 - val_mae: 0.1881 - val_mse: 0.0399 - val_root_mean_squared_error: 0.1998\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0259 - mae: 0.1342 - mse: 0.0259 - root_mean_squared_error: 0.1609 - val_loss: 0.0294 - val_mae: 0.1576 - val_mse: 0.0294 - val_root_mean_squared_error: 0.1715\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0315 - mae: 0.1463 - mse: 0.0315 - root_mean_squared_error: 0.1775 - val_loss: 0.0254 - val_mae: 0.1452 - val_mse: 0.0254 - val_root_mean_squared_error: 0.1594\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.0343 - mae: 0.1523 - mse: 0.0343 - root_mean_squared_error: 0.1852 - val_loss: 0.0266 - val_mae: 0.1490 - val_mse: 0.0266 - val_root_mean_squared_error: 0.1631\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0331 - mae: 0.1496 - mse: 0.0331 - root_mean_squared_error: 0.1819 - val_loss: 0.0327 - val_mae: 0.1678 - val_mse: 0.0327 - val_root_mean_squared_error: 0.1809\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0290 - mae: 0.1408 - mse: 0.0290 - root_mean_squared_error: 0.1702 - val_loss: 0.0439 - val_mae: 0.1983 - val_mse: 0.0439 - val_root_mean_squared_error: 0.2095\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0238 - mae: 0.1298 - mse: 0.0238 - root_mean_squared_error: 0.1543 - val_loss: 0.0597 - val_mae: 0.2348 - val_mse: 0.0597 - val_root_mean_squared_error: 0.2443\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0205 - mae: 0.1225 - mse: 0.0205 - root_mean_squared_error: 0.1433 - val_loss: 0.0787 - val_mae: 0.2723 - val_mse: 0.0787 - val_root_mean_squared_error: 0.2805\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0196 - mae: 0.1193 - mse: 0.0196 - root_mean_squared_error: 0.1401 - val_loss: 0.0979 - val_mae: 0.3055 - val_mse: 0.0979 - val_root_mean_squared_error: 0.3128\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0211 - mae: 0.1206 - mse: 0.0211 - root_mean_squared_error: 0.1454 - val_loss: 0.1136 - val_mae: 0.3302 - val_mse: 0.1136 - val_root_mean_squared_error: 0.3371\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0234 - mae: 0.1243 - mse: 0.0234 - root_mean_squared_error: 0.1531 - val_loss: 0.1229 - val_mae: 0.3440 - val_mse: 0.1229 - val_root_mean_squared_error: 0.3506\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0253 - mae: 0.1279 - mse: 0.0253 - root_mean_squared_error: 0.1590 - val_loss: 0.1242 - val_mae: 0.3458 - val_mse: 0.1242 - val_root_mean_squared_error: 0.3524\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0256 - mae: 0.1285 - mse: 0.0256 - root_mean_squared_error: 0.1599 - val_loss: 0.1179 - val_mae: 0.3366 - val_mse: 0.1179 - val_root_mean_squared_error: 0.3433\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0244 - mae: 0.1262 - mse: 0.0244 - root_mean_squared_error: 0.1562 - val_loss: 0.1060 - val_mae: 0.3185 - val_mse: 0.1060 - val_root_mean_squared_error: 0.3256\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0219 - mae: 0.1214 - mse: 0.0219 - root_mean_squared_error: 0.1479 - val_loss: 0.0916 - val_mae: 0.2950 - val_mse: 0.0916 - val_root_mean_squared_error: 0.3026\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0202 - mae: 0.1191 - mse: 0.0202 - root_mean_squared_error: 0.1420 - val_loss: 0.0773 - val_mae: 0.2698 - val_mse: 0.0773 - val_root_mean_squared_error: 0.2781\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0194 - mae: 0.1190 - mse: 0.0194 - root_mean_squared_error: 0.1393 - val_loss: 0.0654 - val_mae: 0.2466 - val_mse: 0.0654 - val_root_mean_squared_error: 0.2557\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0198 - mae: 0.1207 - mse: 0.0198 - root_mean_squared_error: 0.1408 - val_loss: 0.0568 - val_mae: 0.2286 - val_mse: 0.0568 - val_root_mean_squared_error: 0.2383\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0208 - mae: 0.1233 - mse: 0.0208 - root_mean_squared_error: 0.1443 - val_loss: 0.0519 - val_mae: 0.2177 - val_mse: 0.0519 - val_root_mean_squared_error: 0.2279\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0218 - mae: 0.1255 - mse: 0.0218 - root_mean_squared_error: 0.1476 - val_loss: 0.0508 - val_mae: 0.2150 - val_mse: 0.0508 - val_root_mean_squared_error: 0.2254\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0220 - mae: 0.1259 - mse: 0.0220 - root_mean_squared_error: 0.1483 - val_loss: 0.0531 - val_mae: 0.2203 - val_mse: 0.0531 - val_root_mean_squared_error: 0.2304\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0215 - mae: 0.1248 - mse: 0.0215 - root_mean_squared_error: 0.1466 - val_loss: 0.0583 - val_mae: 0.2319 - val_mse: 0.0583 - val_root_mean_squared_error: 0.2415\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0205 - mae: 0.1225 - mse: 0.0205 - root_mean_squared_error: 0.1432 - val_loss: 0.0658 - val_mae: 0.2475 - val_mse: 0.0658 - val_root_mean_squared_error: 0.2565\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0198 - mae: 0.1206 - mse: 0.0198 - root_mean_squared_error: 0.1406 - val_loss: 0.0746 - val_mae: 0.2646 - val_mse: 0.0746 - val_root_mean_squared_error: 0.2730\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0193 - mae: 0.1190 - mse: 0.0193 - root_mean_squared_error: 0.1389 - val_loss: 0.0832 - val_mae: 0.2805 - val_mse: 0.0832 - val_root_mean_squared_error: 0.2885\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0194 - mae: 0.1184 - mse: 0.0194 - root_mean_squared_error: 0.1393 - val_loss: 0.0903 - val_mae: 0.2928 - val_mse: 0.0903 - val_root_mean_squared_error: 0.3005\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0199 - mae: 0.1188 - mse: 0.0199 - root_mean_squared_error: 0.1411 - val_loss: 0.0947 - val_mae: 0.3002 - val_mse: 0.0947 - val_root_mean_squared_error: 0.3077\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0204 - mae: 0.1192 - mse: 0.0204 - root_mean_squared_error: 0.1428 - val_loss: 0.0956 - val_mae: 0.3018 - val_mse: 0.0956 - val_root_mean_squared_error: 0.3092\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0204 - mae: 0.1191 - mse: 0.0204 - root_mean_squared_error: 0.1428 - val_loss: 0.0933 - val_mae: 0.2979 - val_mse: 0.0933 - val_root_mean_squared_error: 0.3054\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.0201 - mae: 0.1188 - mse: 0.0201 - root_mean_squared_error: 0.1420 - val_loss: 0.0884 - val_mae: 0.2895 - val_mse: 0.0884 - val_root_mean_squared_error: 0.2973\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.0197 - mae: 0.1184 - mse: 0.0197 - root_mean_squared_error: 0.1403 - val_loss: 0.0821 - val_mae: 0.2784 - val_mse: 0.0821 - val_root_mean_squared_error: 0.2865\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0194 - mae: 0.1185 - mse: 0.0194 - root_mean_squared_error: 0.1392 - val_loss: 0.0757 - val_mae: 0.2667 - val_mse: 0.0757 - val_root_mean_squared_error: 0.2751\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0192 - mae: 0.1188 - mse: 0.0192 - root_mean_squared_error: 0.1386 - val_loss: 0.0702 - val_mae: 0.2563 - val_mse: 0.0702 - val_root_mean_squared_error: 0.2650\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0194 - mae: 0.1195 - mse: 0.0194 - root_mean_squared_error: 0.1391 - val_loss: 0.0663 - val_mae: 0.2486 - val_mse: 0.0663 - val_root_mean_squared_error: 0.2576\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0195 - mae: 0.1201 - mse: 0.0195 - root_mean_squared_error: 0.1398 - val_loss: 0.0644 - val_mae: 0.2446 - val_mse: 0.0644 - val_root_mean_squared_error: 0.2537\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0198 - mae: 0.1207 - mse: 0.0198 - root_mean_squared_error: 0.1405 - val_loss: 0.0644 - val_mae: 0.2446 - val_mse: 0.0644 - val_root_mean_squared_error: 0.2537\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0198 - mae: 0.1209 - mse: 0.0198 - root_mean_squared_error: 0.1407 - val_loss: 0.0663 - val_mae: 0.2485 - val_mse: 0.0663 - val_root_mean_squared_error: 0.2575\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0195 - mae: 0.1201 - mse: 0.0195 - root_mean_squared_error: 0.1396 - val_loss: 0.0696 - val_mae: 0.2549 - val_mse: 0.0696 - val_root_mean_squared_error: 0.2637\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0192 - mae: 0.1192 - mse: 0.0192 - root_mean_squared_error: 0.1385 - val_loss: 0.0735 - val_mae: 0.2626 - val_mse: 0.0735 - val_root_mean_squared_error: 0.2712\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.0191 - mae: 0.1188 - mse: 0.0191 - root_mean_squared_error: 0.1384 - val_loss: 0.0776 - val_mae: 0.2703 - val_mse: 0.0776 - val_root_mean_squared_error: 0.2786\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.0191 - mae: 0.1184 - mse: 0.0191 - root_mean_squared_error: 0.1383 - val_loss: 0.0811 - val_mae: 0.2767 - val_mse: 0.0811 - val_root_mean_squared_error: 0.2848\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0192 - mae: 0.1182 - mse: 0.0192 - root_mean_squared_error: 0.1386 - val_loss: 0.0834 - val_mae: 0.2808 - val_mse: 0.0834 - val_root_mean_squared_error: 0.2889\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0192 - mae: 0.1181 - mse: 0.0192 - root_mean_squared_error: 0.1387 - val_loss: 0.0843 - val_mae: 0.2825 - val_mse: 0.0843 - val_root_mean_squared_error: 0.2904\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0193 - mae: 0.1181 - mse: 0.0193 - root_mean_squared_error: 0.1389 - val_loss: 0.0838 - val_mae: 0.2816 - val_mse: 0.0838 - val_root_mean_squared_error: 0.2895\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0192 - mae: 0.1180 - mse: 0.0192 - root_mean_squared_error: 0.1387 - val_loss: 0.0821 - val_mae: 0.2785 - val_mse: 0.0821 - val_root_mean_squared_error: 0.2866\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0192 - mae: 0.1183 - mse: 0.0192 - root_mean_squared_error: 0.1386 - val_loss: 0.0798 - val_mae: 0.2742 - val_mse: 0.0798 - val_root_mean_squared_error: 0.2824\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0191 - mae: 0.1181 - mse: 0.0191 - root_mean_squared_error: 0.1383 - val_loss: 0.0769 - val_mae: 0.2690 - val_mse: 0.0769 - val_root_mean_squared_error: 0.2774\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0190 - mae: 0.1183 - mse: 0.0190 - root_mean_squared_error: 0.1380 - val_loss: 0.0743 - val_mae: 0.2641 - val_mse: 0.0743 - val_root_mean_squared_error: 0.2726\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.0191 - mae: 0.1187 - mse: 0.0191 - root_mean_squared_error: 0.1382 - val_loss: 0.0723 - val_mae: 0.2603 - val_mse: 0.0723 - val_root_mean_squared_error: 0.2689\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0190 - mae: 0.1187 - mse: 0.0190 - root_mean_squared_error: 0.1379 - val_loss: 0.0711 - val_mae: 0.2579 - val_mse: 0.0711 - val_root_mean_squared_error: 0.2666\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0191 - mae: 0.1190 - mse: 0.0191 - root_mean_squared_error: 0.1382 - val_loss: 0.0708 - val_mae: 0.2574 - val_mse: 0.0708 - val_root_mean_squared_error: 0.2661\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0191 - mae: 0.1189 - mse: 0.0191 - root_mean_squared_error: 0.1382 - val_loss: 0.0713 - val_mae: 0.2584 - val_mse: 0.0713 - val_root_mean_squared_error: 0.2671\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0191 - mae: 0.1190 - mse: 0.0191 - root_mean_squared_error: 0.1382 - val_loss: 0.0727 - val_mae: 0.2609 - val_mse: 0.0727 - val_root_mean_squared_error: 0.2695\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0190 - mae: 0.1187 - mse: 0.0190 - root_mean_squared_error: 0.1378 - val_loss: 0.0746 - val_mae: 0.2647 - val_mse: 0.0746 - val_root_mean_squared_error: 0.2732\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0190 - mae: 0.1184 - mse: 0.0190 - root_mean_squared_error: 0.1378 - val_loss: 0.0767 - val_mae: 0.2686 - val_mse: 0.0767 - val_root_mean_squared_error: 0.2770\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0190 - mae: 0.1182 - mse: 0.0190 - root_mean_squared_error: 0.1377 - val_loss: 0.0786 - val_mae: 0.2720 - val_mse: 0.0786 - val_root_mean_squared_error: 0.2803\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0190 - mae: 0.1181 - mse: 0.0190 - root_mean_squared_error: 0.1380 - val_loss: 0.0797 - val_mae: 0.2741 - val_mse: 0.0797 - val_root_mean_squared_error: 0.2823\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0190 - mae: 0.1179 - mse: 0.0190 - root_mean_squared_error: 0.1377 - val_loss: 0.0802 - val_mae: 0.2750 - val_mse: 0.0802 - val_root_mean_squared_error: 0.2831\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0189 - mae: 0.1178 - mse: 0.0189 - root_mean_squared_error: 0.1376 - val_loss: 0.0799 - val_mae: 0.2744 - val_mse: 0.0799 - val_root_mean_squared_error: 0.2826\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.0190 - mae: 0.1179 - mse: 0.0190 - root_mean_squared_error: 0.1378 - val_loss: 0.0788 - val_mae: 0.2725 - val_mse: 0.0788 - val_root_mean_squared_error: 0.2808\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0189 - mae: 0.1180 - mse: 0.0189 - root_mean_squared_error: 0.1376 - val_loss: 0.0775 - val_mae: 0.2701 - val_mse: 0.0775 - val_root_mean_squared_error: 0.2784\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0190 - mae: 0.1182 - mse: 0.0190 - root_mean_squared_error: 0.1378 - val_loss: 0.0761 - val_mae: 0.2674 - val_mse: 0.0761 - val_root_mean_squared_error: 0.2758\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0189 - mae: 0.1182 - mse: 0.0189 - root_mean_squared_error: 0.1375 - val_loss: 0.0749 - val_mae: 0.2651 - val_mse: 0.0749 - val_root_mean_squared_error: 0.2736\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0189 - mae: 0.1182 - mse: 0.0189 - root_mean_squared_error: 0.1373 - val_loss: 0.0741 - val_mae: 0.2636 - val_mse: 0.0741 - val_root_mean_squared_error: 0.2721\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0189 - mae: 0.1182 - mse: 0.0189 - root_mean_squared_error: 0.1374 - val_loss: 0.0736 - val_mae: 0.2628 - val_mse: 0.0736 - val_root_mean_squared_error: 0.2713\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0189 - mae: 0.1183 - mse: 0.0189 - root_mean_squared_error: 0.1374 - val_loss: 0.0736 - val_mae: 0.2627 - val_mse: 0.0736 - val_root_mean_squared_error: 0.2713\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0188 - mae: 0.1183 - mse: 0.0188 - root_mean_squared_error: 0.1373 - val_loss: 0.0740 - val_mae: 0.2636 - val_mse: 0.0740 - val_root_mean_squared_error: 0.2721\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0189 - mae: 0.1183 - mse: 0.0189 - root_mean_squared_error: 0.1375 - val_loss: 0.0749 - val_mae: 0.2651 - val_mse: 0.0749 - val_root_mean_squared_error: 0.2736\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0188 - mae: 0.1180 - mse: 0.0188 - root_mean_squared_error: 0.1372 - val_loss: 0.0757 - val_mae: 0.2667 - val_mse: 0.0757 - val_root_mean_squared_error: 0.2751\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0189 - mae: 0.1181 - mse: 0.0189 - root_mean_squared_error: 0.1374 - val_loss: 0.0764 - val_mae: 0.2680 - val_mse: 0.0764 - val_root_mean_squared_error: 0.2764\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0189 - mae: 0.1182 - mse: 0.0189 - root_mean_squared_error: 0.1376 - val_loss: 0.0769 - val_mae: 0.2690 - val_mse: 0.0769 - val_root_mean_squared_error: 0.2774\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0189 - mae: 0.1180 - mse: 0.0189 - root_mean_squared_error: 0.1374 - val_loss: 0.0773 - val_mae: 0.2697 - val_mse: 0.0773 - val_root_mean_squared_error: 0.2780\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0188 - mae: 0.1178 - mse: 0.0188 - root_mean_squared_error: 0.1371 - val_loss: 0.0773 - val_mae: 0.2696 - val_mse: 0.0773 - val_root_mean_squared_error: 0.2780\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0188 - mae: 0.1178 - mse: 0.0188 - root_mean_squared_error: 0.1370 - val_loss: 0.0770 - val_mae: 0.2691 - val_mse: 0.0770 - val_root_mean_squared_error: 0.2774\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0188 - mae: 0.1179 - mse: 0.0188 - root_mean_squared_error: 0.1371 - val_loss: 0.0765 - val_mae: 0.2683 - val_mse: 0.0765 - val_root_mean_squared_error: 0.2767\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0188 - mae: 0.1179 - mse: 0.0188 - root_mean_squared_error: 0.1371 - val_loss: 0.0761 - val_mae: 0.2675 - val_mse: 0.0761 - val_root_mean_squared_error: 0.2759\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0188 - mae: 0.1180 - mse: 0.0188 - root_mean_squared_error: 0.1371 - val_loss: 0.0758 - val_mae: 0.2669 - val_mse: 0.0758 - val_root_mean_squared_error: 0.2753\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0187 - mae: 0.1179 - mse: 0.0187 - root_mean_squared_error: 0.1368 - val_loss: 0.0756 - val_mae: 0.2665 - val_mse: 0.0756 - val_root_mean_squared_error: 0.2749\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0187 - mae: 0.1178 - mse: 0.0187 - root_mean_squared_error: 0.1368 - val_loss: 0.0754 - val_mae: 0.2663 - val_mse: 0.0754 - val_root_mean_squared_error: 0.2747\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0187 - mae: 0.1179 - mse: 0.0187 - root_mean_squared_error: 0.1369 - val_loss: 0.0754 - val_mae: 0.2662 - val_mse: 0.0754 - val_root_mean_squared_error: 0.2746\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0187 - mae: 0.1179 - mse: 0.0187 - root_mean_squared_error: 0.1369 - val_loss: 0.0755 - val_mae: 0.2664 - val_mse: 0.0755 - val_root_mean_squared_error: 0.2748\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.0187 - mae: 0.1179 - mse: 0.0187 - root_mean_squared_error: 0.1369 - val_loss: 0.0757 - val_mae: 0.2668 - val_mse: 0.0757 - val_root_mean_squared_error: 0.2752\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0187 - mae: 0.1179 - mse: 0.0187 - root_mean_squared_error: 0.1369 - val_loss: 0.0759 - val_mae: 0.2671 - val_mse: 0.0759 - val_root_mean_squared_error: 0.2755\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0187 - mae: 0.1178 - mse: 0.0187 - root_mean_squared_error: 0.1368 - val_loss: 0.0760 - val_mae: 0.2673 - val_mse: 0.0760 - val_root_mean_squared_error: 0.2757\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0187 - mae: 0.1178 - mse: 0.0187 - root_mean_squared_error: 0.1367 - val_loss: 0.0762 - val_mae: 0.2676 - val_mse: 0.0762 - val_root_mean_squared_error: 0.2760\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0187 - mae: 0.1178 - mse: 0.0187 - root_mean_squared_error: 0.1369 - val_loss: 0.0761 - val_mae: 0.2675 - val_mse: 0.0761 - val_root_mean_squared_error: 0.2759\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0187 - mae: 0.1177 - mse: 0.0187 - root_mean_squared_error: 0.1367 - val_loss: 0.0760 - val_mae: 0.2674 - val_mse: 0.0760 - val_root_mean_squared_error: 0.2757\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0186 - mae: 0.1177 - mse: 0.0186 - root_mean_squared_error: 0.1365 - val_loss: 0.0760 - val_mae: 0.2673 - val_mse: 0.0760 - val_root_mean_squared_error: 0.2757\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0187 - mae: 0.1179 - mse: 0.0187 - root_mean_squared_error: 0.1369 - val_loss: 0.0761 - val_mae: 0.2674 - val_mse: 0.0761 - val_root_mean_squared_error: 0.2758\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0187 - mae: 0.1178 - mse: 0.0187 - root_mean_squared_error: 0.1368 - val_loss: 0.0761 - val_mae: 0.2675 - val_mse: 0.0761 - val_root_mean_squared_error: 0.2759\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0187 - mae: 0.1178 - mse: 0.0187 - root_mean_squared_error: 0.1366 - val_loss: 0.0762 - val_mae: 0.2676 - val_mse: 0.0762 - val_root_mean_squared_error: 0.2760\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0187 - mae: 0.1179 - mse: 0.0187 - root_mean_squared_error: 0.1369 - val_loss: 0.0763 - val_mae: 0.2678 - val_mse: 0.0763 - val_root_mean_squared_error: 0.2762\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0187 - mae: 0.1178 - mse: 0.0187 - root_mean_squared_error: 0.1367 - val_loss: 0.0763 - val_mae: 0.2679 - val_mse: 0.0763 - val_root_mean_squared_error: 0.2763\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0187 - mae: 0.1178 - mse: 0.0187 - root_mean_squared_error: 0.1367 - val_loss: 0.0763 - val_mae: 0.2679 - val_mse: 0.0763 - val_root_mean_squared_error: 0.2763\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0187 - mae: 0.1177 - mse: 0.0187 - root_mean_squared_error: 0.1367 - val_loss: 0.0762 - val_mae: 0.2677 - val_mse: 0.0762 - val_root_mean_squared_error: 0.2761\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0186 - mae: 0.1177 - mse: 0.0186 - root_mean_squared_error: 0.1365 - val_loss: 0.0762 - val_mae: 0.2676 - val_mse: 0.0762 - val_root_mean_squared_error: 0.2760\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0186 - mae: 0.1177 - mse: 0.0186 - root_mean_squared_error: 0.1365 - val_loss: 0.0761 - val_mae: 0.2675 - val_mse: 0.0761 - val_root_mean_squared_error: 0.2759\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0186 - mae: 0.1177 - mse: 0.0186 - root_mean_squared_error: 0.1364 - val_loss: 0.0761 - val_mae: 0.2675 - val_mse: 0.0761 - val_root_mean_squared_error: 0.2759\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0186 - mae: 0.1176 - mse: 0.0186 - root_mean_squared_error: 0.1365 - val_loss: 0.0760 - val_mae: 0.2673 - val_mse: 0.0760 - val_root_mean_squared_error: 0.2756\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0186 - mae: 0.1176 - mse: 0.0186 - root_mean_squared_error: 0.1363 - val_loss: 0.0759 - val_mae: 0.2670 - val_mse: 0.0759 - val_root_mean_squared_error: 0.2754\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0186 - mae: 0.1176 - mse: 0.0186 - root_mean_squared_error: 0.1363 - val_loss: 0.0758 - val_mae: 0.2668 - val_mse: 0.0758 - val_root_mean_squared_error: 0.2752\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0185 - mae: 0.1175 - mse: 0.0185 - root_mean_squared_error: 0.1362 - val_loss: 0.0756 - val_mae: 0.2665 - val_mse: 0.0756 - val_root_mean_squared_error: 0.2749\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0186 - mae: 0.1176 - mse: 0.0186 - root_mean_squared_error: 0.1363 - val_loss: 0.0755 - val_mae: 0.2664 - val_mse: 0.0755 - val_root_mean_squared_error: 0.2748\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0186 - mae: 0.1177 - mse: 0.0186 - root_mean_squared_error: 0.1364 - val_loss: 0.0756 - val_mae: 0.2666 - val_mse: 0.0756 - val_root_mean_squared_error: 0.2750\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.0186 - mae: 0.1177 - mse: 0.0186 - root_mean_squared_error: 0.1363 - val_loss: 0.0759 - val_mae: 0.2671 - val_mse: 0.0759 - val_root_mean_squared_error: 0.2755\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0186 - mae: 0.1176 - mse: 0.0186 - root_mean_squared_error: 0.1363 - val_loss: 0.0762 - val_mae: 0.2677 - val_mse: 0.0762 - val_root_mean_squared_error: 0.2761\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0186 - mae: 0.1175 - mse: 0.0186 - root_mean_squared_error: 0.1362 - val_loss: 0.0764 - val_mae: 0.2680 - val_mse: 0.0764 - val_root_mean_squared_error: 0.2764\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0185 - mae: 0.1175 - mse: 0.0185 - root_mean_squared_error: 0.1362 - val_loss: 0.0765 - val_mae: 0.2683 - val_mse: 0.0765 - val_root_mean_squared_error: 0.2767\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0185 - mae: 0.1175 - mse: 0.0185 - root_mean_squared_error: 0.1362 - val_loss: 0.0766 - val_mae: 0.2685 - val_mse: 0.0766 - val_root_mean_squared_error: 0.2768\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0185 - mae: 0.1175 - mse: 0.0185 - root_mean_squared_error: 0.1361 - val_loss: 0.0766 - val_mae: 0.2684 - val_mse: 0.0766 - val_root_mean_squared_error: 0.2768\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0185 - mae: 0.1175 - mse: 0.0185 - root_mean_squared_error: 0.1362 - val_loss: 0.0765 - val_mae: 0.2683 - val_mse: 0.0765 - val_root_mean_squared_error: 0.2766\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0185 - mae: 0.1175 - mse: 0.0185 - root_mean_squared_error: 0.1362 - val_loss: 0.0764 - val_mae: 0.2680 - val_mse: 0.0764 - val_root_mean_squared_error: 0.2763\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0185 - mae: 0.1175 - mse: 0.0185 - root_mean_squared_error: 0.1361 - val_loss: 0.0762 - val_mae: 0.2676 - val_mse: 0.0762 - val_root_mean_squared_error: 0.2760\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0185 - mae: 0.1175 - mse: 0.0185 - root_mean_squared_error: 0.1361 - val_loss: 0.0759 - val_mae: 0.2671 - val_mse: 0.0759 - val_root_mean_squared_error: 0.2755\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0186 - mae: 0.1176 - mse: 0.0186 - root_mean_squared_error: 0.1362 - val_loss: 0.0758 - val_mae: 0.2668 - val_mse: 0.0758 - val_root_mean_squared_error: 0.2752\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0185 - mae: 0.1176 - mse: 0.0185 - root_mean_squared_error: 0.1362 - val_loss: 0.0758 - val_mae: 0.2669 - val_mse: 0.0758 - val_root_mean_squared_error: 0.2753\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0185 - mae: 0.1175 - mse: 0.0185 - root_mean_squared_error: 0.1361 - val_loss: 0.0759 - val_mae: 0.2671 - val_mse: 0.0759 - val_root_mean_squared_error: 0.2755\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0185 - mae: 0.1174 - mse: 0.0185 - root_mean_squared_error: 0.1359 - val_loss: 0.0760 - val_mae: 0.2672 - val_mse: 0.0760 - val_root_mean_squared_error: 0.2756\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0185 - mae: 0.1175 - mse: 0.0185 - root_mean_squared_error: 0.1360 - val_loss: 0.0761 - val_mae: 0.2675 - val_mse: 0.0761 - val_root_mean_squared_error: 0.2759\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0185 - mae: 0.1175 - mse: 0.0185 - root_mean_squared_error: 0.1361 - val_loss: 0.0763 - val_mae: 0.2678 - val_mse: 0.0763 - val_root_mean_squared_error: 0.2762\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0185 - mae: 0.1174 - mse: 0.0185 - root_mean_squared_error: 0.1359 - val_loss: 0.0764 - val_mae: 0.2680 - val_mse: 0.0764 - val_root_mean_squared_error: 0.2764\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.0185 - mae: 0.1174 - mse: 0.0185 - root_mean_squared_error: 0.1358 - val_loss: 0.0764 - val_mae: 0.2681 - val_mse: 0.0764 - val_root_mean_squared_error: 0.2765\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0185 - mae: 0.1174 - mse: 0.0185 - root_mean_squared_error: 0.1360 - val_loss: 0.0764 - val_mae: 0.2680 - val_mse: 0.0764 - val_root_mean_squared_error: 0.2763\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0185 - mae: 0.1174 - mse: 0.0185 - root_mean_squared_error: 0.1358 - val_loss: 0.0763 - val_mae: 0.2678 - val_mse: 0.0763 - val_root_mean_squared_error: 0.2762\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0184 - mae: 0.1174 - mse: 0.0184 - root_mean_squared_error: 0.1358 - val_loss: 0.0763 - val_mae: 0.2679 - val_mse: 0.0763 - val_root_mean_squared_error: 0.2763\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0185 - mae: 0.1175 - mse: 0.0185 - root_mean_squared_error: 0.1359 - val_loss: 0.0764 - val_mae: 0.2680 - val_mse: 0.0764 - val_root_mean_squared_error: 0.2764\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0184 - mae: 0.1174 - mse: 0.0184 - root_mean_squared_error: 0.1358 - val_loss: 0.0764 - val_mae: 0.2681 - val_mse: 0.0764 - val_root_mean_squared_error: 0.2764\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0184 - mae: 0.1173 - mse: 0.0184 - root_mean_squared_error: 0.1356 - val_loss: 0.0763 - val_mae: 0.2679 - val_mse: 0.0763 - val_root_mean_squared_error: 0.2763\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.0184 - mae: 0.1174 - mse: 0.0184 - root_mean_squared_error: 0.1358 - val_loss: 0.0762 - val_mae: 0.2676 - val_mse: 0.0762 - val_root_mean_squared_error: 0.2760\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0184 - mae: 0.1173 - mse: 0.0184 - root_mean_squared_error: 0.1356 - val_loss: 0.0760 - val_mae: 0.2672 - val_mse: 0.0760 - val_root_mean_squared_error: 0.2756\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.0184 - mae: 0.1174 - mse: 0.0184 - root_mean_squared_error: 0.1357 - val_loss: 0.0758 - val_mae: 0.2669 - val_mse: 0.0758 - val_root_mean_squared_error: 0.2753\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0184 - mae: 0.1174 - mse: 0.0184 - root_mean_squared_error: 0.1358 - val_loss: 0.0757 - val_mae: 0.2667 - val_mse: 0.0757 - val_root_mean_squared_error: 0.2751\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0184 - mae: 0.1174 - mse: 0.0184 - root_mean_squared_error: 0.1358 - val_loss: 0.0757 - val_mae: 0.2667 - val_mse: 0.0757 - val_root_mean_squared_error: 0.2751\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0184 - mae: 0.1174 - mse: 0.0184 - root_mean_squared_error: 0.1357 - val_loss: 0.0758 - val_mae: 0.2669 - val_mse: 0.0758 - val_root_mean_squared_error: 0.2753\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0184 - mae: 0.1174 - mse: 0.0184 - root_mean_squared_error: 0.1358 - val_loss: 0.0759 - val_mae: 0.2671 - val_mse: 0.0759 - val_root_mean_squared_error: 0.2755\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0184 - mae: 0.1173 - mse: 0.0184 - root_mean_squared_error: 0.1356 - val_loss: 0.0760 - val_mae: 0.2673 - val_mse: 0.0760 - val_root_mean_squared_error: 0.2757\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0184 - mae: 0.1173 - mse: 0.0184 - root_mean_squared_error: 0.1357 - val_loss: 0.0761 - val_mae: 0.2675 - val_mse: 0.0761 - val_root_mean_squared_error: 0.2759\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0184 - mae: 0.1173 - mse: 0.0184 - root_mean_squared_error: 0.1356 - val_loss: 0.0761 - val_mae: 0.2675 - val_mse: 0.0761 - val_root_mean_squared_error: 0.2759\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0183 - mae: 0.1172 - mse: 0.0183 - root_mean_squared_error: 0.1353 - val_loss: 0.0761 - val_mae: 0.2675 - val_mse: 0.0761 - val_root_mean_squared_error: 0.2759\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0184 - mae: 0.1173 - mse: 0.0184 - root_mean_squared_error: 0.1356 - val_loss: 0.0761 - val_mae: 0.2674 - val_mse: 0.0761 - val_root_mean_squared_error: 0.2758\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0184 - mae: 0.1172 - mse: 0.0184 - root_mean_squared_error: 0.1356 - val_loss: 0.0758 - val_mae: 0.2670 - val_mse: 0.0758 - val_root_mean_squared_error: 0.2754\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0183 - mae: 0.1172 - mse: 0.0183 - root_mean_squared_error: 0.1354 - val_loss: 0.0756 - val_mae: 0.2666 - val_mse: 0.0756 - val_root_mean_squared_error: 0.2750\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.0183 - mae: 0.1172 - mse: 0.0183 - root_mean_squared_error: 0.1354 - val_loss: 0.0756 - val_mae: 0.2665 - val_mse: 0.0756 - val_root_mean_squared_error: 0.2749\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0184 - mae: 0.1173 - mse: 0.0184 - root_mean_squared_error: 0.1355 - val_loss: 0.0755 - val_mae: 0.2664 - val_mse: 0.0755 - val_root_mean_squared_error: 0.2748\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.0184 - mae: 0.1173 - mse: 0.0184 - root_mean_squared_error: 0.1355 - val_loss: 0.0756 - val_mae: 0.2665 - val_mse: 0.0756 - val_root_mean_squared_error: 0.2749\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0184 - mae: 0.1173 - mse: 0.0184 - root_mean_squared_error: 0.1356 - val_loss: 0.0758 - val_mae: 0.2668 - val_mse: 0.0758 - val_root_mean_squared_error: 0.2752\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0184 - mae: 0.1173 - mse: 0.0184 - root_mean_squared_error: 0.1356 - val_loss: 0.0760 - val_mae: 0.2673 - val_mse: 0.0760 - val_root_mean_squared_error: 0.2757\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0184 - mae: 0.1173 - mse: 0.0184 - root_mean_squared_error: 0.1356 - val_loss: 0.0764 - val_mae: 0.2679 - val_mse: 0.0764 - val_root_mean_squared_error: 0.2763\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.0183 - mae: 0.1172 - mse: 0.0183 - root_mean_squared_error: 0.1355 - val_loss: 0.0765 - val_mae: 0.2683 - val_mse: 0.0765 - val_root_mean_squared_error: 0.2766\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0184 - mae: 0.1172 - mse: 0.0184 - root_mean_squared_error: 0.1355 - val_loss: 0.0766 - val_mae: 0.2684 - val_mse: 0.0766 - val_root_mean_squared_error: 0.2768\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0183 - mae: 0.1171 - mse: 0.0183 - root_mean_squared_error: 0.1353 - val_loss: 0.0765 - val_mae: 0.2683 - val_mse: 0.0765 - val_root_mean_squared_error: 0.2766\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0183 - mae: 0.1172 - mse: 0.0183 - root_mean_squared_error: 0.1354 - val_loss: 0.0764 - val_mae: 0.2680 - val_mse: 0.0764 - val_root_mean_squared_error: 0.2763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1afb25d5910>"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the keras model\n",
    "keras_model.fit(train_data_inputs, train_data_targets,\n",
    "          validation_data=(val_data_inputs, val_data_targets),\n",
    "          epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Evaluate the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0634 - mae: 0.2427 - mse: 0.0634 - root_mean_squared_error: 0.2519\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0764 - mae: 0.2680 - mse: 0.0764 - root_mean_squared_error: 0.2763\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3566 - mae: 0.5962 - mse: 0.3566 - root_mean_squared_error: 0.5971\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3863 - mae: 0.6206 - mse: 0.3863 - root_mean_squared_error: 0.6215\n",
      "\n",
      "\n",
      "Manual Transformer:\n",
      "-------------------\n",
      "Validation Loss: 0.06344471871852875, Validation MSE: 0.06344471871852875, Validation MAE: 0.2426632195711136, Validation RMSE: 0.2518823742866516\n",
      "Test Loss: 0.35656213760375977, Test MSE: 0.35656213760375977, Test MAE: 0.5961843729019165, Test RMSE: 0.5971282720565796\n",
      "\n",
      "Keras Transformer:\n",
      "------------------\n",
      "Validation Loss: 0.07636135816574097, Validation MSE: 0.07636135816574097, Validation MAE: 0.2679590582847595, Validation RMSE: 0.2763355076313019\n",
      "Test Loss: 0.38632282614707947, Test MSE: 0.3863227963447571, Test MAE: 0.6206419467926025, Test RMSE: 0.6215487122535706\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "val_metrics_manul = manul_model.evaluate(val_data_inputs, val_data_targets, return_dict=True)\n",
    "val_metrics_keras = keras_model.evaluate(val_data_inputs, val_data_targets, return_dict=True)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics_manul = manul_model.evaluate(test_data_inputs, test_data_targets, return_dict=True)\n",
    "test_metrics_keras = keras_model.evaluate(test_data_inputs, test_data_targets, return_dict=True)\n",
    "\n",
    "# Extract individual metrics\n",
    "val_loss_manul, val_mae_manul, val_mse_manul, val_rmse_manul = val_metrics_manul['loss'], val_metrics_manul['mae'], val_metrics_manul['mse'], val_metrics_manul['root_mean_squared_error']\n",
    "test_loss_manul, test_mae_manul, test_mse_manul, test_rmse_manul = test_metrics_manul['loss'], test_metrics_manul['mae'], test_metrics_manul['mse'], test_metrics_manul['root_mean_squared_error']\n",
    "\n",
    "val_loss_keras, val_mae_keras, val_mse_keras, val_rmse_keras = val_metrics_keras['loss'], val_metrics_keras['mae'], val_metrics_keras['mse'], val_metrics_keras['root_mean_squared_error']\n",
    "test_loss_keras, test_mae_keras, test_mse_keras, test_rmse_keras = test_metrics_keras['loss'], test_metrics_keras['mae'], test_metrics_keras['mse'], test_metrics_keras['root_mean_squared_error']\n",
    "\n",
    "print('\\n\\nManual Transformer:\\n-------------------')\n",
    "print(f'Validation Loss: {val_loss_manul}, Validation MSE: {val_mse_manul}, Validation MAE: {val_mae_manul}, Validation RMSE: {val_rmse_manul}')\n",
    "print(f'Test Loss: {test_loss_manul}, Test MSE: {test_mse_manul}, Test MAE: {test_mae_manul}, Test RMSE: {test_rmse_manul}')\n",
    "\n",
    "print('\\nKeras Transformer:\\n------------------')\n",
    "print(f'Validation Loss: {val_loss_keras}, Validation MSE: {val_mse_keras}, Validation MAE: {val_mae_keras}, Validation RMSE: {val_rmse_keras}')\n",
    "print(f'Test Loss: {test_loss_keras}, Test MSE: {test_mse_keras}, Test MAE: {test_mae_keras}, Test RMSE: {test_rmse_keras}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 928ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\n",
      "\n",
      "Manual Transformer:\n",
      "-------------------\n",
      "Validation MAE: 0.2426632046699524\n",
      "Validation RMSE: 0.2518823444843292\n",
      "\n",
      "Test MAE: 0.5961843654245372\n",
      "Test RMSE: 0.5971282407581258\n",
      "\n",
      "==============================\n",
      "\n",
      "Keras Transformer:\n",
      "------------------\n",
      "Validation MAE: 0.2679591178894043\n",
      "Validation RMSE: 0.27633556723594666\n",
      "\n",
      "Test MAE: 0.6206419799547538\n",
      "Test RMSE: 0.6215487148291851\n"
     ]
    }
   ],
   "source": [
    "# Assuming manul_model.predict returns the predictions\n",
    "val_predictions_manul = manul_model.predict(val_data_inputs)\n",
    "test_predictions_manul = manul_model.predict(test_data_inputs)\n",
    "\n",
    "# Assuming keras_model.predict returns the predictions\n",
    "val_predictions_keras = keras_model.predict(val_data_inputs)\n",
    "test_predictions_keras  = keras_model.predict(test_data_inputs)\n",
    "\n",
    "# Calculate MAE and RMSE for validation set\n",
    "val_mae_manul = np.mean(np.abs(val_data_targets - val_predictions_manul))\n",
    "val_rmse_manul = np.sqrt(np.mean(np.square(val_data_targets - val_predictions_manul)))\n",
    "\n",
    "val_mae_keras  = np.mean(np.abs(val_data_targets - val_predictions_keras ))\n",
    "val_rmse_keras  = np.sqrt(np.mean(np.square(val_data_targets - val_predictions_keras )))\n",
    "\n",
    "# Calculate MAE and RMSE for test set\n",
    "test_mae_manul = np.mean(np.abs(test_data_targets - test_predictions_manul))\n",
    "test_rmse_manul = np.sqrt(np.mean(np.square(test_data_targets - test_predictions_manul)))\n",
    "\n",
    "test_mae_keras  = np.mean(np.abs(test_data_targets - test_predictions_keras ))\n",
    "test_rmse_keras  = np.sqrt(np.mean(np.square(test_data_targets - test_predictions_keras )))\n",
    "\n",
    "\n",
    "print('\\n\\nManual Transformer:\\n-------------------')\n",
    "print(f'Validation MAE: {val_mae_manul}')\n",
    "print(f'Validation RMSE: {val_rmse_manul}')\n",
    "print(f'\\nTest MAE: {test_mae_manul}')\n",
    "print(f'Test RMSE: {test_rmse_manul}')\n",
    "print('\\n==============================')\n",
    "print('\\nKeras Transformer:\\n------------------')\n",
    "print(f'Validation MAE: {val_mae_keras }')\n",
    "print(f'Validation RMSE: {val_rmse_keras }')\n",
    "print(f'\\nTest MAE: {test_mae_keras }')\n",
    "print(f'Test RMSE: {test_rmse_keras }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual Transformer:\n",
    "-------------------\n",
    "Validation MAE: 0.23751895129680634\n",
    "Validation RMSE: 0.24693451821804047\n",
    "\n",
    "Test MAE: 0.5918878594314744\n",
    "Test RMSE: 0.5928387148608047\n",
    "\n",
    "==============================\n",
    "\n",
    "Keras Transformer:\n",
    "------------------\n",
    "Validation MAE: 0.25300586223602295\n",
    "Validation RMSE: 0.261861115694046\n",
    "\n",
    "Test MAE: 0.6056900992721553\n",
    "Test RMSE: 0.6066191836564087"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
