{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **_Transformer Model for Daily EV Charging Dataset Process_**\n",
    "\n",
    "### **Load and Preprocess Data**\n",
    "\n",
    "``Load Data``\n",
    "\n",
    "Load the daily EV charging demand dataset.\n",
    "\n",
    "``Preprocess Data``\n",
    "- **Normalize Features:**\n",
    "  - MinMaxScaler.\n",
    "- **Split Dataset:**\n",
    "  - Divide the dataset into training (80%), validation (10%) and testing (10%) sets.\n",
    "\n",
    "### **Define and Compile Model**\n",
    "\n",
    "``Define Model``\n",
    "\n",
    "Utilize the transformer model architecture as specified in the referenced paper.\n",
    "\n",
    "``Compile Model``\n",
    "- **Optimizer:** Adam\n",
    "- **Learning Rate:** 1e-4\n",
    "- **Loss Function:** Mean Squared Error\n",
    "\n",
    "### **Train the Model**\n",
    "\n",
    "``Train the model on the training set using the following parameters:``\n",
    "- **Input Data:** Daily EV charging demand dataset\n",
    "- **Output:** Forecasted EV charging loads for the next day\n",
    "- **Training Data Split:** 80% of the dataset\n",
    "- **Validation Data Split:** 10% of the dataset\n",
    "- **Testing Data Split:** 10% of the dataset\n",
    "\n",
    "``Hyperparameters:``\n",
    "- **Number of Heads:** 1\n",
    "- **Hidden Dimension of Feedforward Network (dff):** 64\n",
    "- **Number of Layers:** 6\n",
    "- **Dropout Rate:** 0.1\n",
    "- **Epochs:** 60\n",
    "- **Batch Size:** 32\n",
    "\n",
    "### **Evaluate the Model**\n",
    "\n",
    "``Evaluate Model``\n",
    "\n",
    "- RMSE\n",
    "- MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1668 entries, 0 to 1667\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Energy__kWh_   1668 non-null   float64\n",
      " 1   Weekday        1668 non-null   object \n",
      " 2   Month          1668 non-null   object \n",
      " 3   Minimum T      1668 non-null   int64  \n",
      " 4   Maximum T      1668 non-null   int64  \n",
      " 5   Snow           1668 non-null   float64\n",
      " 6   Precipitation  1668 non-null   float64\n",
      "dtypes: float64(3), int64(2), object(2)\n",
      "memory usage: 91.3+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "daily = pd.read_csv('../Dataset/Boulder_Daily.csv')\n",
    "daily.drop(columns={'Unnamed: 0'}, inplace=True)\n",
    "daily.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Scale the Dataset with MinMaxScaler / One-Hot Encode and Extract the Entire Scaled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Energy__kWh_', 'Minimum T', 'Maximum T', 'Snow', 'Precipitation',\n",
       "       'Weekday_Friday', 'Weekday_Monday', 'Weekday_Saturday',\n",
       "       'Weekday_Sunday', 'Weekday_Thursday', 'Weekday_Tuesday',\n",
       "       'Weekday_Wednesday', 'Month_April', 'Month_August', 'Month_December',\n",
       "       'Month_February', 'Month_January', 'Month_July', 'Month_June',\n",
       "       'Month_March', 'Month_May', 'Month_November', 'Month_October',\n",
       "       'Month_September'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the columns we need to scale and we need to use for One-Hot Encoding\n",
    "columns_to_scale = ['Energy__kWh_', 'Minimum T', 'Maximum T', 'Snow', 'Precipitation']\n",
    "categorical_columns = ['Weekday','Month']\n",
    "\n",
    "# MinMax scaling for numerical columns and One-hot encoding for categorical columns\n",
    "scaler = MinMaxScaler()\n",
    "daily_scaled = daily.copy()\n",
    "daily_scaled[columns_to_scale] = scaler.fit_transform(daily[columns_to_scale])\n",
    "\n",
    "# One-hot encoding for categorical columns\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "categorical_encoded = onehot_encoder.fit_transform(daily[categorical_columns])\n",
    "\n",
    "# Get the feature names from the encoder\n",
    "encoded_columns = []\n",
    "for col, values in zip(categorical_columns, onehot_encoder.categories_):\n",
    "    encoded_columns.extend([f'{col}_{value}' for value in values])\n",
    "\n",
    "# Create DataFrame with encoded columns\n",
    "categorical_encoded_df = pd.DataFrame(categorical_encoded, columns=encoded_columns)\n",
    "\n",
    "# Concatenate the new encoded columns to the original DataFrame\n",
    "daily_scaled = pd.concat([daily_scaled, categorical_encoded_df], axis=1)\n",
    "\n",
    "# Drop the original categorical columns\n",
    "daily_scaled = daily_scaled.drop(categorical_columns, axis=1)\n",
    "daily_scaled.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Divided the dataset into training, testing, and validation datasets according to 0.70, 0.20, and 0.10, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and testing sets\n",
    "def split_dataset(df, train_ratio, val_ratio):\n",
    "\n",
    "    total_size = len(df)\n",
    "    train_size = int(train_ratio * total_size)\n",
    "    val_size = int(val_ratio * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    train_df = df[:train_size]\n",
    "    val_df = df[train_size:train_size + val_size]\n",
    "    test_df = df[train_size + val_size:]\n",
    "\n",
    "    assert len(train_df) + len(val_df) + len(test_df) == total_size, \"Dataset not split correctly.\"\n",
    "\n",
    "    print(f'Training split ratio:   {round(len(train_df) / len(df), 3)}')\n",
    "    print(f'Validation split ratio: {round(len(val_df) / len(df), 3)}')\n",
    "    print(f'Testing split ratio:    {round(len(test_df) / len(df), 3)}')\n",
    "    print(\"\\nShapes of the datasets:\")\n",
    "    print(train_df.shape, val_df.shape, test_df.shape)\n",
    "\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training split ratio:   0.7\n",
      "Validation split ratio: 0.2\n",
      "Testing split ratio:    0.101\n",
      "\n",
      "Shapes of the datasets:\n",
      "(1167, 24) (333, 24) (168, 24)\n"
     ]
    }
   ],
   "source": [
    "train_daily_scaled, val_daily_scaled, test_daily_scaled = split_dataset(daily_scaled, train_ratio=0.7, val_ratio=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Create sequences for the Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data\n",
    "def create_sequences(data, sequence_length):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequence = data.iloc[i:i + sequence_length].values\n",
    "        target = data.iloc[i + sequence_length]['Energy__kWh_']  # Predict the next value\n",
    "        inputs.append(sequence)\n",
    "        targets.append(target)\n",
    "\n",
    "    inputs_array = np.array(inputs)\n",
    "    targets_array = np.array(targets)\n",
    "    \n",
    "    print(f'Dataset split into sequences:')\n",
    "    print(f'Sequences shape: {inputs_array.shape}')\n",
    "    print(f'Targets shape: {targets_array.shape}\\n')\n",
    "\n",
    "    return np.array(inputs), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split into sequences:\n",
      "Sequences shape: (1047, 120, 24)\n",
      "Targets shape: (1047,)\n",
      "\n",
      "Dataset split into sequences:\n",
      "Sequences shape: (213, 120, 24)\n",
      "Targets shape: (213,)\n",
      "\n",
      "Dataset split into sequences:\n",
      "Sequences shape: (48, 120, 24)\n",
      "Targets shape: (48,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 120\n",
    "num_features = len(daily_scaled.columns)\n",
    "\n",
    "# Create the training, validation, and test data sequences\n",
    "train_data_inputs, train_data_targets = create_sequences(train_daily_scaled, sequence_length)\n",
    "val_data_inputs, val_data_targets = create_sequences(val_daily_scaled, sequence_length)\n",
    "test_data_inputs, test_data_targets = create_sequences(test_daily_scaled, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1047, 120, 24), (213, 120, 24), (48, 120, 24))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The input Datasets must have this input shape (-1, sequence_length, num_features)\n",
    "train_data_inputs = train_data_inputs.reshape((-1, sequence_length, num_features))\n",
    "val_data_inputs = val_data_inputs.reshape((-1, sequence_length, num_features))\n",
    "test_data_inputs = test_data_inputs.reshape((-1, sequence_length, num_features))\n",
    "\n",
    "train_data_inputs.shape, val_data_inputs.shape, test_data_inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Create the Transformer Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 120, 24)]    0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention_77 (Multi  (None, None, 24)    2400        ['input_5[0][0]',                \n",
      " HeadAttention)                                                   'input_5[0][0]',                \n",
      "                                                                  'input_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_159 (Dropout)          (None, None, 24)     0           ['multi_head_attention_77[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_70 (TFOpL  (None, 120, 24)     0           ['input_5[0][0]',                \n",
      " ambda)                                                           'dropout_159[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_130 (Layer  (None, 120, 24)     48          ['tf.__operators__.add_70[0][0]']\n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " positionwise_feed_forward_29 (  (None, 120, 24)     3160        ['layer_normalization_130[0][0]']\n",
      " PositionwiseFeedForward)                                                                         \n",
      "                                                                                                  \n",
      " dropout_161 (Dropout)          (None, 120, 24)      0           ['positionwise_feed_forward_29[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_71 (TFOpL  (None, 120, 24)     0           ['layer_normalization_130[0][0]',\n",
      " ambda)                                                           'dropout_161[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_131 (Layer  (None, 120, 24)     48          ['tf.__operators__.add_71[0][0]']\n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_88 (Multi  (None, None, 24)    2400        ['layer_normalization_131[0][0]',\n",
      " HeadAttention)                                                   'layer_normalization_131[0][0]',\n",
      "                                                                  'layer_normalization_131[0][0]']\n",
      "                                                                                                  \n",
      " dropout_182 (Dropout)          (None, None, 24)     0           ['multi_head_attention_88[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_87 (TFOpL  (None, 120, 24)     0           ['dropout_182[0][0]',            \n",
      " ambda)                                                           'layer_normalization_131[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_147 (Layer  (None, 120, 24)     48          ['tf.__operators__.add_87[0][0]']\n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_89 (Multi  (None, None, 24)    2400        ['layer_normalization_147[0][0]',\n",
      " HeadAttention)                                                   'input_5[0][0]',                \n",
      "                                                                  'input_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_183 (Dropout)          (None, None, 24)     0           ['multi_head_attention_89[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_88 (TFOpL  (None, None, 24)    0           ['dropout_182[0][0]',            \n",
      " ambda)                                                           'dropout_183[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_148 (Layer  (None, None, 24)    48          ['tf.__operators__.add_88[0][0]']\n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " positionwise_feed_forward_35 (  (None, None, 24)    3160        ['layer_normalization_148[0][0]']\n",
      " PositionwiseFeedForward)                                                                         \n",
      "                                                                                                  \n",
      " dropout_185 (Dropout)          (None, None, 24)     0           ['positionwise_feed_forward_35[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_89 (TFOpL  (None, None, 24)    0           ['layer_normalization_148[0][0]',\n",
      " ambda)                                                           'dropout_185[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_149 (Layer  (None, None, 24)    48          ['tf.__operators__.add_89[0][0]']\n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " time_distributed_4 (TimeDistri  (None, None, 1)     25          ['layer_normalization_149[0][0]']\n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13,785\n",
      "Trainable params: 13,785\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "%run \"../Code/Transformer.ipynb\"\n",
    "\n",
    "# Define the hyperparameters of the manual model\n",
    "input_shape = (sequence_length, num_features)\n",
    "num_heads = 1\n",
    "d_ff = 64\n",
    "num_layers = 6\n",
    "dropout_rate = 0.1\n",
    "encoder_mask = None\n",
    "decoder_mask = tf.linalg.band_part(tf.ones((sequence_length, sequence_length)), -1, 0)  # Create a lower triangular mask\n",
    "decoder_mask = 1 - decoder_mask  # Invert the mask\n",
    "\n",
    "# Create the transformer model\n",
    "manul_model = TransformerModel(input_shape, num_heads, d_ff, num_layers, dropout_rate, encoder_mask, decoder_mask)\n",
    "\n",
    "manul_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 120, 24)]    0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention_90 (Multi  (None, 120, 24)     11904       ['input_6[0][0]',                \n",
      " HeadAttention)                                                   'input_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_186 (Dropout)          (None, 120, 24)      0           ['multi_head_attention_90[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_150 (Layer  (None, 120, 24)     48          ['dropout_186[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_60 (Sequential)     (None, 120, 120)     9400        ['layer_normalization_150[0][0]']\n",
      "                                                                                                  \n",
      " dropout_187 (Dropout)          (None, 120, 120)     0           ['sequential_60[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_151 (Layer  (None, 120, 120)    240         ['dropout_187[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_91 (Multi  (None, 120, 120)    58080       ['layer_normalization_151[0][0]',\n",
      " HeadAttention)                                                   'layer_normalization_151[0][0]']\n",
      "                                                                                                  \n",
      " dropout_188 (Dropout)          (None, 120, 120)     0           ['multi_head_attention_91[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_152 (Layer  (None, 120, 120)    240         ['dropout_188[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_61 (Sequential)     (None, 120, 120)     15544       ['layer_normalization_152[0][0]']\n",
      "                                                                                                  \n",
      " dropout_189 (Dropout)          (None, 120, 120)     0           ['sequential_61[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_153 (Layer  (None, 120, 120)    240         ['dropout_189[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_92 (Multi  (None, 120, 120)    58080       ['layer_normalization_153[0][0]',\n",
      " HeadAttention)                                                   'layer_normalization_153[0][0]']\n",
      "                                                                                                  \n",
      " dropout_190 (Dropout)          (None, 120, 120)     0           ['multi_head_attention_92[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_154 (Layer  (None, 120, 120)    240         ['dropout_190[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_62 (Sequential)     (None, 120, 120)     15544       ['layer_normalization_154[0][0]']\n",
      "                                                                                                  \n",
      " dropout_191 (Dropout)          (None, 120, 120)     0           ['sequential_62[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_155 (Layer  (None, 120, 120)    240         ['dropout_191[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_93 (Multi  (None, 120, 120)    58080       ['layer_normalization_155[0][0]',\n",
      " HeadAttention)                                                   'layer_normalization_155[0][0]']\n",
      "                                                                                                  \n",
      " dropout_192 (Dropout)          (None, 120, 120)     0           ['multi_head_attention_93[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_156 (Layer  (None, 120, 120)    240         ['dropout_192[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_63 (Sequential)     (None, 120, 120)     15544       ['layer_normalization_156[0][0]']\n",
      "                                                                                                  \n",
      " dropout_193 (Dropout)          (None, 120, 120)     0           ['sequential_63[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_157 (Layer  (None, 120, 120)    240         ['dropout_193[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_94 (Multi  (None, 120, 120)    58080       ['layer_normalization_157[0][0]',\n",
      " HeadAttention)                                                   'layer_normalization_157[0][0]']\n",
      "                                                                                                  \n",
      " dropout_194 (Dropout)          (None, 120, 120)     0           ['multi_head_attention_94[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_158 (Layer  (None, 120, 120)    240         ['dropout_194[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_64 (Sequential)     (None, 120, 120)     15544       ['layer_normalization_158[0][0]']\n",
      "                                                                                                  \n",
      " dropout_195 (Dropout)          (None, 120, 120)     0           ['sequential_64[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_159 (Layer  (None, 120, 120)    240         ['dropout_195[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_95 (Multi  (None, 120, 120)    58080       ['layer_normalization_159[0][0]',\n",
      " HeadAttention)                                                   'layer_normalization_159[0][0]']\n",
      "                                                                                                  \n",
      " dropout_196 (Dropout)          (None, 120, 120)     0           ['multi_head_attention_95[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_160 (Layer  (None, 120, 120)    240         ['dropout_196[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_65 (Sequential)     (None, 120, 120)     15544       ['layer_normalization_160[0][0]']\n",
      "                                                                                                  \n",
      " dropout_197 (Dropout)          (None, 120, 120)     0           ['sequential_65[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_161 (Layer  (None, 120, 120)    240         ['dropout_197[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_96 (Multi  (None, 120, 120)    58080       ['layer_normalization_161[0][0]',\n",
      " HeadAttention)                                                   'layer_normalization_161[0][0]']\n",
      "                                                                                                  \n",
      " dropout_198 (Dropout)          (None, 120, 120)     0           ['multi_head_attention_96[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_162 (Layer  (None, 120, 120)    240         ['dropout_198[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_97 (Multi  (None, 120, 120)    58080       ['layer_normalization_162[0][0]',\n",
      " HeadAttention)                                                   'layer_normalization_161[0][0]']\n",
      "                                                                                                  \n",
      " dropout_199 (Dropout)          (None, 120, 120)     0           ['multi_head_attention_97[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_163 (Layer  (None, 120, 120)    240         ['dropout_199[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_66 (Sequential)     (None, 120, 120)     15544       ['layer_normalization_163[0][0]']\n",
      "                                                                                                  \n",
      " dropout_200 (Dropout)          (None, 120, 120)     0           ['sequential_66[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_164 (Layer  (None, 120, 120)    240         ['dropout_200[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_98 (Multi  (None, 120, 120)    58080       ['layer_normalization_164[0][0]',\n",
      " HeadAttention)                                                   'layer_normalization_164[0][0]']\n",
      "                                                                                                  \n",
      " dropout_201 (Dropout)          (None, 120, 120)     0           ['multi_head_attention_98[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_165 (Layer  (None, 120, 120)    240         ['dropout_201[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_99 (Multi  (None, 120, 120)    58080       ['layer_normalization_165[0][0]',\n",
      " HeadAttention)                                                   'layer_normalization_161[0][0]']\n",
      "                                                                                                  \n",
      " dropout_202 (Dropout)          (None, 120, 120)     0           ['multi_head_attention_99[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_166 (Layer  (None, 120, 120)    240         ['dropout_202[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_67 (Sequential)     (None, 120, 120)     15544       ['layer_normalization_166[0][0]']\n",
      "                                                                                                  \n",
      " dropout_203 (Dropout)          (None, 120, 120)     0           ['sequential_67[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_167 (Layer  (None, 120, 120)    240         ['dropout_203[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_100 (Mult  (None, 120, 120)    58080       ['layer_normalization_167[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_167[0][0]']\n",
      "                                                                                                  \n",
      " dropout_204 (Dropout)          (None, 120, 120)     0           ['multi_head_attention_100[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_168 (Layer  (None, 120, 120)    240         ['dropout_204[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_101 (Mult  (None, 120, 120)    58080       ['layer_normalization_168[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_161[0][0]']\n",
      "                                                                                                  \n",
      " dropout_205 (Dropout)          (None, 120, 120)     0           ['multi_head_attention_101[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_169 (Layer  (None, 120, 120)    240         ['dropout_205[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_68 (Sequential)     (None, 120, 120)     15544       ['layer_normalization_169[0][0]']\n",
      "                                                                                                  \n",
      " dropout_206 (Dropout)          (None, 120, 120)     0           ['sequential_68[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_170 (Layer  (None, 120, 120)    240         ['dropout_206[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_102 (Mult  (None, 120, 120)    58080       ['layer_normalization_170[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_170[0][0]']\n",
      "                                                                                                  \n",
      " dropout_207 (Dropout)          (None, 120, 120)     0           ['multi_head_attention_102[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_171 (Layer  (None, 120, 120)    240         ['dropout_207[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_103 (Mult  (None, 120, 120)    58080       ['layer_normalization_171[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_161[0][0]']\n",
      "                                                                                                  \n",
      " dropout_208 (Dropout)          (None, 120, 120)     0           ['multi_head_attention_103[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_172 (Layer  (None, 120, 120)    240         ['dropout_208[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_69 (Sequential)     (None, 120, 120)     15544       ['layer_normalization_172[0][0]']\n",
      "                                                                                                  \n",
      " dropout_209 (Dropout)          (None, 120, 120)     0           ['sequential_69[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_173 (Layer  (None, 120, 120)    240         ['dropout_209[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_104 (Mult  (None, 120, 120)    58080       ['layer_normalization_173[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_173[0][0]']\n",
      "                                                                                                  \n",
      " dropout_210 (Dropout)          (None, 120, 120)     0           ['multi_head_attention_104[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_174 (Layer  (None, 120, 120)    240         ['dropout_210[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_105 (Mult  (None, 120, 120)    58080       ['layer_normalization_174[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_161[0][0]']\n",
      "                                                                                                  \n",
      " dropout_211 (Dropout)          (None, 120, 120)     0           ['multi_head_attention_105[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_175 (Layer  (None, 120, 120)    240         ['dropout_211[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_70 (Sequential)     (None, 120, 120)     15544       ['layer_normalization_175[0][0]']\n",
      "                                                                                                  \n",
      " dropout_212 (Dropout)          (None, 120, 120)     0           ['sequential_70[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_176 (Layer  (None, 120, 120)    240         ['dropout_212[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_106 (Mult  (None, 120, 120)    58080       ['layer_normalization_176[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_176[0][0]']\n",
      "                                                                                                  \n",
      " dropout_213 (Dropout)          (None, 120, 120)     0           ['multi_head_attention_106[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_177 (Layer  (None, 120, 120)    240         ['dropout_213[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_107 (Mult  (None, 120, 120)    58080       ['layer_normalization_177[0][0]',\n",
      " iHeadAttention)                                                  'layer_normalization_161[0][0]']\n",
      "                                                                                                  \n",
      " dropout_214 (Dropout)          (None, 120, 120)     0           ['multi_head_attention_107[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_178 (Layer  (None, 120, 120)    240         ['dropout_214[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential_71 (Sequential)     (None, 120, 120)     15544       ['layer_normalization_178[0][0]']\n",
      "                                                                                                  \n",
      " dropout_215 (Dropout)          (None, 120, 120)     0           ['sequential_71[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_179 (Layer  (None, 120, 120)    240         ['dropout_215[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " time_distributed_5 (TimeDistri  (None, 120, 1)      121         ['layer_normalization_179[0][0]']\n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,186,777\n",
      "Trainable params: 1,186,777\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the transformer model\n",
    "keras_model = keras_transformer_model(input_shape, num_heads, d_ff, num_layers, dropout_rate)\n",
    "\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Compile the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return tf.keras.backend.sqrt(\n",
    "        tf.keras.backend.mean(\n",
    "            tf.keras.backend.square(\n",
    "                y_pred - y_true\n",
    "            )\n",
    "        ) + 1e-9\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate for Adam optimizer\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Compile the manual model\n",
    "manul_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss='mse',  metrics=['mae', 'mse', root_mean_squared_error])\n",
    "\n",
    "# Compile the keras model\n",
    "keras_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss='mse',  metrics=['mae', 'mse', root_mean_squared_error])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Train the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1047, 120, 24), (1047,), (213, 120, 24), (213,))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the parameters for training\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "# Convert the data to float32\n",
    "train_data_inputs = train_data_inputs.astype('float32')\n",
    "train_data_targets = train_data_targets.astype('float32')\n",
    "\n",
    "val_data_inputs = val_data_inputs.astype('float32')\n",
    "val_data_targets = val_data_targets.astype('float32')\n",
    "\n",
    "train_data_inputs.shape, train_data_targets.shape, val_data_inputs.shape, val_data_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 7s 78ms/step - loss: 1.0495 - mae: 0.6552 - mse: 1.0495 - root_mean_squared_error: 0.7369 - val_loss: 0.0281 - val_mae: 0.1384 - val_mse: 0.0281 - val_root_mean_squared_error: 0.1684\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 2s 53ms/step - loss: 0.0710 - mae: 0.2091 - mse: 0.0710 - root_mean_squared_error: 0.2639 - val_loss: 0.0321 - val_mae: 0.1506 - val_mse: 0.0321 - val_root_mean_squared_error: 0.1801\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 2s 55ms/step - loss: 0.0369 - mae: 0.1518 - mse: 0.0369 - root_mean_squared_error: 0.1914 - val_loss: 0.0526 - val_mae: 0.2043 - val_mse: 0.0526 - val_root_mean_squared_error: 0.2307\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 2s 51ms/step - loss: 0.0252 - mae: 0.1265 - mse: 0.0252 - root_mean_squared_error: 0.1577 - val_loss: 0.0561 - val_mae: 0.2123 - val_mse: 0.0561 - val_root_mean_squared_error: 0.2382\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 2s 52ms/step - loss: 0.0201 - mae: 0.1141 - mse: 0.0201 - root_mean_squared_error: 0.1414 - val_loss: 0.0484 - val_mae: 0.1943 - val_mse: 0.0484 - val_root_mean_squared_error: 0.2212\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 2s 51ms/step - loss: 0.0183 - mae: 0.1096 - mse: 0.0183 - root_mean_squared_error: 0.1346 - val_loss: 0.0509 - val_mae: 0.2003 - val_mse: 0.0509 - val_root_mean_squared_error: 0.2268\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 2s 51ms/step - loss: 0.0175 - mae: 0.1071 - mse: 0.0175 - root_mean_squared_error: 0.1313 - val_loss: 0.0490 - val_mae: 0.1958 - val_mse: 0.0490 - val_root_mean_squared_error: 0.2225\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 2s 58ms/step - loss: 0.0171 - mae: 0.1063 - mse: 0.0171 - root_mean_squared_error: 0.1300 - val_loss: 0.0444 - val_mae: 0.1843 - val_mse: 0.0444 - val_root_mean_squared_error: 0.2117\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 2s 71ms/step - loss: 0.0170 - mae: 0.1060 - mse: 0.0170 - root_mean_squared_error: 0.1295 - val_loss: 0.0387 - val_mae: 0.1695 - val_mse: 0.0387 - val_root_mean_squared_error: 0.1977\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 2s 61ms/step - loss: 0.0172 - mae: 0.1067 - mse: 0.0172 - root_mean_squared_error: 0.1304 - val_loss: 0.0653 - val_mae: 0.2323 - val_mse: 0.0653 - val_root_mean_squared_error: 0.2568\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 2s 63ms/step - loss: 0.0172 - mae: 0.1061 - mse: 0.0172 - root_mean_squared_error: 0.1297 - val_loss: 0.0345 - val_mae: 0.1576 - val_mse: 0.0345 - val_root_mean_squared_error: 0.1866\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 2s 61ms/step - loss: 0.0167 - mae: 0.1049 - mse: 0.0167 - root_mean_squared_error: 0.1288 - val_loss: 0.0455 - val_mae: 0.1871 - val_mse: 0.0455 - val_root_mean_squared_error: 0.2143\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 2s 61ms/step - loss: 0.0167 - mae: 0.1053 - mse: 0.0167 - root_mean_squared_error: 0.1286 - val_loss: 0.0484 - val_mae: 0.1943 - val_mse: 0.0484 - val_root_mean_squared_error: 0.2210\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 2s 63ms/step - loss: 0.0167 - mae: 0.1052 - mse: 0.0167 - root_mean_squared_error: 0.1282 - val_loss: 0.0478 - val_mae: 0.1929 - val_mse: 0.0478 - val_root_mean_squared_error: 0.2197\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 2s 62ms/step - loss: 0.0163 - mae: 0.1037 - mse: 0.0163 - root_mean_squared_error: 0.1267 - val_loss: 0.0538 - val_mae: 0.2070 - val_mse: 0.0538 - val_root_mean_squared_error: 0.2330\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 2s 65ms/step - loss: 0.0165 - mae: 0.1043 - mse: 0.0165 - root_mean_squared_error: 0.1276 - val_loss: 0.0602 - val_mae: 0.2214 - val_mse: 0.0602 - val_root_mean_squared_error: 0.2464\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 2s 63ms/step - loss: 0.0180 - mae: 0.1084 - mse: 0.0180 - root_mean_squared_error: 0.1329 - val_loss: 0.0415 - val_mae: 0.1771 - val_mse: 0.0415 - val_root_mean_squared_error: 0.2048\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 2s 63ms/step - loss: 0.0169 - mae: 0.1059 - mse: 0.0169 - root_mean_squared_error: 0.1293 - val_loss: 0.0441 - val_mae: 0.1839 - val_mse: 0.0441 - val_root_mean_squared_error: 0.2111\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0163 - mae: 0.1042 - mse: 0.0163 - root_mean_squared_error: 0.1267 - val_loss: 0.0334 - val_mae: 0.1545 - val_mse: 0.0334 - val_root_mean_squared_error: 0.1836\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 2s 61ms/step - loss: 0.0179 - mae: 0.1085 - mse: 0.0179 - root_mean_squared_error: 0.1330 - val_loss: 0.0313 - val_mae: 0.1482 - val_mse: 0.0313 - val_root_mean_squared_error: 0.1777\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 2s 62ms/step - loss: 0.0170 - mae: 0.1062 - mse: 0.0170 - root_mean_squared_error: 0.1296 - val_loss: 0.0530 - val_mae: 0.2054 - val_mse: 0.0530 - val_root_mean_squared_error: 0.2314\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0175 - mae: 0.1065 - mse: 0.0175 - root_mean_squared_error: 0.1312 - val_loss: 0.0503 - val_mae: 0.1989 - val_mse: 0.0503 - val_root_mean_squared_error: 0.2254\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 2s 64ms/step - loss: 0.0163 - mae: 0.1043 - mse: 0.0163 - root_mean_squared_error: 0.1273 - val_loss: 0.0369 - val_mae: 0.1646 - val_mse: 0.0369 - val_root_mean_squared_error: 0.1930\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 2s 62ms/step - loss: 0.0185 - mae: 0.1104 - mse: 0.0185 - root_mean_squared_error: 0.1354 - val_loss: 0.0567 - val_mae: 0.2137 - val_mse: 0.0567 - val_root_mean_squared_error: 0.2393\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 2s 63ms/step - loss: 0.0167 - mae: 0.1056 - mse: 0.0167 - root_mean_squared_error: 0.1284 - val_loss: 0.0432 - val_mae: 0.1813 - val_mse: 0.0432 - val_root_mean_squared_error: 0.2087\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0164 - mae: 0.1047 - mse: 0.0164 - root_mean_squared_error: 0.1276 - val_loss: 0.0636 - val_mae: 0.2287 - val_mse: 0.0636 - val_root_mean_squared_error: 0.2533\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 2s 63ms/step - loss: 0.0170 - mae: 0.1062 - mse: 0.0170 - root_mean_squared_error: 0.1296 - val_loss: 0.0477 - val_mae: 0.1926 - val_mse: 0.0477 - val_root_mean_squared_error: 0.2194\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0161 - mae: 0.1035 - mse: 0.0161 - root_mean_squared_error: 0.1260 - val_loss: 0.0624 - val_mae: 0.2263 - val_mse: 0.0624 - val_root_mean_squared_error: 0.2511\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 2s 62ms/step - loss: 0.0164 - mae: 0.1043 - mse: 0.0164 - root_mean_squared_error: 0.1271 - val_loss: 0.0491 - val_mae: 0.1961 - val_mse: 0.0491 - val_root_mean_squared_error: 0.2227\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0167 - mae: 0.1055 - mse: 0.0167 - root_mean_squared_error: 0.1288 - val_loss: 0.0671 - val_mae: 0.2361 - val_mse: 0.0671 - val_root_mean_squared_error: 0.2603\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 2s 65ms/step - loss: 0.0162 - mae: 0.1040 - mse: 0.0162 - root_mean_squared_error: 0.1269 - val_loss: 0.0506 - val_mae: 0.1997 - val_mse: 0.0506 - val_root_mean_squared_error: 0.2261\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 2s 62ms/step - loss: 0.0161 - mae: 0.1034 - mse: 0.0161 - root_mean_squared_error: 0.1263 - val_loss: 0.0371 - val_mae: 0.1651 - val_mse: 0.0371 - val_root_mean_squared_error: 0.1935\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 2s 62ms/step - loss: 0.0165 - mae: 0.1048 - mse: 0.0165 - root_mean_squared_error: 0.1270 - val_loss: 0.0320 - val_mae: 0.1504 - val_mse: 0.0320 - val_root_mean_squared_error: 0.1798\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0170 - mae: 0.1074 - mse: 0.0170 - root_mean_squared_error: 0.1293 - val_loss: 0.0507 - val_mae: 0.1999 - val_mse: 0.0507 - val_root_mean_squared_error: 0.2263\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 2s 62ms/step - loss: 0.0166 - mae: 0.1045 - mse: 0.0166 - root_mean_squared_error: 0.1279 - val_loss: 0.0627 - val_mae: 0.2268 - val_mse: 0.0627 - val_root_mean_squared_error: 0.2515\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 2s 61ms/step - loss: 0.0163 - mae: 0.1034 - mse: 0.0163 - root_mean_squared_error: 0.1270 - val_loss: 0.0522 - val_mae: 0.2035 - val_mse: 0.0522 - val_root_mean_squared_error: 0.2297\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 2s 61ms/step - loss: 0.0174 - mae: 0.1075 - mse: 0.0174 - root_mean_squared_error: 0.1305 - val_loss: 0.0332 - val_mae: 0.1539 - val_mse: 0.0332 - val_root_mean_squared_error: 0.1830\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 2s 63ms/step - loss: 0.0178 - mae: 0.1098 - mse: 0.0178 - root_mean_squared_error: 0.1326 - val_loss: 0.0586 - val_mae: 0.2180 - val_mse: 0.0586 - val_root_mean_squared_error: 0.2433\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 2s 61ms/step - loss: 0.0173 - mae: 0.1072 - mse: 0.0173 - root_mean_squared_error: 0.1308 - val_loss: 0.0546 - val_mae: 0.2088 - val_mse: 0.0546 - val_root_mean_squared_error: 0.2347\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 2s 64ms/step - loss: 0.0161 - mae: 0.1038 - mse: 0.0161 - root_mean_squared_error: 0.1261 - val_loss: 0.0583 - val_mae: 0.2173 - val_mse: 0.0583 - val_root_mean_squared_error: 0.2426\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0160 - mae: 0.1033 - mse: 0.0160 - root_mean_squared_error: 0.1260 - val_loss: 0.0604 - val_mae: 0.2219 - val_mse: 0.0604 - val_root_mean_squared_error: 0.2469\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 2s 63ms/step - loss: 0.0170 - mae: 0.1057 - mse: 0.0170 - root_mean_squared_error: 0.1291 - val_loss: 0.0582 - val_mae: 0.2172 - val_mse: 0.0582 - val_root_mean_squared_error: 0.2425\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 2s 61ms/step - loss: 0.0171 - mae: 0.1074 - mse: 0.0171 - root_mean_squared_error: 0.1294 - val_loss: 0.0618 - val_mae: 0.2249 - val_mse: 0.0618 - val_root_mean_squared_error: 0.2498\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 2s 63ms/step - loss: 0.0164 - mae: 0.1047 - mse: 0.0164 - root_mean_squared_error: 0.1277 - val_loss: 0.0546 - val_mae: 0.2089 - val_mse: 0.0546 - val_root_mean_squared_error: 0.2347\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 2s 67ms/step - loss: 0.0160 - mae: 0.1039 - mse: 0.0160 - root_mean_squared_error: 0.1256 - val_loss: 0.0421 - val_mae: 0.1786 - val_mse: 0.0421 - val_root_mean_squared_error: 0.2061\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 3s 78ms/step - loss: 0.0166 - mae: 0.1061 - mse: 0.0166 - root_mean_squared_error: 0.1282 - val_loss: 0.0490 - val_mae: 0.1959 - val_mse: 0.0490 - val_root_mean_squared_error: 0.2225\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 2s 63ms/step - loss: 0.0164 - mae: 0.1045 - mse: 0.0164 - root_mean_squared_error: 0.1273 - val_loss: 0.0535 - val_mae: 0.2064 - val_mse: 0.0535 - val_root_mean_squared_error: 0.2324\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 2s 55ms/step - loss: 0.0164 - mae: 0.1038 - mse: 0.0164 - root_mean_squared_error: 0.1273 - val_loss: 0.0574 - val_mae: 0.2152 - val_mse: 0.0574 - val_root_mean_squared_error: 0.2406\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 0.0166 - mae: 0.1056 - mse: 0.0166 - root_mean_squared_error: 0.1277 - val_loss: 0.0440 - val_mae: 0.1835 - val_mse: 0.0440 - val_root_mean_squared_error: 0.2108\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 2s 55ms/step - loss: 0.0161 - mae: 0.1036 - mse: 0.0161 - root_mean_squared_error: 0.1259 - val_loss: 0.0594 - val_mae: 0.2198 - val_mse: 0.0594 - val_root_mean_squared_error: 0.2450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2afa00d8520>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the manual model\n",
    "manul_model.fit(train_data_inputs, train_data_targets,\n",
    "          validation_data=(val_data_inputs, val_data_targets),\n",
    "          epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "33/33 [==============================] - 82s 2s/step - loss: 2.5582 - mae: 0.5918 - mse: 2.5582 - root_mean_squared_error: 0.6566 - val_loss: 0.0742 - val_mae: 0.2500 - val_mse: 0.0742 - val_root_mean_squared_error: 0.2736\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 53s 2s/step - loss: 0.0596 - mae: 0.1939 - mse: 0.0596 - root_mean_squared_error: 0.2437 - val_loss: 0.0538 - val_mae: 0.2071 - val_mse: 0.0538 - val_root_mean_squared_error: 0.2331\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 54s 2s/step - loss: 0.0432 - mae: 0.1655 - mse: 0.0432 - root_mean_squared_error: 0.2073 - val_loss: 0.0610 - val_mae: 0.2233 - val_mse: 0.0610 - val_root_mean_squared_error: 0.2482\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 53s 2s/step - loss: 0.0325 - mae: 0.1437 - mse: 0.0325 - root_mean_squared_error: 0.1796 - val_loss: 0.0379 - val_mae: 0.1673 - val_mse: 0.0379 - val_root_mean_squared_error: 0.1956\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 54s 2s/step - loss: 0.0273 - mae: 0.1324 - mse: 0.0273 - root_mean_squared_error: 0.1647 - val_loss: 0.0500 - val_mae: 0.1981 - val_mse: 0.0500 - val_root_mean_squared_error: 0.2246\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 56s 2s/step - loss: 0.0237 - mae: 0.1238 - mse: 0.0237 - root_mean_squared_error: 0.1534 - val_loss: 0.0655 - val_mae: 0.2327 - val_mse: 0.0655 - val_root_mean_squared_error: 0.2571\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 53s 2s/step - loss: 0.0215 - mae: 0.1183 - mse: 0.0215 - root_mean_squared_error: 0.1459 - val_loss: 0.0358 - val_mae: 0.1614 - val_mse: 0.0358 - val_root_mean_squared_error: 0.1901\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 56s 2s/step - loss: 0.0215 - mae: 0.1186 - mse: 0.0215 - root_mean_squared_error: 0.1458 - val_loss: 0.1034 - val_mae: 0.3019 - val_mse: 0.1034 - val_root_mean_squared_error: 0.3230\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 54s 2s/step - loss: 0.0195 - mae: 0.1128 - mse: 0.0195 - root_mean_squared_error: 0.1392 - val_loss: 0.0896 - val_mae: 0.2783 - val_mse: 0.0896 - val_root_mean_squared_error: 0.3007\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 54s 2s/step - loss: 0.0212 - mae: 0.1174 - mse: 0.0212 - root_mean_squared_error: 0.1442 - val_loss: 0.0403 - val_mae: 0.1739 - val_mse: 0.0403 - val_root_mean_squared_error: 0.2018\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 55s 2s/step - loss: 0.0178 - mae: 0.1085 - mse: 0.0178 - root_mean_squared_error: 0.1327 - val_loss: 0.0897 - val_mae: 0.2785 - val_mse: 0.0897 - val_root_mean_squared_error: 0.3008\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 53s 2s/step - loss: 0.0194 - mae: 0.1132 - mse: 0.0194 - root_mean_squared_error: 0.1377 - val_loss: 0.0816 - val_mae: 0.2640 - val_mse: 0.0816 - val_root_mean_squared_error: 0.2869\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 56s 2s/step - loss: 0.0179 - mae: 0.1087 - mse: 0.0179 - root_mean_squared_error: 0.1332 - val_loss: 0.0619 - val_mae: 0.2252 - val_mse: 0.0619 - val_root_mean_squared_error: 0.2500\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 55s 2s/step - loss: 0.0195 - mae: 0.1143 - mse: 0.0195 - root_mean_squared_error: 0.1388 - val_loss: 0.1062 - val_mae: 0.3065 - val_mse: 0.1062 - val_root_mean_squared_error: 0.3273\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0195 - mae: 0.1129 - mse: 0.0195 - root_mean_squared_error: 0.1383 - val_loss: 0.0476 - val_mae: 0.1924 - val_mse: 0.0476 - val_root_mean_squared_error: 0.2191\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0183 - mae: 0.1098 - mse: 0.0183 - root_mean_squared_error: 0.1345 - val_loss: 0.0653 - val_mae: 0.2323 - val_mse: 0.0653 - val_root_mean_squared_error: 0.2567\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 54s 2s/step - loss: 0.0173 - mae: 0.1070 - mse: 0.0173 - root_mean_squared_error: 0.1302 - val_loss: 0.0820 - val_mae: 0.2647 - val_mse: 0.0820 - val_root_mean_squared_error: 0.2876\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 54s 2s/step - loss: 0.0172 - mae: 0.1067 - mse: 0.0172 - root_mean_squared_error: 0.1302 - val_loss: 0.0817 - val_mae: 0.2643 - val_mse: 0.0817 - val_root_mean_squared_error: 0.2872\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 59s 2s/step - loss: 0.0204 - mae: 0.1163 - mse: 0.0204 - root_mean_squared_error: 0.1413 - val_loss: 0.0983 - val_mae: 0.2935 - val_mse: 0.0983 - val_root_mean_squared_error: 0.3149\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 53s 2s/step - loss: 0.0196 - mae: 0.1141 - mse: 0.0196 - root_mean_squared_error: 0.1388 - val_loss: 0.0309 - val_mae: 0.1471 - val_mse: 0.0309 - val_root_mean_squared_error: 0.1766\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 53s 2s/step - loss: 0.0176 - mae: 0.1079 - mse: 0.0176 - root_mean_squared_error: 0.1317 - val_loss: 0.0780 - val_mae: 0.2574 - val_mse: 0.0780 - val_root_mean_squared_error: 0.2806\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0188 - mae: 0.1131 - mse: 0.0188 - root_mean_squared_error: 0.1358 - val_loss: 0.0905 - val_mae: 0.2799 - val_mse: 0.0905 - val_root_mean_squared_error: 0.3021\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 48s 1s/step - loss: 0.0210 - mae: 0.1159 - mse: 0.0210 - root_mean_squared_error: 0.1428 - val_loss: 0.0290 - val_mae: 0.1414 - val_mse: 0.0290 - val_root_mean_squared_error: 0.1711\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 49s 1s/step - loss: 0.0195 - mae: 0.1135 - mse: 0.0195 - root_mean_squared_error: 0.1380 - val_loss: 0.0615 - val_mae: 0.2243 - val_mse: 0.0615 - val_root_mean_squared_error: 0.2491\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 50s 2s/step - loss: 0.0163 - mae: 0.1042 - mse: 0.0163 - root_mean_squared_error: 0.1272 - val_loss: 0.0620 - val_mae: 0.2253 - val_mse: 0.0620 - val_root_mean_squared_error: 0.2501\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 47s 1s/step - loss: 0.0183 - mae: 0.1107 - mse: 0.0183 - root_mean_squared_error: 0.1345 - val_loss: 0.0698 - val_mae: 0.2414 - val_mse: 0.0698 - val_root_mean_squared_error: 0.2653\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 46s 1s/step - loss: 0.0184 - mae: 0.1101 - mse: 0.0184 - root_mean_squared_error: 0.1335 - val_loss: 0.0432 - val_mae: 0.1815 - val_mse: 0.0432 - val_root_mean_squared_error: 0.2088\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 46s 1s/step - loss: 0.0172 - mae: 0.1075 - mse: 0.0172 - root_mean_squared_error: 0.1303 - val_loss: 0.0569 - val_mae: 0.2141 - val_mse: 0.0569 - val_root_mean_squared_error: 0.2397\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 46s 1s/step - loss: 0.0165 - mae: 0.1046 - mse: 0.0165 - root_mean_squared_error: 0.1273 - val_loss: 0.0689 - val_mae: 0.2396 - val_mse: 0.0689 - val_root_mean_squared_error: 0.2637\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 50s 2s/step - loss: 0.0159 - mae: 0.1023 - mse: 0.0159 - root_mean_squared_error: 0.1252 - val_loss: 0.0491 - val_mae: 0.1961 - val_mse: 0.0491 - val_root_mean_squared_error: 0.2227\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 47s 1s/step - loss: 0.0175 - mae: 0.1085 - mse: 0.0175 - root_mean_squared_error: 0.1311 - val_loss: 0.0377 - val_mae: 0.1668 - val_mse: 0.0377 - val_root_mean_squared_error: 0.1951\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 49s 1s/step - loss: 0.0164 - mae: 0.1053 - mse: 0.0164 - root_mean_squared_error: 0.1274 - val_loss: 0.0803 - val_mae: 0.2616 - val_mse: 0.0803 - val_root_mean_squared_error: 0.2846\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 48s 1s/step - loss: 0.0165 - mae: 0.1047 - mse: 0.0165 - root_mean_squared_error: 0.1273 - val_loss: 0.0668 - val_mae: 0.2353 - val_mse: 0.0668 - val_root_mean_squared_error: 0.2596\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 47s 1s/step - loss: 0.0166 - mae: 0.1063 - mse: 0.0166 - root_mean_squared_error: 0.1280 - val_loss: 0.0605 - val_mae: 0.2222 - val_mse: 0.0605 - val_root_mean_squared_error: 0.2472\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 50s 2s/step - loss: 0.0163 - mae: 0.1042 - mse: 0.0163 - root_mean_squared_error: 0.1269 - val_loss: 0.0519 - val_mae: 0.2026 - val_mse: 0.0519 - val_root_mean_squared_error: 0.2289\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 50s 2s/step - loss: 0.0165 - mae: 0.1049 - mse: 0.0165 - root_mean_squared_error: 0.1276 - val_loss: 0.0754 - val_mae: 0.2525 - val_mse: 0.0754 - val_root_mean_squared_error: 0.2759\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 50s 2s/step - loss: 0.0174 - mae: 0.1077 - mse: 0.0174 - root_mean_squared_error: 0.1307 - val_loss: 0.0880 - val_mae: 0.2755 - val_mse: 0.0880 - val_root_mean_squared_error: 0.2980\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 48s 1s/step - loss: 0.0174 - mae: 0.1075 - mse: 0.0174 - root_mean_squared_error: 0.1306 - val_loss: 0.0655 - val_mae: 0.2328 - val_mse: 0.0655 - val_root_mean_squared_error: 0.2572\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 48s 1s/step - loss: 0.0162 - mae: 0.1033 - mse: 0.0162 - root_mean_squared_error: 0.1265 - val_loss: 0.0470 - val_mae: 0.1911 - val_mse: 0.0470 - val_root_mean_squared_error: 0.2179\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 51s 2s/step - loss: 0.0181 - mae: 0.1107 - mse: 0.0181 - root_mean_squared_error: 0.1336 - val_loss: 0.0755 - val_mae: 0.2525 - val_mse: 0.0755 - val_root_mean_squared_error: 0.2759\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 49s 1s/step - loss: 0.0162 - mae: 0.1046 - mse: 0.0162 - root_mean_squared_error: 0.1264 - val_loss: 0.0445 - val_mae: 0.1847 - val_mse: 0.0445 - val_root_mean_squared_error: 0.2119\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0167 - mae: 0.1047 - mse: 0.0167 - root_mean_squared_error: 0.1284 - val_loss: 0.0574 - val_mae: 0.2153 - val_mse: 0.0574 - val_root_mean_squared_error: 0.2407\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 47s 1s/step - loss: 0.0165 - mae: 0.1055 - mse: 0.0165 - root_mean_squared_error: 0.1278 - val_loss: 0.0785 - val_mae: 0.2583 - val_mse: 0.0785 - val_root_mean_squared_error: 0.2814\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 51s 2s/step - loss: 0.0161 - mae: 0.1039 - mse: 0.0161 - root_mean_squared_error: 0.1258 - val_loss: 0.0372 - val_mae: 0.1655 - val_mse: 0.0372 - val_root_mean_squared_error: 0.1939\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 46s 1s/step - loss: 0.0170 - mae: 0.1067 - mse: 0.0170 - root_mean_squared_error: 0.1292 - val_loss: 0.0382 - val_mae: 0.1683 - val_mse: 0.0382 - val_root_mean_squared_error: 0.1965\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 48s 1s/step - loss: 0.0167 - mae: 0.1055 - mse: 0.0167 - root_mean_squared_error: 0.1282 - val_loss: 0.0636 - val_mae: 0.2287 - val_mse: 0.0636 - val_root_mean_squared_error: 0.2533\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 49s 1s/step - loss: 0.0166 - mae: 0.1052 - mse: 0.0166 - root_mean_squared_error: 0.1276 - val_loss: 0.0772 - val_mae: 0.2559 - val_mse: 0.0772 - val_root_mean_squared_error: 0.2792\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 51s 2s/step - loss: 0.0167 - mae: 0.1045 - mse: 0.0167 - root_mean_squared_error: 0.1278 - val_loss: 0.0779 - val_mae: 0.2571 - val_mse: 0.0779 - val_root_mean_squared_error: 0.2804\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 52s 2s/step - loss: 0.0165 - mae: 0.1056 - mse: 0.0165 - root_mean_squared_error: 0.1274 - val_loss: 0.0885 - val_mae: 0.2764 - val_mse: 0.0885 - val_root_mean_squared_error: 0.2989\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 49s 1s/step - loss: 0.0174 - mae: 0.1071 - mse: 0.0174 - root_mean_squared_error: 0.1309 - val_loss: 0.0718 - val_mae: 0.2454 - val_mse: 0.0718 - val_root_mean_squared_error: 0.2692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2af8495a550>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the keras model\n",
    "keras_model.fit(train_data_inputs, train_data_targets,\n",
    "          validation_data=(val_data_inputs, val_data_targets),\n",
    "          epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Evaluate the Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0594 - mae: 0.2198 - mse: 0.0594 - root_mean_squared_error: 0.2450\n",
      "7/7 [==============================] - 4s 557ms/step - loss: 0.0718 - mae: 0.2454 - mse: 0.0718 - root_mean_squared_error: 0.2692\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.2454 - mae: 0.4747 - mse: 0.2454 - root_mean_squared_error: 0.4990\n",
      "2/2 [==============================] - 1s 192ms/step - loss: 0.2719 - mae: 0.5018 - mse: 0.2719 - root_mean_squared_error: 0.5249\n",
      "\n",
      "\n",
      "Manual Transformer:\n",
      "-------------------\n",
      "Validation Loss: 0.05943479761481285, Validation MSE: 0.05943480134010315, Validation MAE: 0.2198190987110138, Validation RMSE: 0.24495282769203186\n",
      "Test Loss: 0.24539761245250702, Test MSE: 0.24539761245250702, Test MAE: 0.4746626019477844, Test RMSE: 0.4989876449108124\n",
      "\n",
      "Keras Transformer:\n",
      "------------------\n",
      "Validation Loss: 0.07178176939487457, Validation MSE: 0.07178176939487457, Validation MAE: 0.24538035690784454, Validation RMSE: 0.26915842294692993\n",
      "Test Loss: 0.27185699343681335, Test MSE: 0.27185699343681335, Test MAE: 0.5017684698104858, Test RMSE: 0.5249119997024536\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "val_metrics_manul = manul_model.evaluate(val_data_inputs, val_data_targets, return_dict=True)\n",
    "val_metrics_keras = keras_model.evaluate(val_data_inputs, val_data_targets, return_dict=True)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics_manul = manul_model.evaluate(test_data_inputs, test_data_targets, return_dict=True)\n",
    "test_metrics_keras = keras_model.evaluate(test_data_inputs, test_data_targets, return_dict=True)\n",
    "\n",
    "# Extract individual metrics\n",
    "val_loss_manul, val_mae_manul, val_mse_manul, val_rmse_manul = val_metrics_manul['loss'], val_metrics_manul['mae'], val_metrics_manul['mse'], val_metrics_manul['root_mean_squared_error']\n",
    "test_loss_manul, test_mae_manul, test_mse_manul, test_rmse_manul = test_metrics_manul['loss'], test_metrics_manul['mae'], test_metrics_manul['mse'], test_metrics_manul['root_mean_squared_error']\n",
    "\n",
    "val_loss_keras, val_mae_keras, val_mse_keras, val_rmse_keras = val_metrics_keras['loss'], val_metrics_keras['mae'], val_metrics_keras['mse'], val_metrics_keras['root_mean_squared_error']\n",
    "test_loss_keras, test_mae_keras, test_mse_keras, test_rmse_keras = test_metrics_keras['loss'], test_metrics_keras['mae'], test_metrics_keras['mse'], test_metrics_keras['root_mean_squared_error']\n",
    "\n",
    "print('\\n\\nManual Transformer:\\n-------------------')\n",
    "print(f'Validation Loss: {val_loss_manul}, Validation MSE: {val_mse_manul}, Validation MAE: {val_mae_manul}, Validation RMSE: {val_rmse_manul}')\n",
    "print(f'Test Loss: {test_loss_manul}, Test MSE: {test_mse_manul}, Test MAE: {test_mae_manul}, Test RMSE: {test_rmse_manul}')\n",
    "\n",
    "print('\\nKeras Transformer:\\n------------------')\n",
    "print(f'Validation Loss: {val_loss_keras}, Validation MSE: {val_mse_keras}, Validation MAE: {val_mae_keras}, Validation RMSE: {val_rmse_keras}')\n",
    "print(f'Test Loss: {test_loss_keras}, Test MSE: {test_mse_keras}, Test MAE: {test_mae_keras}, Test RMSE: {test_rmse_keras}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 21ms/step\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "7/7 [==============================] - 7s 543ms/step\n",
      "2/2 [==============================] - 1s 235ms/step\n",
      "\n",
      "\n",
      "Manual Transformer:\n",
      "-------------------\n",
      "Validation MAE: 0.21982847154140472\n",
      "Validation RMSE: 0.24380365014076233\n",
      "\n",
      "Test MAE: 0.47466260563791707\n",
      "Test RMSE: 0.49536998971944574\n",
      "\n",
      "==============================\n",
      "\n",
      "Keras Transformer:\n",
      "------------------\n",
      "Validation MAE: 0.245380237698555\n",
      "Validation RMSE: 0.2679213285446167\n",
      "\n",
      "Test MAE: 0.5017678853445425\n",
      "Test RMSE: 0.5213986479250746\n"
     ]
    }
   ],
   "source": [
    "# Assuming manul_model.predict returns the predictions\n",
    "val_predictions_manul = manul_model.predict(val_data_inputs)\n",
    "test_predictions_manul = manul_model.predict(test_data_inputs)\n",
    "\n",
    "# Assuming keras_model.predict returns the predictions\n",
    "val_predictions_keras = keras_model.predict(val_data_inputs)\n",
    "test_predictions_keras  = keras_model.predict(test_data_inputs)\n",
    "\n",
    "# Calculate MAE and RMSE for validation set\n",
    "val_mae_manul = np.mean(np.abs(val_data_targets - val_predictions_manul))\n",
    "val_rmse_manul = np.sqrt(np.mean(np.square(val_data_targets - val_predictions_manul)))\n",
    "\n",
    "val_mae_keras  = np.mean(np.abs(val_data_targets - val_predictions_keras ))\n",
    "val_rmse_keras  = np.sqrt(np.mean(np.square(val_data_targets - val_predictions_keras )))\n",
    "\n",
    "# Calculate MAE and RMSE for test set\n",
    "test_mae_manul = np.mean(np.abs(test_data_targets - test_predictions_manul))\n",
    "test_rmse_manul = np.sqrt(np.mean(np.square(test_data_targets - test_predictions_manul)))\n",
    "\n",
    "test_mae_keras  = np.mean(np.abs(test_data_targets - test_predictions_keras ))\n",
    "test_rmse_keras  = np.sqrt(np.mean(np.square(test_data_targets - test_predictions_keras )))\n",
    "\n",
    "\n",
    "print('\\n\\nManual Transformer:\\n-------------------')\n",
    "print(f'Validation MAE: {val_mae_manul}')\n",
    "print(f'Validation RMSE: {val_rmse_manul}')\n",
    "print(f'\\nTest MAE: {test_mae_manul}')\n",
    "print(f'Test RMSE: {test_rmse_manul}')\n",
    "print('\\n==============================')\n",
    "print('\\nKeras Transformer:\\n------------------')\n",
    "print(f'Validation MAE: {val_mae_keras }')\n",
    "print(f'Validation RMSE: {val_rmse_keras }')\n",
    "print(f'\\nTest MAE: {test_mae_keras }')\n",
    "print(f'Test RMSE: {test_rmse_keras }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual Transformer:\n",
    "-------------------\n",
    "Validation MAE: 0.1764928102493286\n",
    "Validation RMSE: 0.20637793838977814\n",
    "\n",
    "Test MAE: 0.3902653265425372\n",
    "Test RMSE: 0.4151124082036069\n",
    "\n",
    "==============================\n",
    "\n",
    "Keras Transformer:\n",
    "------------------\n",
    "Validation MAE: 0.20550836622714996\n",
    "Validation RMSE: 0.23427383601665497\n",
    "\n",
    "Test MAE: 0.42464347135930863\n",
    "Test RMSE: 0.4475859202184749"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
